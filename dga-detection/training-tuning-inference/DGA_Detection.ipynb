{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Generation Algorithm (DGA) Detection\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Data Importing\n",
    "* Data Preprocessing\n",
    "* Training and Evaluation\n",
    "* Inference\n",
    "* Conclusion\n",
    "\n",
    "## Introduction\n",
    "[Domain Generation Algorithms](https://en.wikipedia.org/wiki/Domain_generation_algorithm) (DGAs) are used to generate domain names that can be used by the malware to communicate with the command and control servers. IP addresses and static domain names can be easily blocked, and a DGA provides an easy method to generate a large number of domain names and rotate through them to circumvent traditional block lists. We will use a type of recurrent neural network called the [Gated Recurrent Unit](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) (GRU) for this example. This implementation enables users to train their models with up-to-date domain names representative of both benign and DGA generated strings. This capability could also be used in production. This notebook provides a view into the data science workflow to create a DGA detection implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import torch\n",
    "import requests\n",
    "import logging\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from dga_detector import DGADetector\n",
    "from dataloader import DataLoader\n",
    "from dga_dataset import DGADataset\n",
    "from utils import str2ascii\n",
    "from cuml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable console logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Input Dataset to GPU Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV = \"../datasets/dga-training-data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = cudf.read_csv(INPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gdf['domain']\n",
    "labels = gdf['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have only benign and DGA (malicious) categoriesm, the number of domain types need to be set to 2 (`N_DOMAIN_TYPE=2`). Vocabulary size(`CHAR_VOCAB`) is set to 128 ASCII characters. The values below set for `HIDDEN_SIZE`, `N_LAYERS` of the network, and the `LR` (Learning Rate) give an optimum balance for the network size and performance. They might need be set via experiments when working with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "N_LAYERS = 3\n",
    "CHAR_VOCAB = 128\n",
    "HIDDEN_SIZE = 100\n",
    "N_DOMAIN_TYPE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate DGA detector\n",
    "Now that the data is ready, the datasets are created, and we've set the parameters for the model, we can use the DGA detector method to create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 15:35:26,644 [INFO] Found GPU's now setting up cuda for the model\n"
     ]
    }
   ],
   "source": [
    "dd = DGADetector(lr=LR)\n",
    "dd.init_model(n_layers=N_LAYERS, char_vocab=CHAR_VOCAB, hidden_size=HIDDEN_SIZE, n_domain_type=N_DOMAIN_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "TRAIN_SIZE = 0.7\n",
    "BATCH_SIZE = 10000\n",
    "MODELS_DIR = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Now we train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 15:35:28,718 [INFO] Initiating model training ...\n",
      "2023-05-10 15:35:28,719 [INFO] Truncate domains to width: 100\n",
      "/opt/conda/envs/mor_exp/lib/python3.8/site-packages/cudf/core/column/string.py:3563: FutureWarning: The expand parameter is deprecated and will be removed in a future version. Set expand=False to match future behavior.\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/25 [00:00<?, ?it/s]2023-05-10 15:35:32,470 [INFO] [100000/1433083 (7%)]\tLoss: 7167.31\n",
      "2023-05-10 15:35:34,650 [INFO] [200000/1433083 (14%)]\tLoss: 5351.56\n",
      "2023-05-10 15:35:36,630 [INFO] [300000/1433083 (21%)]\tLoss: 4190.30\n",
      "2023-05-10 15:35:38,518 [INFO] [400000/1433083 (28%)]\tLoss: 3618.32\n",
      "2023-05-10 15:35:40,254 [INFO] [500000/1433083 (35%)]\tLoss: 3225.12\n",
      "2023-05-10 15:35:41,884 [INFO] [600000/1433083 (42%)]\tLoss: 2774.30\n",
      "2023-05-10 15:35:43,496 [INFO] [700000/1433083 (49%)]\tLoss: 2429.28\n",
      "2023-05-10 15:35:45,104 [INFO] [800000/1433083 (56%)]\tLoss: 2158.28\n",
      "2023-05-10 15:35:46,728 [INFO] [900000/1433083 (63%)]\tLoss: 2332.08\n",
      "2023-05-10 15:35:48,285 [INFO] [1000000/1433083 (70%)]\tLoss: 2223.66\n",
      "2023-05-10 15:35:49,775 [INFO] [1100000/1433083 (77%)]\tLoss: 2236.70\n",
      "2023-05-10 15:35:51,195 [INFO] [1200000/1433083 (84%)]\tLoss: 2175.82\n",
      "2023-05-10 15:35:52,453 [INFO] [1300000/1433083 (91%)]\tLoss: 2182.41\n",
      "2023-05-10 15:35:53,640 [INFO] [1400000/1433083 (98%)]\tLoss: 2218.00\n",
      "2023-05-10 15:35:54,044 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:35:57,348 [INFO] Test set accuracy: 402607/614179 (0.6555206218382589)\n",
      "\n",
      "Epoch:   4%|▍         | 1/25 [00:27<11:04, 27.69s/it]2023-05-10 15:35:59,892 [INFO] [100000/1433083 (7%)]\tLoss: 3423.64\n",
      "2023-05-10 15:36:02,094 [INFO] [200000/1433083 (14%)]\tLoss: 2651.06\n",
      "2023-05-10 15:36:04,107 [INFO] [300000/1433083 (21%)]\tLoss: 1975.89\n",
      "2023-05-10 15:36:06,000 [INFO] [400000/1433083 (28%)]\tLoss: 1737.74\n",
      "2023-05-10 15:36:07,839 [INFO] [500000/1433083 (35%)]\tLoss: 1517.93\n",
      "2023-05-10 15:36:09,568 [INFO] [600000/1433083 (42%)]\tLoss: 1299.54\n",
      "2023-05-10 15:36:11,301 [INFO] [700000/1433083 (49%)]\tLoss: 1135.94\n",
      "2023-05-10 15:36:13,041 [INFO] [800000/1433083 (56%)]\tLoss: 1011.32\n",
      "2023-05-10 15:36:14,799 [INFO] [900000/1433083 (63%)]\tLoss: 963.94\n",
      "2023-05-10 15:36:16,380 [INFO] [1000000/1433083 (70%)]\tLoss: 956.96\n",
      "2023-05-10 15:36:17,872 [INFO] [1100000/1433083 (77%)]\tLoss: 930.60\n",
      "2023-05-10 15:36:19,273 [INFO] [1200000/1433083 (84%)]\tLoss: 930.75\n",
      "2023-05-10 15:36:20,532 [INFO] [1300000/1433083 (91%)]\tLoss: 985.98\n",
      "2023-05-10 15:36:21,724 [INFO] [1400000/1433083 (98%)]\tLoss: 1083.15\n",
      "2023-05-10 15:36:22,121 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:36:25,430 [INFO] Test set accuracy: 528119/614179 (0.859877983454335)\n",
      "\n",
      "Epoch:   8%|▊         | 2/25 [00:55<10:42, 27.92s/it]2023-05-10 15:36:28,034 [INFO] [100000/1433083 (7%)]\tLoss: 2524.56\n",
      "2023-05-10 15:36:30,253 [INFO] [200000/1433083 (14%)]\tLoss: 1866.57\n",
      "2023-05-10 15:36:32,291 [INFO] [300000/1433083 (21%)]\tLoss: 1391.64\n",
      "2023-05-10 15:36:34,182 [INFO] [400000/1433083 (28%)]\tLoss: 1251.94\n",
      "2023-05-10 15:36:36,026 [INFO] [500000/1433083 (35%)]\tLoss: 1105.01\n",
      "2023-05-10 15:36:37,783 [INFO] [600000/1433083 (42%)]\tLoss: 945.77\n",
      "2023-05-10 15:36:39,536 [INFO] [700000/1433083 (49%)]\tLoss: 827.29\n",
      "2023-05-10 15:36:41,276 [INFO] [800000/1433083 (56%)]\tLoss: 737.41\n",
      "2023-05-10 15:36:43,042 [INFO] [900000/1433083 (63%)]\tLoss: 710.46\n",
      "2023-05-10 15:36:44,620 [INFO] [1000000/1433083 (70%)]\tLoss: 716.51\n",
      "2023-05-10 15:36:46,250 [INFO] [1100000/1433083 (77%)]\tLoss: 698.57\n",
      "2023-05-10 15:36:47,668 [INFO] [1200000/1433083 (84%)]\tLoss: 710.28\n",
      "2023-05-10 15:36:48,945 [INFO] [1300000/1433083 (91%)]\tLoss: 750.61\n",
      "2023-05-10 15:36:50,126 [INFO] [1400000/1433083 (98%)]\tLoss: 857.05\n",
      "2023-05-10 15:36:50,524 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:36:53,852 [INFO] Test set accuracy: 588926/614179 (0.958883322288779)\n",
      "\n",
      "Epoch:  12%|█▏        | 3/25 [01:24<10:19, 28.15s/it]2023-05-10 15:36:56,468 [INFO] [100000/1433083 (7%)]\tLoss: 2216.44\n",
      "2023-05-10 15:36:58,687 [INFO] [200000/1433083 (14%)]\tLoss: 1512.23\n",
      "2023-05-10 15:37:00,735 [INFO] [300000/1433083 (21%)]\tLoss: 1110.58\n",
      "2023-05-10 15:37:02,665 [INFO] [400000/1433083 (28%)]\tLoss: 954.06\n",
      "2023-05-10 15:37:04,532 [INFO] [500000/1433083 (35%)]\tLoss: 832.85\n",
      "2023-05-10 15:37:06,290 [INFO] [600000/1433083 (42%)]\tLoss: 713.12\n",
      "2023-05-10 15:37:08,030 [INFO] [700000/1433083 (49%)]\tLoss: 624.85\n",
      "2023-05-10 15:37:09,780 [INFO] [800000/1433083 (56%)]\tLoss: 558.04\n",
      "2023-05-10 15:37:11,536 [INFO] [900000/1433083 (63%)]\tLoss: 546.94\n",
      "2023-05-10 15:37:13,124 [INFO] [1000000/1433083 (70%)]\tLoss: 566.27\n",
      "2023-05-10 15:37:14,626 [INFO] [1100000/1433083 (77%)]\tLoss: 558.73\n",
      "2023-05-10 15:37:16,036 [INFO] [1200000/1433083 (84%)]\tLoss: 581.20\n",
      "2023-05-10 15:37:17,310 [INFO] [1300000/1433083 (91%)]\tLoss: 618.57\n",
      "2023-05-10 15:37:18,497 [INFO] [1400000/1433083 (98%)]\tLoss: 730.15\n",
      "2023-05-10 15:37:18,898 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:37:22,217 [INFO] Test set accuracy: 596065/614179 (0.9705069694665561)\n",
      "\n",
      "Epoch:  16%|█▌        | 4/25 [01:52<09:52, 28.23s/it]2023-05-10 15:37:24,861 [INFO] [100000/1433083 (7%)]\tLoss: 488.21\n",
      "2023-05-10 15:37:27,077 [INFO] [200000/1433083 (14%)]\tLoss: 522.55\n",
      "2023-05-10 15:37:29,132 [INFO] [300000/1433083 (21%)]\tLoss: 425.20\n",
      "2023-05-10 15:37:31,060 [INFO] [400000/1433083 (28%)]\tLoss: 412.23\n",
      "2023-05-10 15:37:32,895 [INFO] [500000/1433083 (35%)]\tLoss: 386.82\n",
      "2023-05-10 15:37:34,664 [INFO] [600000/1433083 (42%)]\tLoss: 336.66\n",
      "2023-05-10 15:37:36,421 [INFO] [700000/1433083 (49%)]\tLoss: 299.13\n",
      "2023-05-10 15:37:38,192 [INFO] [800000/1433083 (56%)]\tLoss: 271.09\n",
      "2023-05-10 15:37:39,980 [INFO] [900000/1433083 (63%)]\tLoss: 286.38\n",
      "2023-05-10 15:37:41,567 [INFO] [1000000/1433083 (70%)]\tLoss: 318.71\n",
      "2023-05-10 15:37:43,083 [INFO] [1100000/1433083 (77%)]\tLoss: 326.12\n",
      "2023-05-10 15:37:44,498 [INFO] [1200000/1433083 (84%)]\tLoss: 360.03\n",
      "2023-05-10 15:37:45,783 [INFO] [1300000/1433083 (91%)]\tLoss: 404.98\n",
      "2023-05-10 15:37:46,944 [INFO] [1400000/1433083 (98%)]\tLoss: 545.61\n",
      "2023-05-10 15:37:47,333 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:37:50,672 [INFO] Test set accuracy: 596916/614179 (0.9718925590096699)\n",
      "\n",
      "Epoch:  20%|██        | 5/25 [02:21<09:26, 28.31s/it]2023-05-10 15:37:53,285 [INFO] [100000/1433083 (7%)]\tLoss: 479.99\n",
      "2023-05-10 15:37:55,486 [INFO] [200000/1433083 (14%)]\tLoss: 371.59\n",
      "2023-05-10 15:37:57,534 [INFO] [300000/1433083 (21%)]\tLoss: 312.82\n",
      "2023-05-10 15:37:59,473 [INFO] [400000/1433083 (28%)]\tLoss: 291.06\n",
      "2023-05-10 15:38:01,325 [INFO] [500000/1433083 (35%)]\tLoss: 281.88\n",
      "2023-05-10 15:38:03,069 [INFO] [600000/1433083 (42%)]\tLoss: 246.72\n",
      "2023-05-10 15:38:04,807 [INFO] [700000/1433083 (49%)]\tLoss: 220.31\n",
      "2023-05-10 15:38:06,545 [INFO] [800000/1433083 (56%)]\tLoss: 200.96\n",
      "2023-05-10 15:38:08,204 [INFO] [900000/1433083 (63%)]\tLoss: 217.33\n",
      "2023-05-10 15:38:09,802 [INFO] [1000000/1433083 (70%)]\tLoss: 248.63\n",
      "2023-05-10 15:38:11,317 [INFO] [1100000/1433083 (77%)]\tLoss: 257.08\n",
      "2023-05-10 15:38:12,721 [INFO] [1200000/1433083 (84%)]\tLoss: 293.78\n",
      "2023-05-10 15:38:13,992 [INFO] [1300000/1433083 (91%)]\tLoss: 335.50\n",
      "2023-05-10 15:38:15,187 [INFO] [1400000/1433083 (98%)]\tLoss: 480.07\n",
      "2023-05-10 15:38:15,592 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:38:18,938 [INFO] Test set accuracy: 596797/614179 (0.9716988044202097)\n",
      "\n",
      "Epoch:  24%|██▍       | 6/25 [02:49<08:57, 28.30s/it]2023-05-10 15:38:21,572 [INFO] [100000/1433083 (7%)]\tLoss: 211.80\n",
      "2023-05-10 15:38:23,801 [INFO] [200000/1433083 (14%)]\tLoss: 215.96\n",
      "2023-05-10 15:38:25,822 [INFO] [300000/1433083 (21%)]\tLoss: 197.25\n",
      "2023-05-10 15:38:27,754 [INFO] [400000/1433083 (28%)]\tLoss: 191.43\n",
      "2023-05-10 15:38:29,609 [INFO] [500000/1433083 (35%)]\tLoss: 195.65\n",
      "2023-05-10 15:38:31,391 [INFO] [600000/1433083 (42%)]\tLoss: 173.05\n",
      "2023-05-10 15:38:33,129 [INFO] [700000/1433083 (49%)]\tLoss: 156.01\n",
      "2023-05-10 15:38:34,875 [INFO] [800000/1433083 (56%)]\tLoss: 143.60\n",
      "2023-05-10 15:38:36,607 [INFO] [900000/1433083 (63%)]\tLoss: 161.83\n",
      "2023-05-10 15:38:38,088 [INFO] [1000000/1433083 (70%)]\tLoss: 195.15\n",
      "2023-05-10 15:38:39,506 [INFO] [1100000/1433083 (77%)]\tLoss: 204.39\n",
      "2023-05-10 15:38:40,813 [INFO] [1200000/1433083 (84%)]\tLoss: 241.46\n",
      "2023-05-10 15:38:42,035 [INFO] [1300000/1433083 (91%)]\tLoss: 280.09\n",
      "2023-05-10 15:38:43,234 [INFO] [1400000/1433083 (98%)]\tLoss: 421.78\n",
      "2023-05-10 15:38:43,644 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:38:46,988 [INFO] Test set accuracy: 597361/614179 (0.9726171034828609)\n",
      "\n",
      "Epoch:  28%|██▊       | 7/25 [03:17<08:27, 28.22s/it]2023-05-10 15:38:49,603 [INFO] [100000/1433083 (7%)]\tLoss: 201.55\n",
      "2023-05-10 15:38:51,822 [INFO] [200000/1433083 (14%)]\tLoss: 201.49\n",
      "2023-05-10 15:38:53,857 [INFO] [300000/1433083 (21%)]\tLoss: 181.37\n",
      "2023-05-10 15:38:55,745 [INFO] [400000/1433083 (28%)]\tLoss: 174.05\n",
      "2023-05-10 15:38:57,484 [INFO] [500000/1433083 (35%)]\tLoss: 177.36\n",
      "2023-05-10 15:38:59,126 [INFO] [600000/1433083 (42%)]\tLoss: 156.87\n",
      "2023-05-10 15:39:00,759 [INFO] [700000/1433083 (49%)]\tLoss: 141.36\n",
      "2023-05-10 15:39:02,395 [INFO] [800000/1433083 (56%)]\tLoss: 130.14\n",
      "2023-05-10 15:39:04,058 [INFO] [900000/1433083 (63%)]\tLoss: 146.23\n",
      "2023-05-10 15:39:05,594 [INFO] [1000000/1433083 (70%)]\tLoss: 182.77\n",
      "2023-05-10 15:39:07,103 [INFO] [1100000/1433083 (77%)]\tLoss: 190.21\n",
      "2023-05-10 15:39:08,607 [INFO] [1200000/1433083 (84%)]\tLoss: 219.07\n",
      "2023-05-10 15:39:09,869 [INFO] [1300000/1433083 (91%)]\tLoss: 255.21\n",
      "2023-05-10 15:39:11,066 [INFO] [1400000/1433083 (98%)]\tLoss: 377.69\n",
      "2023-05-10 15:39:11,475 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:39:14,779 [INFO] Test set accuracy: 600552/614179 (0.9778126572220802)\n",
      "\n",
      "Epoch:  32%|███▏      | 8/25 [03:45<07:57, 28.08s/it]2023-05-10 15:39:17,371 [INFO] [100000/1433083 (7%)]\tLoss: 234.74\n",
      "2023-05-10 15:39:19,603 [INFO] [200000/1433083 (14%)]\tLoss: 206.60\n",
      "2023-05-10 15:39:21,647 [INFO] [300000/1433083 (21%)]\tLoss: 178.95\n",
      "2023-05-10 15:39:23,570 [INFO] [400000/1433083 (28%)]\tLoss: 167.15\n",
      "2023-05-10 15:39:25,428 [INFO] [500000/1433083 (35%)]\tLoss: 167.93\n",
      "2023-05-10 15:39:27,179 [INFO] [600000/1433083 (42%)]\tLoss: 148.19\n",
      "2023-05-10 15:39:28,938 [INFO] [700000/1433083 (49%)]\tLoss: 133.17\n",
      "2023-05-10 15:39:30,696 [INFO] [800000/1433083 (56%)]\tLoss: 122.36\n",
      "2023-05-10 15:39:32,480 [INFO] [900000/1433083 (63%)]\tLoss: 135.64\n",
      "2023-05-10 15:39:34,084 [INFO] [1000000/1433083 (70%)]\tLoss: 171.41\n",
      "2023-05-10 15:39:35,594 [INFO] [1100000/1433083 (77%)]\tLoss: 177.27\n",
      "2023-05-10 15:39:37,004 [INFO] [1200000/1433083 (84%)]\tLoss: 202.53\n",
      "2023-05-10 15:39:38,267 [INFO] [1300000/1433083 (91%)]\tLoss: 234.66\n",
      "2023-05-10 15:39:39,455 [INFO] [1400000/1433083 (98%)]\tLoss: 351.93\n",
      "2023-05-10 15:39:39,828 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:39:43,159 [INFO] Test set accuracy: 602313/614179 (0.9806798995081238)\n",
      "\n",
      "Epoch:  36%|███▌      | 9/25 [04:13<07:30, 28.17s/it]2023-05-10 15:39:45,778 [INFO] [100000/1433083 (7%)]\tLoss: 194.86\n",
      "2023-05-10 15:39:47,978 [INFO] [200000/1433083 (14%)]\tLoss: 171.58\n",
      "2023-05-10 15:39:50,032 [INFO] [300000/1433083 (21%)]\tLoss: 151.35\n",
      "2023-05-10 15:39:51,952 [INFO] [400000/1433083 (28%)]\tLoss: 140.59\n",
      "2023-05-10 15:39:53,807 [INFO] [500000/1433083 (35%)]\tLoss: 141.69\n",
      "2023-05-10 15:39:55,546 [INFO] [600000/1433083 (42%)]\tLoss: 125.28\n",
      "2023-05-10 15:39:57,291 [INFO] [700000/1433083 (49%)]\tLoss: 112.64\n",
      "2023-05-10 15:39:59,039 [INFO] [800000/1433083 (56%)]\tLoss: 103.61\n",
      "2023-05-10 15:40:00,814 [INFO] [900000/1433083 (63%)]\tLoss: 115.59\n",
      "2023-05-10 15:40:02,406 [INFO] [1000000/1433083 (70%)]\tLoss: 142.53\n",
      "2023-05-10 15:40:03,928 [INFO] [1100000/1433083 (77%)]\tLoss: 148.04\n",
      "2023-05-10 15:40:05,336 [INFO] [1200000/1433083 (84%)]\tLoss: 173.51\n",
      "2023-05-10 15:40:06,584 [INFO] [1300000/1433083 (91%)]\tLoss: 203.13\n",
      "2023-05-10 15:40:07,760 [INFO] [1400000/1433083 (98%)]\tLoss: 324.25\n",
      "2023-05-10 15:40:08,158 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:40:11,512 [INFO] Test set accuracy: 603192/614179 (0.9821110783664045)\n",
      "\n",
      "Epoch:  40%|████      | 10/25 [04:41<07:03, 28.23s/it]2023-05-10 15:40:14,113 [INFO] [100000/1433083 (7%)]\tLoss: 176.06\n",
      "2023-05-10 15:40:16,347 [INFO] [200000/1433083 (14%)]\tLoss: 158.82\n",
      "2023-05-10 15:40:18,386 [INFO] [300000/1433083 (21%)]\tLoss: 138.36\n",
      "2023-05-10 15:40:20,305 [INFO] [400000/1433083 (28%)]\tLoss: 127.69\n",
      "2023-05-10 15:40:22,172 [INFO] [500000/1433083 (35%)]\tLoss: 128.19\n",
      "2023-05-10 15:40:23,948 [INFO] [600000/1433083 (42%)]\tLoss: 113.47\n",
      "2023-05-10 15:40:25,731 [INFO] [700000/1433083 (49%)]\tLoss: 102.06\n",
      "2023-05-10 15:40:27,507 [INFO] [800000/1433083 (56%)]\tLoss: 93.85\n",
      "2023-05-10 15:40:29,243 [INFO] [900000/1433083 (63%)]\tLoss: 104.04\n",
      "2023-05-10 15:40:30,731 [INFO] [1000000/1433083 (70%)]\tLoss: 135.03\n",
      "2023-05-10 15:40:32,227 [INFO] [1100000/1433083 (77%)]\tLoss: 139.71\n",
      "2023-05-10 15:40:33,620 [INFO] [1200000/1433083 (84%)]\tLoss: 159.31\n",
      "2023-05-10 15:40:34,893 [INFO] [1300000/1433083 (91%)]\tLoss: 186.51\n",
      "2023-05-10 15:40:36,083 [INFO] [1400000/1433083 (98%)]\tLoss: 304.13\n",
      "2023-05-10 15:40:36,482 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:40:39,785 [INFO] Test set accuracy: 603286/614179 (0.9822641282101797)\n",
      "\n",
      "Epoch:  44%|████▍     | 11/25 [05:10<06:35, 28.24s/it]2023-05-10 15:40:42,401 [INFO] [100000/1433083 (7%)]\tLoss: 173.10\n",
      "2023-05-10 15:40:44,633 [INFO] [200000/1433083 (14%)]\tLoss: 146.21\n",
      "2023-05-10 15:40:46,670 [INFO] [300000/1433083 (21%)]\tLoss: 125.36\n",
      "2023-05-10 15:40:48,570 [INFO] [400000/1433083 (28%)]\tLoss: 114.12\n",
      "2023-05-10 15:40:50,430 [INFO] [500000/1433083 (35%)]\tLoss: 116.30\n",
      "2023-05-10 15:40:52,172 [INFO] [600000/1433083 (42%)]\tLoss: 103.00\n",
      "2023-05-10 15:40:53,906 [INFO] [700000/1433083 (49%)]\tLoss: 92.68\n",
      "2023-05-10 15:40:55,655 [INFO] [800000/1433083 (56%)]\tLoss: 85.23\n",
      "2023-05-10 15:40:57,411 [INFO] [900000/1433083 (63%)]\tLoss: 94.29\n",
      "2023-05-10 15:40:58,997 [INFO] [1000000/1433083 (70%)]\tLoss: 115.20\n",
      "2023-05-10 15:41:00,494 [INFO] [1100000/1433083 (77%)]\tLoss: 119.13\n",
      "2023-05-10 15:41:01,912 [INFO] [1200000/1433083 (84%)]\tLoss: 137.51\n",
      "2023-05-10 15:41:03,185 [INFO] [1300000/1433083 (91%)]\tLoss: 162.29\n",
      "2023-05-10 15:41:04,389 [INFO] [1400000/1433083 (98%)]\tLoss: 266.50\n",
      "2023-05-10 15:41:04,797 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:41:08,122 [INFO] Test set accuracy: 604115/614179 (0.9836138975770907)\n",
      "\n",
      "Epoch:  48%|████▊     | 12/25 [05:38<06:07, 28.27s/it]2023-05-10 15:41:10,708 [INFO] [100000/1433083 (7%)]\tLoss: 144.12\n",
      "2023-05-10 15:41:12,954 [INFO] [200000/1433083 (14%)]\tLoss: 125.59\n",
      "2023-05-10 15:41:15,000 [INFO] [300000/1433083 (21%)]\tLoss: 109.45\n",
      "2023-05-10 15:41:16,925 [INFO] [400000/1433083 (28%)]\tLoss: 102.31\n",
      "2023-05-10 15:41:18,793 [INFO] [500000/1433083 (35%)]\tLoss: 103.47\n",
      "2023-05-10 15:41:20,541 [INFO] [600000/1433083 (42%)]\tLoss: 91.66\n",
      "2023-05-10 15:41:22,290 [INFO] [700000/1433083 (49%)]\tLoss: 82.57\n",
      "2023-05-10 15:41:24,043 [INFO] [800000/1433083 (56%)]\tLoss: 76.01\n",
      "2023-05-10 15:41:25,835 [INFO] [900000/1433083 (63%)]\tLoss: 83.72\n",
      "2023-05-10 15:41:27,428 [INFO] [1000000/1433083 (70%)]\tLoss: 102.52\n",
      "2023-05-10 15:41:28,947 [INFO] [1100000/1433083 (77%)]\tLoss: 126.79\n",
      "2023-05-10 15:41:30,465 [INFO] [1200000/1433083 (84%)]\tLoss: 144.55\n",
      "2023-05-10 15:41:31,753 [INFO] [1300000/1433083 (91%)]\tLoss: 166.45\n",
      "2023-05-10 15:41:32,953 [INFO] [1400000/1433083 (98%)]\tLoss: 269.56\n",
      "2023-05-10 15:41:33,357 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:41:36,683 [INFO] Test set accuracy: 605479/614179 (0.9858347485016583)\n",
      "\n",
      "Epoch:  52%|█████▏    | 13/25 [06:07<05:40, 28.36s/it]2023-05-10 15:41:39,302 [INFO] [100000/1433083 (7%)]\tLoss: 136.79\n",
      "2023-05-10 15:41:41,497 [INFO] [200000/1433083 (14%)]\tLoss: 112.70\n",
      "2023-05-10 15:41:43,536 [INFO] [300000/1433083 (21%)]\tLoss: 98.06\n",
      "2023-05-10 15:41:45,446 [INFO] [400000/1433083 (28%)]\tLoss: 89.29\n",
      "2023-05-10 15:41:47,287 [INFO] [500000/1433083 (35%)]\tLoss: 91.95\n",
      "2023-05-10 15:41:49,042 [INFO] [600000/1433083 (42%)]\tLoss: 81.58\n",
      "2023-05-10 15:41:50,799 [INFO] [700000/1433083 (49%)]\tLoss: 73.61\n",
      "2023-05-10 15:41:52,545 [INFO] [800000/1433083 (56%)]\tLoss: 67.78\n",
      "2023-05-10 15:41:54,306 [INFO] [900000/1433083 (63%)]\tLoss: 74.97\n",
      "2023-05-10 15:41:55,896 [INFO] [1000000/1433083 (70%)]\tLoss: 90.44\n",
      "2023-05-10 15:41:57,396 [INFO] [1100000/1433083 (77%)]\tLoss: 93.65\n",
      "2023-05-10 15:41:58,715 [INFO] [1200000/1433083 (84%)]\tLoss: 110.42\n",
      "2023-05-10 15:41:59,889 [INFO] [1300000/1433083 (91%)]\tLoss: 132.60\n",
      "2023-05-10 15:42:00,980 [INFO] [1400000/1433083 (98%)]\tLoss: 240.37\n",
      "2023-05-10 15:42:01,340 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:42:04,637 [INFO] Test set accuracy: 604351/614179 (0.9839981503763561)\n",
      "\n",
      "Epoch:  56%|█████▌    | 14/25 [06:34<05:10, 28.24s/it]2023-05-10 15:42:07,230 [INFO] [100000/1433083 (7%)]\tLoss: 113.72\n",
      "2023-05-10 15:42:09,419 [INFO] [200000/1433083 (14%)]\tLoss: 107.43\n",
      "2023-05-10 15:42:11,479 [INFO] [300000/1433083 (21%)]\tLoss: 93.28\n",
      "2023-05-10 15:42:13,373 [INFO] [400000/1433083 (28%)]\tLoss: 84.03\n",
      "2023-05-10 15:42:15,220 [INFO] [500000/1433083 (35%)]\tLoss: 85.56\n",
      "2023-05-10 15:42:16,958 [INFO] [600000/1433083 (42%)]\tLoss: 75.90\n",
      "2023-05-10 15:42:18,704 [INFO] [700000/1433083 (49%)]\tLoss: 68.45\n",
      "2023-05-10 15:42:20,434 [INFO] [800000/1433083 (56%)]\tLoss: 62.99\n",
      "2023-05-10 15:42:22,205 [INFO] [900000/1433083 (63%)]\tLoss: 69.51\n",
      "2023-05-10 15:42:23,777 [INFO] [1000000/1433083 (70%)]\tLoss: 93.66\n",
      "2023-05-10 15:42:25,277 [INFO] [1100000/1433083 (77%)]\tLoss: 99.95\n",
      "2023-05-10 15:42:26,691 [INFO] [1200000/1433083 (84%)]\tLoss: 118.83\n",
      "2023-05-10 15:42:27,977 [INFO] [1300000/1433083 (91%)]\tLoss: 140.22\n",
      "2023-05-10 15:42:29,173 [INFO] [1400000/1433083 (98%)]\tLoss: 244.95\n",
      "2023-05-10 15:42:29,571 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:42:32,923 [INFO] Test set accuracy: 604442/614179 (0.984146315650649)\n",
      "\n",
      "Epoch:  60%|██████    | 15/25 [07:03<04:42, 28.25s/it]2023-05-10 15:42:35,520 [INFO] [100000/1433083 (7%)]\tLoss: 133.72\n",
      "2023-05-10 15:42:37,742 [INFO] [200000/1433083 (14%)]\tLoss: 112.70\n",
      "2023-05-10 15:42:39,768 [INFO] [300000/1433083 (21%)]\tLoss: 96.04\n",
      "2023-05-10 15:42:41,670 [INFO] [400000/1433083 (28%)]\tLoss: 85.58\n",
      "2023-05-10 15:42:43,523 [INFO] [500000/1433083 (35%)]\tLoss: 85.60\n",
      "2023-05-10 15:42:45,242 [INFO] [600000/1433083 (42%)]\tLoss: 75.67\n",
      "2023-05-10 15:42:46,976 [INFO] [700000/1433083 (49%)]\tLoss: 68.06\n",
      "2023-05-10 15:42:48,706 [INFO] [800000/1433083 (56%)]\tLoss: 62.50\n",
      "2023-05-10 15:42:50,486 [INFO] [900000/1433083 (63%)]\tLoss: 68.27\n",
      "2023-05-10 15:42:52,089 [INFO] [1000000/1433083 (70%)]\tLoss: 86.57\n",
      "2023-05-10 15:42:53,586 [INFO] [1100000/1433083 (77%)]\tLoss: 89.43\n",
      "2023-05-10 15:42:54,987 [INFO] [1200000/1433083 (84%)]\tLoss: 103.36\n",
      "2023-05-10 15:42:56,239 [INFO] [1300000/1433083 (91%)]\tLoss: 120.82\n",
      "2023-05-10 15:42:57,420 [INFO] [1400000/1433083 (98%)]\tLoss: 221.06\n",
      "2023-05-10 15:42:57,829 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:43:01,195 [INFO] Test set accuracy: 605714/614179 (0.9862173731110963)\n",
      "\n",
      "Epoch:  64%|██████▍   | 16/25 [07:31<04:14, 28.26s/it]2023-05-10 15:43:03,809 [INFO] [100000/1433083 (7%)]\tLoss: 130.36\n",
      "2023-05-10 15:43:05,953 [INFO] [200000/1433083 (14%)]\tLoss: 104.78\n",
      "2023-05-10 15:43:07,893 [INFO] [300000/1433083 (21%)]\tLoss: 87.88\n",
      "2023-05-10 15:43:09,690 [INFO] [400000/1433083 (28%)]\tLoss: 77.89\n",
      "2023-05-10 15:43:11,498 [INFO] [500000/1433083 (35%)]\tLoss: 78.03\n",
      "2023-05-10 15:43:13,250 [INFO] [600000/1433083 (42%)]\tLoss: 69.12\n",
      "2023-05-10 15:43:14,997 [INFO] [700000/1433083 (49%)]\tLoss: 62.18\n",
      "2023-05-10 15:43:16,742 [INFO] [800000/1433083 (56%)]\tLoss: 57.02\n",
      "2023-05-10 15:43:18,511 [INFO] [900000/1433083 (63%)]\tLoss: 62.55\n",
      "2023-05-10 15:43:20,092 [INFO] [1000000/1433083 (70%)]\tLoss: 79.83\n",
      "2023-05-10 15:43:21,585 [INFO] [1100000/1433083 (77%)]\tLoss: 81.55\n",
      "2023-05-10 15:43:22,962 [INFO] [1200000/1433083 (84%)]\tLoss: 93.65\n",
      "2023-05-10 15:43:24,223 [INFO] [1300000/1433083 (91%)]\tLoss: 109.93\n",
      "2023-05-10 15:43:25,438 [INFO] [1400000/1433083 (98%)]\tLoss: 197.26\n",
      "2023-05-10 15:43:25,821 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:43:29,116 [INFO] Test set accuracy: 606397/614179 (0.9873294267632075)\n",
      "\n",
      "Epoch:  68%|██████▊   | 17/25 [07:59<03:45, 28.16s/it]2023-05-10 15:43:31,716 [INFO] [100000/1433083 (7%)]\tLoss: 111.55\n",
      "2023-05-10 15:43:33,913 [INFO] [200000/1433083 (14%)]\tLoss: 96.64\n",
      "2023-05-10 15:43:35,943 [INFO] [300000/1433083 (21%)]\tLoss: 84.39\n",
      "2023-05-10 15:43:37,871 [INFO] [400000/1433083 (28%)]\tLoss: 74.24\n",
      "2023-05-10 15:43:39,719 [INFO] [500000/1433083 (35%)]\tLoss: 73.88\n",
      "2023-05-10 15:43:41,356 [INFO] [600000/1433083 (42%)]\tLoss: 65.44\n",
      "2023-05-10 15:43:43,007 [INFO] [700000/1433083 (49%)]\tLoss: 58.83\n",
      "2023-05-10 15:43:44,654 [INFO] [800000/1433083 (56%)]\tLoss: 53.89\n",
      "2023-05-10 15:43:46,315 [INFO] [900000/1433083 (63%)]\tLoss: 58.66\n",
      "2023-05-10 15:43:47,809 [INFO] [1000000/1433083 (70%)]\tLoss: 78.44\n",
      "2023-05-10 15:43:49,227 [INFO] [1100000/1433083 (77%)]\tLoss: 80.77\n",
      "2023-05-10 15:43:50,699 [INFO] [1200000/1433083 (84%)]\tLoss: 91.52\n",
      "2023-05-10 15:43:51,974 [INFO] [1300000/1433083 (91%)]\tLoss: 105.82\n",
      "2023-05-10 15:43:53,159 [INFO] [1400000/1433083 (98%)]\tLoss: 193.05\n",
      "2023-05-10 15:43:53,562 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:43:56,908 [INFO] Test set accuracy: 607018/614179 (0.9883405326460202)\n",
      "\n",
      "Epoch:  72%|███████▏  | 18/25 [08:27<03:16, 28.05s/it]2023-05-10 15:43:59,500 [INFO] [100000/1433083 (7%)]\tLoss: 95.61\n",
      "2023-05-10 15:44:01,729 [INFO] [200000/1433083 (14%)]\tLoss: 74.17\n",
      "2023-05-10 15:44:03,762 [INFO] [300000/1433083 (21%)]\tLoss: 63.89\n",
      "2023-05-10 15:44:05,686 [INFO] [400000/1433083 (28%)]\tLoss: 56.97\n",
      "2023-05-10 15:44:07,542 [INFO] [500000/1433083 (35%)]\tLoss: 60.67\n",
      "2023-05-10 15:44:09,319 [INFO] [600000/1433083 (42%)]\tLoss: 54.22\n",
      "2023-05-10 15:44:10,964 [INFO] [700000/1433083 (49%)]\tLoss: 48.99\n",
      "2023-05-10 15:44:12,602 [INFO] [800000/1433083 (56%)]\tLoss: 45.01\n",
      "2023-05-10 15:44:14,286 [INFO] [900000/1433083 (63%)]\tLoss: 49.74\n",
      "2023-05-10 15:44:15,782 [INFO] [1000000/1433083 (70%)]\tLoss: 66.05\n",
      "2023-05-10 15:44:17,184 [INFO] [1100000/1433083 (77%)]\tLoss: 67.55\n",
      "2023-05-10 15:44:18,478 [INFO] [1200000/1433083 (84%)]\tLoss: 77.10\n",
      "2023-05-10 15:44:19,681 [INFO] [1300000/1433083 (91%)]\tLoss: 91.11\n",
      "2023-05-10 15:44:20,867 [INFO] [1400000/1433083 (98%)]\tLoss: 170.74\n",
      "2023-05-10 15:44:21,274 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:44:24,589 [INFO] Test set accuracy: 607536/614179 (0.9891839349766111)\n",
      "\n",
      "Epoch:  76%|███████▌  | 19/25 [08:54<02:47, 27.94s/it]2023-05-10 15:44:27,130 [INFO] [100000/1433083 (7%)]\tLoss: 135.43\n",
      "2023-05-10 15:44:29,236 [INFO] [200000/1433083 (14%)]\tLoss: 97.00\n",
      "2023-05-10 15:44:31,245 [INFO] [300000/1433083 (21%)]\tLoss: 82.25\n",
      "2023-05-10 15:44:33,163 [INFO] [400000/1433083 (28%)]\tLoss: 71.22\n",
      "2023-05-10 15:44:35,013 [INFO] [500000/1433083 (35%)]\tLoss: 71.46\n",
      "2023-05-10 15:44:36,760 [INFO] [600000/1433083 (42%)]\tLoss: 63.03\n",
      "2023-05-10 15:44:38,504 [INFO] [700000/1433083 (49%)]\tLoss: 56.54\n",
      "2023-05-10 15:44:40,264 [INFO] [800000/1433083 (56%)]\tLoss: 51.66\n",
      "2023-05-10 15:44:42,045 [INFO] [900000/1433083 (63%)]\tLoss: 55.89\n",
      "2023-05-10 15:44:43,645 [INFO] [1000000/1433083 (70%)]\tLoss: 67.47\n",
      "2023-05-10 15:44:45,143 [INFO] [1100000/1433083 (77%)]\tLoss: 68.61\n",
      "2023-05-10 15:44:46,543 [INFO] [1200000/1433083 (84%)]\tLoss: 76.56\n",
      "2023-05-10 15:44:47,824 [INFO] [1300000/1433083 (91%)]\tLoss: 91.26\n",
      "2023-05-10 15:44:49,019 [INFO] [1400000/1433083 (98%)]\tLoss: 170.43\n",
      "2023-05-10 15:44:49,421 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:44:52,728 [INFO] Test set accuracy: 607895/614179 (0.9897684551246461)\n",
      "\n",
      "Epoch:  80%|████████  | 20/25 [09:23<02:19, 28.00s/it]2023-05-10 15:44:55,357 [INFO] [100000/1433083 (7%)]\tLoss: 93.70\n",
      "2023-05-10 15:44:57,552 [INFO] [200000/1433083 (14%)]\tLoss: 70.34\n",
      "2023-05-10 15:44:59,479 [INFO] [300000/1433083 (21%)]\tLoss: 58.62\n",
      "2023-05-10 15:45:01,382 [INFO] [400000/1433083 (28%)]\tLoss: 51.35\n",
      "2023-05-10 15:45:03,225 [INFO] [500000/1433083 (35%)]\tLoss: 54.41\n",
      "2023-05-10 15:45:04,975 [INFO] [600000/1433083 (42%)]\tLoss: 48.55\n",
      "2023-05-10 15:45:06,715 [INFO] [700000/1433083 (49%)]\tLoss: 43.74\n",
      "2023-05-10 15:45:08,463 [INFO] [800000/1433083 (56%)]\tLoss: 40.12\n",
      "2023-05-10 15:45:10,215 [INFO] [900000/1433083 (63%)]\tLoss: 44.34\n",
      "2023-05-10 15:45:11,797 [INFO] [1000000/1433083 (70%)]\tLoss: 56.49\n",
      "2023-05-10 15:45:13,303 [INFO] [1100000/1433083 (77%)]\tLoss: 57.60\n",
      "2023-05-10 15:45:14,690 [INFO] [1200000/1433083 (84%)]\tLoss: 64.92\n",
      "2023-05-10 15:45:15,926 [INFO] [1300000/1433083 (91%)]\tLoss: 77.71\n",
      "2023-05-10 15:45:17,080 [INFO] [1400000/1433083 (98%)]\tLoss: 152.94\n",
      "2023-05-10 15:45:17,466 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:45:20,804 [INFO] Test set accuracy: 607726/614179 (0.9894932910438162)\n",
      "\n",
      "Epoch:  84%|████████▍ | 21/25 [09:51<01:52, 28.02s/it]2023-05-10 15:45:23,418 [INFO] [100000/1433083 (7%)]\tLoss: 66.22\n",
      "2023-05-10 15:45:25,651 [INFO] [200000/1433083 (14%)]\tLoss: 57.24\n",
      "2023-05-10 15:45:27,703 [INFO] [300000/1433083 (21%)]\tLoss: 51.62\n",
      "2023-05-10 15:45:29,610 [INFO] [400000/1433083 (28%)]\tLoss: 45.14\n",
      "2023-05-10 15:45:31,488 [INFO] [500000/1433083 (35%)]\tLoss: 46.70\n",
      "2023-05-10 15:45:33,225 [INFO] [600000/1433083 (42%)]\tLoss: 41.74\n",
      "2023-05-10 15:45:34,978 [INFO] [700000/1433083 (49%)]\tLoss: 37.68\n",
      "2023-05-10 15:45:36,728 [INFO] [800000/1433083 (56%)]\tLoss: 34.53\n",
      "2023-05-10 15:45:38,485 [INFO] [900000/1433083 (63%)]\tLoss: 38.11\n",
      "2023-05-10 15:45:40,070 [INFO] [1000000/1433083 (70%)]\tLoss: 58.79\n",
      "2023-05-10 15:45:41,565 [INFO] [1100000/1433083 (77%)]\tLoss: 59.95\n",
      "2023-05-10 15:45:42,965 [INFO] [1200000/1433083 (84%)]\tLoss: 66.36\n",
      "2023-05-10 15:45:44,222 [INFO] [1300000/1433083 (91%)]\tLoss: 78.20\n",
      "2023-05-10 15:45:45,383 [INFO] [1400000/1433083 (98%)]\tLoss: 143.30\n",
      "2023-05-10 15:45:45,781 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:45:49,095 [INFO] Test set accuracy: 608242/614179 (0.9903334369947523)\n",
      "\n",
      "Epoch:  88%|████████▊ | 22/25 [10:19<01:24, 28.10s/it]2023-05-10 15:45:51,695 [INFO] [100000/1433083 (7%)]\tLoss: 65.03\n",
      "2023-05-10 15:45:53,917 [INFO] [200000/1433083 (14%)]\tLoss: 63.16\n",
      "2023-05-10 15:45:55,934 [INFO] [300000/1433083 (21%)]\tLoss: 59.89\n",
      "2023-05-10 15:45:57,829 [INFO] [400000/1433083 (28%)]\tLoss: 51.53\n",
      "2023-05-10 15:45:59,670 [INFO] [500000/1433083 (35%)]\tLoss: 51.47\n",
      "2023-05-10 15:46:01,415 [INFO] [600000/1433083 (42%)]\tLoss: 45.54\n",
      "2023-05-10 15:46:03,166 [INFO] [700000/1433083 (49%)]\tLoss: 40.81\n",
      "2023-05-10 15:46:04,922 [INFO] [800000/1433083 (56%)]\tLoss: 37.14\n",
      "2023-05-10 15:46:06,665 [INFO] [900000/1433083 (63%)]\tLoss: 40.06\n",
      "2023-05-10 15:46:08,255 [INFO] [1000000/1433083 (70%)]\tLoss: 49.47\n",
      "2023-05-10 15:46:09,752 [INFO] [1100000/1433083 (77%)]\tLoss: 50.68\n",
      "2023-05-10 15:46:11,151 [INFO] [1200000/1433083 (84%)]\tLoss: 56.36\n",
      "2023-05-10 15:46:12,513 [INFO] [1300000/1433083 (91%)]\tLoss: 70.98\n",
      "2023-05-10 15:46:13,695 [INFO] [1400000/1433083 (98%)]\tLoss: 140.10\n",
      "2023-05-10 15:46:14,099 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:46:17,429 [INFO] Test set accuracy: 608289/614179 (0.99040996191664)\n",
      "\n",
      "Epoch:  92%|█████████▏| 23/25 [10:47<00:56, 28.17s/it]2023-05-10 15:46:20,037 [INFO] [100000/1433083 (7%)]\tLoss: 54.33\n",
      "2023-05-10 15:46:22,232 [INFO] [200000/1433083 (14%)]\tLoss: 41.83\n",
      "2023-05-10 15:46:24,280 [INFO] [300000/1433083 (21%)]\tLoss: 36.88\n",
      "2023-05-10 15:46:26,201 [INFO] [400000/1433083 (28%)]\tLoss: 32.48\n",
      "2023-05-10 15:46:28,066 [INFO] [500000/1433083 (35%)]\tLoss: 44.21\n",
      "2023-05-10 15:46:29,813 [INFO] [600000/1433083 (42%)]\tLoss: 43.55\n",
      "2023-05-10 15:46:31,554 [INFO] [700000/1433083 (49%)]\tLoss: 40.88\n",
      "2023-05-10 15:46:33,313 [INFO] [800000/1433083 (56%)]\tLoss: 38.28\n",
      "2023-05-10 15:46:35,078 [INFO] [900000/1433083 (63%)]\tLoss: 42.04\n",
      "2023-05-10 15:46:36,679 [INFO] [1000000/1433083 (70%)]\tLoss: 48.21\n",
      "2023-05-10 15:46:38,181 [INFO] [1100000/1433083 (77%)]\tLoss: 48.49\n",
      "2023-05-10 15:46:39,575 [INFO] [1200000/1433083 (84%)]\tLoss: 53.01\n",
      "2023-05-10 15:46:40,853 [INFO] [1300000/1433083 (91%)]\tLoss: 69.77\n",
      "2023-05-10 15:46:42,044 [INFO] [1400000/1433083 (98%)]\tLoss: 126.59\n",
      "2023-05-10 15:46:42,444 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:46:45,767 [INFO] Test set accuracy: 608518/614179 (0.9907828173871135)\n",
      "\n",
      "Epoch:  96%|█████████▌| 24/25 [11:16<00:28, 28.22s/it]2023-05-10 15:46:48,345 [INFO] [100000/1433083 (7%)]\tLoss: 53.75\n",
      "2023-05-10 15:46:50,545 [INFO] [200000/1433083 (14%)]\tLoss: 39.75\n",
      "2023-05-10 15:46:52,577 [INFO] [300000/1433083 (21%)]\tLoss: 34.24\n",
      "2023-05-10 15:46:54,484 [INFO] [400000/1433083 (28%)]\tLoss: 29.74\n",
      "2023-05-10 15:46:56,343 [INFO] [500000/1433083 (35%)]\tLoss: 43.90\n",
      "2023-05-10 15:46:58,093 [INFO] [600000/1433083 (42%)]\tLoss: 41.30\n",
      "2023-05-10 15:46:59,828 [INFO] [700000/1433083 (49%)]\tLoss: 37.75\n",
      "2023-05-10 15:47:01,557 [INFO] [800000/1433083 (56%)]\tLoss: 34.64\n",
      "2023-05-10 15:47:03,332 [INFO] [900000/1433083 (63%)]\tLoss: 37.36\n",
      "2023-05-10 15:47:04,926 [INFO] [1000000/1433083 (70%)]\tLoss: 42.69\n",
      "2023-05-10 15:47:06,435 [INFO] [1100000/1433083 (77%)]\tLoss: 42.64\n",
      "2023-05-10 15:47:07,839 [INFO] [1200000/1433083 (84%)]\tLoss: 46.44\n",
      "2023-05-10 15:47:09,112 [INFO] [1300000/1433083 (91%)]\tLoss: 57.47\n",
      "2023-05-10 15:47:10,292 [INFO] [1400000/1433083 (98%)]\tLoss: 122.36\n",
      "2023-05-10 15:47:10,693 [INFO] Evaluating trained model ...\n",
      "2023-05-10 15:47:14,038 [INFO] Test set accuracy: 608359/614179 (0.9905239352045576)\n",
      "\n",
      "Epoch: 100%|██████████| 25/25 [11:44<00:00, 28.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 32s, sys: 24.8 s, total: 21min 56s\n",
      "Wall time: 11min 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dd.train_model(train_data, labels, batch_size=BATCH_SIZE, epochs=EPOCHS, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "Save pretrained model to a given output location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 15:47:14,076 [INFO] Pretrained model checkpoint saved to location: 'models/rnn_classifier_2023-05-10_15_47_14.bin'\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODELS_DIR):\n",
    "    print(\"Creating directory '{}'\".format(MODELS_DIR))\n",
    "    os.makedirs(MODELS_DIR)\n",
    "\n",
    "now = datetime.now()\n",
    "model_filename = \"rnn_classifier_{}.bin\".format(now.strftime(\"%Y-%m-%d_%H_%M_%S\"))\n",
    "model_filepath = os.path.join(MODELS_DIR, model_filename)\n",
    "dd.save_checkpoint(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model generated above, we now score the test dataset against the model to determine if the domain is likely generated by a DGA or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 15:47:14,126 [INFO] Found GPU's now setting up cuda for the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9917841541309618\n"
     ]
    }
   ],
   "source": [
    "dga_detector = DGADetector()\n",
    "dga_detector.load_checkpoint(model_filepath)\n",
    "\n",
    "domain_train, domain_test, type_train, type_test = train_test_split(gdf, \"type\", train_size=0.7)\n",
    "test_df = cudf.DataFrame()\n",
    "test_df[\"type\"] = type_test.reset_index(drop=True)\n",
    "test_df[\"domain\"] = domain_test.reset_index(drop=True)\n",
    "\n",
    "test_dataset = DGADataset(test_df, 100)\n",
    "test_dataloader = DataLoader(test_dataset, batchsize=BATCH_SIZE)\n",
    "\n",
    "pred_results = []\n",
    "true_results = []\n",
    "for chunk in test_dataloader.get_chunks():\n",
    "    pred_results.append(list(dga_detector.predict(chunk['domain']).values_host))\n",
    "    true_results.append(list(chunk['type'].values_host))\n",
    "pred_results = np.concatenate(pred_results)\n",
    "true_results = np.concatenate(true_results)\n",
    "accuracy_score_result = accuracy_score(pred_results, true_results)\n",
    "\n",
    "print('Model accuracy: %s'%(accuracy_score_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.978\n"
     ]
    }
   ],
   "source": [
    "average_precision = average_precision_score(true_results, pred_results)\n",
    "\n",
    "print('Average precision score: {0:0.3f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(df, pad_max_len):\n",
    "    df = str2ascii(df[0:32], 'domain')\n",
    "    df = df.drop(\"domain\", axis=1)\n",
    "    seq_len_arr = df[\"len\"].values_host\n",
    "    df = df.drop(\"len\", axis=1)\n",
    "    seq_len_tensor = torch.LongTensor(seq_len_arr)\n",
    "\n",
    "    seq_cp = df.to_cupy()\n",
    "    input = cp.zeros((seq_cp.shape[0], pad_max_len))\n",
    "    input[:seq_cp.shape[0], :seq_cp.shape[1]] = seq_cp\n",
    "    input = input.astype(\"long\")\n",
    "    seq_tensor = torch.as_tensor(input)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        seq_tensor = seq_tensor.cuda()\n",
    "        seq_len_tensor = seq_len_tensor.cuda()\n",
    "\n",
    "    return seq_tensor, seq_len_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mor_exp/lib/python3.8/site-packages/cudf/core/column/string.py:3563: FutureWarning: The expand parameter is deprecated and will be removed in a future version. Set expand=False to match future behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input, seq_lengths = preprocess(gdf[0:32], 100)\n",
    "sample_model_input = (input, seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/my_data/gitrepos/efajardo-nv/morpheus-experimental/dga-detection/training-tuning-inference/rnn_classifier.py:57: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  gru_input = pack_padded_sequence(embedded, seq_lengths.data.cpu().numpy())\n",
      "/my_data/gitrepos/efajardo-nv/morpheus-experimental/dga-detection/training-tuning-inference/rnn_classifier.py:57: UserWarning: pack_padded_sequence has been called with a Python list of sequence lengths. The tracer cannot track the data flow of Python values, and it will treat them as constants, likely rendering the trace incorrect for any other combination of lengths.\n",
      "  gru_input = pack_padded_sequence(embedded, seq_lengths.data.cpu().numpy())\n",
      "/opt/conda/envs/mor_exp/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::PackPadded type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/opt/conda/envs/mor_exp/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:4315: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(dga_detector.model,              \n",
    "                  sample_model_input,               \n",
    "                  \"model.onnx\",                                      # where to save the model\n",
    "                  export_params=True,                                # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,                                  # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,                          # whether to execute constant folding for optimization\n",
    "                  input_names = ['domains', \"seq_lengths\"],          # the model's input names\n",
    "                  output_names = ['output'],                         # the model's output names\n",
    "                  dynamic_axes={'domains' : {0 : 'batch_size'},      # variable length axes\n",
    "                                'seq_lengths': {0: 'batch_size'}, \n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DGA detection implementation enables users to train their models for detection and also use existing models. This capability could also be used in conjunction with log parsing efforts if the logs contain domain names. Data is kept in GPU memory, removing unnecessary copy/converts and providing a 4X speed advantage over CPU only implementations. This is esepcially true with large batch sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
