{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Generation Algorithm (DGA) Detection\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Data Importing\n",
    "* Data Preprocessing\n",
    "* Training and Evaluation\n",
    "* Inference\n",
    "* Conclusion\n",
    "\n",
    "## Introduction\n",
    "[Domain Generation Algorithms](https://en.wikipedia.org/wiki/Domain_generation_algorithm) (DGAs) are used to generate domain names that can be used by the malware to communicate with the command and control servers. IP addresses and static domain names can be easily blocked, and a DGA provides an easy method to generate a large number of domain names and rotate through them to circumvent traditional block lists. We will use a type of recurrent neural network called the [Gated Recurrent Unit](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) (GRU) for this example. This implementation enables users to train their models with up-to-date domain names representative of both benign and DGA generated strings. This capability could also be used in production. This notebook provides a view into the data science workflow to create a DGA detection implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import torch\n",
    "import requests\n",
    "import logging\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from dga_detector import DGADetector\n",
    "from dataloader import DataLoader\n",
    "from dga_dataset import DGADataset\n",
    "from cuml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable console logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Input Dataset to GPU Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV = \"../datasets/benign_and_dga_domains.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = cudf.read_csv(INPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = gdf['domain']\n",
    "labels = gdf['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have only benign and DGA (malicious) categoriesm, the number of domain types need to be set to 2 (`N_DOMAIN_TYPE=2`). Vocabulary size(`CHAR_VOCAB`) is set to 128 ASCII characters. The values below set for `HIDDEN_SIZE`, `N_LAYERS` of the network, and the `LR` (Learning Rate) give an optimum balance for the network size and performance. They might need be set via experiments when working with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "N_LAYERS = 3\n",
    "CHAR_VOCAB = 128\n",
    "HIDDEN_SIZE = 100\n",
    "N_DOMAIN_TYPE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate DGA detector\n",
    "Now that the data is ready, the datasets are created, and we've set the parameters for the model, we can use the DGA detector method to create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 15:33:35,343 [INFO] Found GPU's now setting up cuda for the model\n"
     ]
    }
   ],
   "source": [
    "dd = DGADetector(lr=LR)\n",
    "dd.init_model(n_layers=N_LAYERS, char_vocab=CHAR_VOCAB, hidden_size=HIDDEN_SIZE, n_domain_type=N_DOMAIN_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "TRAIN_SIZE = 0.7\n",
    "BATCH_SIZE = 10000\n",
    "MODELS_DIR = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Now we train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 15:33:37,445 [INFO] Initiating model training ...\n",
      "2023-05-03 15:33:37,446 [INFO] Truncate domains to width: 100\n",
      "/opt/conda/envs/mor_exp/lib/python3.8/site-packages/cudf/core/column/string.py:3563: FutureWarning: The expand parameter is deprecated and will be removed in a future version. Set expand=False to match future behavior.\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/25 [00:00<?, ?it/s]2023-05-03 15:33:41,081 [INFO] [100000/1433083 (7%)]\tLoss: 6523.70\n",
      "2023-05-03 15:33:43,133 [INFO] [200000/1433083 (14%)]\tLoss: 4980.15\n",
      "2023-05-03 15:33:45,004 [INFO] [300000/1433083 (21%)]\tLoss: 3953.82\n",
      "2023-05-03 15:33:46,818 [INFO] [400000/1433083 (28%)]\tLoss: 3448.07\n",
      "2023-05-03 15:33:48,529 [INFO] [500000/1433083 (35%)]\tLoss: 3185.66\n",
      "2023-05-03 15:33:50,149 [INFO] [600000/1433083 (42%)]\tLoss: 2764.20\n",
      "2023-05-03 15:33:51,765 [INFO] [700000/1433083 (49%)]\tLoss: 2431.07\n",
      "2023-05-03 15:33:53,390 [INFO] [800000/1433083 (56%)]\tLoss: 2163.64\n",
      "2023-05-03 15:33:55,033 [INFO] [900000/1433083 (63%)]\tLoss: 2227.37\n",
      "2023-05-03 15:33:56,505 [INFO] [1000000/1433083 (70%)]\tLoss: 2126.80\n",
      "2023-05-03 15:33:57,866 [INFO] [1100000/1433083 (77%)]\tLoss: 2108.05\n",
      "2023-05-03 15:33:59,143 [INFO] [1200000/1433083 (84%)]\tLoss: 2033.37\n",
      "2023-05-03 15:34:00,307 [INFO] [1300000/1433083 (91%)]\tLoss: 2042.40\n",
      "2023-05-03 15:34:01,392 [INFO] [1400000/1433083 (98%)]\tLoss: 2082.71\n",
      "2023-05-03 15:34:01,752 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:34:04,511 [INFO] Test set accuracy: 375477/614179 (0.6113478318210164)\n",
      "\n",
      "Epoch:   4%|▍         | 1/25 [00:26<10:26, 26.09s/it]2023-05-03 15:34:06,994 [INFO] [100000/1433083 (7%)]\tLoss: 3422.95\n",
      "2023-05-03 15:34:09,055 [INFO] [200000/1433083 (14%)]\tLoss: 2582.62\n",
      "2023-05-03 15:34:10,971 [INFO] [300000/1433083 (21%)]\tLoss: 1937.97\n",
      "2023-05-03 15:34:12,792 [INFO] [400000/1433083 (28%)]\tLoss: 1666.57\n",
      "2023-05-03 15:34:14,559 [INFO] [500000/1433083 (35%)]\tLoss: 1458.50\n",
      "2023-05-03 15:34:16,178 [INFO] [600000/1433083 (42%)]\tLoss: 1251.41\n",
      "2023-05-03 15:34:17,816 [INFO] [700000/1433083 (49%)]\tLoss: 1096.94\n",
      "2023-05-03 15:34:19,340 [INFO] [800000/1433083 (56%)]\tLoss: 976.31\n",
      "2023-05-03 15:34:20,890 [INFO] [900000/1433083 (63%)]\tLoss: 930.00\n",
      "2023-05-03 15:34:22,372 [INFO] [1000000/1433083 (70%)]\tLoss: 920.70\n",
      "2023-05-03 15:34:23,782 [INFO] [1100000/1433083 (77%)]\tLoss: 954.97\n",
      "2023-05-03 15:34:25,074 [INFO] [1200000/1433083 (84%)]\tLoss: 948.43\n",
      "2023-05-03 15:34:26,237 [INFO] [1300000/1433083 (91%)]\tLoss: 984.57\n",
      "2023-05-03 15:34:27,326 [INFO] [1400000/1433083 (98%)]\tLoss: 1080.29\n",
      "2023-05-03 15:34:27,682 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:34:30,459 [INFO] Test set accuracy: 492266/614179 (0.8015024935727206)\n",
      "\n",
      "Epoch:   8%|▊         | 2/25 [00:52<09:58, 26.01s/it]2023-05-03 15:34:32,927 [INFO] [100000/1433083 (7%)]\tLoss: 2696.58\n",
      "2023-05-03 15:34:35,043 [INFO] [200000/1433083 (14%)]\tLoss: 1892.27\n",
      "2023-05-03 15:34:36,968 [INFO] [300000/1433083 (21%)]\tLoss: 1401.22\n",
      "2023-05-03 15:34:38,791 [INFO] [400000/1433083 (28%)]\tLoss: 1200.75\n",
      "2023-05-03 15:34:40,426 [INFO] [500000/1433083 (35%)]\tLoss: 1049.29\n",
      "2023-05-03 15:34:42,038 [INFO] [600000/1433083 (42%)]\tLoss: 896.59\n",
      "2023-05-03 15:34:43,685 [INFO] [700000/1433083 (49%)]\tLoss: 784.49\n",
      "2023-05-03 15:34:45,226 [INFO] [800000/1433083 (56%)]\tLoss: 697.82\n",
      "2023-05-03 15:34:46,866 [INFO] [900000/1433083 (63%)]\tLoss: 679.20\n",
      "2023-05-03 15:34:48,355 [INFO] [1000000/1433083 (70%)]\tLoss: 684.12\n",
      "2023-05-03 15:34:49,776 [INFO] [1100000/1433083 (77%)]\tLoss: 668.91\n",
      "2023-05-03 15:34:51,075 [INFO] [1200000/1433083 (84%)]\tLoss: 688.86\n",
      "2023-05-03 15:34:52,252 [INFO] [1300000/1433083 (91%)]\tLoss: 725.82\n",
      "2023-05-03 15:34:53,361 [INFO] [1400000/1433083 (98%)]\tLoss: 829.06\n",
      "2023-05-03 15:34:53,727 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:34:56,674 [INFO] Test set accuracy: 574571/614179 (0.9355106573165152)\n",
      "\n",
      "Epoch:  12%|█▏        | 3/25 [01:18<09:34, 26.10s/it]2023-05-03 15:34:59,170 [INFO] [100000/1433083 (7%)]\tLoss: 2079.51\n",
      "2023-05-03 15:35:01,296 [INFO] [200000/1433083 (14%)]\tLoss: 1459.88\n",
      "2023-05-03 15:35:03,228 [INFO] [300000/1433083 (21%)]\tLoss: 1092.32\n",
      "2023-05-03 15:35:05,061 [INFO] [400000/1433083 (28%)]\tLoss: 939.51\n",
      "2023-05-03 15:35:06,807 [INFO] [500000/1433083 (35%)]\tLoss: 816.49\n",
      "2023-05-03 15:35:08,453 [INFO] [600000/1433083 (42%)]\tLoss: 697.49\n",
      "2023-05-03 15:35:10,119 [INFO] [700000/1433083 (49%)]\tLoss: 610.69\n",
      "2023-05-03 15:35:11,784 [INFO] [800000/1433083 (56%)]\tLoss: 543.36\n",
      "2023-05-03 15:35:13,460 [INFO] [900000/1433083 (63%)]\tLoss: 535.62\n",
      "2023-05-03 15:35:14,978 [INFO] [1000000/1433083 (70%)]\tLoss: 551.06\n",
      "2023-05-03 15:35:16,382 [INFO] [1100000/1433083 (77%)]\tLoss: 543.82\n",
      "2023-05-03 15:35:17,711 [INFO] [1200000/1433083 (84%)]\tLoss: 571.21\n",
      "2023-05-03 15:35:18,880 [INFO] [1300000/1433083 (91%)]\tLoss: 607.51\n",
      "2023-05-03 15:35:19,969 [INFO] [1400000/1433083 (98%)]\tLoss: 713.54\n",
      "2023-05-03 15:35:20,334 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:35:23,144 [INFO] Test set accuracy: 590350/614179 (0.9612018646029903)\n",
      "\n",
      "Epoch:  16%|█▌        | 4/25 [01:44<09:11, 26.25s/it]2023-05-03 15:35:25,641 [INFO] [100000/1433083 (7%)]\tLoss: 350.97\n",
      "2023-05-03 15:35:27,763 [INFO] [200000/1433083 (14%)]\tLoss: 399.39\n",
      "2023-05-03 15:35:29,716 [INFO] [300000/1433083 (21%)]\tLoss: 342.97\n",
      "2023-05-03 15:35:31,582 [INFO] [400000/1433083 (28%)]\tLoss: 327.49\n",
      "2023-05-03 15:35:33,340 [INFO] [500000/1433083 (35%)]\tLoss: 315.26\n",
      "2023-05-03 15:35:34,995 [INFO] [600000/1433083 (42%)]\tLoss: 275.67\n",
      "2023-05-03 15:35:36,665 [INFO] [700000/1433083 (49%)]\tLoss: 246.61\n",
      "2023-05-03 15:35:38,320 [INFO] [800000/1433083 (56%)]\tLoss: 223.26\n",
      "2023-05-03 15:35:40,000 [INFO] [900000/1433083 (63%)]\tLoss: 242.88\n",
      "2023-05-03 15:35:41,499 [INFO] [1000000/1433083 (70%)]\tLoss: 276.60\n",
      "2023-05-03 15:35:42,921 [INFO] [1100000/1433083 (77%)]\tLoss: 284.15\n",
      "2023-05-03 15:35:44,225 [INFO] [1200000/1433083 (84%)]\tLoss: 323.55\n",
      "2023-05-03 15:35:45,402 [INFO] [1300000/1433083 (91%)]\tLoss: 366.25\n",
      "2023-05-03 15:35:46,504 [INFO] [1400000/1433083 (98%)]\tLoss: 521.83\n",
      "2023-05-03 15:35:46,880 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:35:49,689 [INFO] Test set accuracy: 588906/614179 (0.9588507584922311)\n",
      "\n",
      "Epoch:  20%|██        | 5/25 [02:11<08:47, 26.35s/it]2023-05-03 15:35:52,201 [INFO] [100000/1433083 (7%)]\tLoss: 253.52\n",
      "2023-05-03 15:35:54,318 [INFO] [200000/1433083 (14%)]\tLoss: 249.72\n",
      "2023-05-03 15:35:56,249 [INFO] [300000/1433083 (21%)]\tLoss: 229.44\n",
      "2023-05-03 15:35:58,110 [INFO] [400000/1433083 (28%)]\tLoss: 225.08\n",
      "2023-05-03 15:35:59,854 [INFO] [500000/1433083 (35%)]\tLoss: 225.21\n",
      "2023-05-03 15:36:01,523 [INFO] [600000/1433083 (42%)]\tLoss: 198.50\n",
      "2023-05-03 15:36:03,176 [INFO] [700000/1433083 (49%)]\tLoss: 178.95\n",
      "2023-05-03 15:36:04,828 [INFO] [800000/1433083 (56%)]\tLoss: 162.94\n",
      "2023-05-03 15:36:06,494 [INFO] [900000/1433083 (63%)]\tLoss: 182.20\n",
      "2023-05-03 15:36:07,992 [INFO] [1000000/1433083 (70%)]\tLoss: 218.78\n",
      "2023-05-03 15:36:09,413 [INFO] [1100000/1433083 (77%)]\tLoss: 227.23\n",
      "2023-05-03 15:36:10,627 [INFO] [1200000/1433083 (84%)]\tLoss: 262.72\n",
      "2023-05-03 15:36:11,702 [INFO] [1300000/1433083 (91%)]\tLoss: 304.23\n",
      "2023-05-03 15:36:12,696 [INFO] [1400000/1433083 (98%)]\tLoss: 432.93\n",
      "2023-05-03 15:36:13,061 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:36:15,874 [INFO] Test set accuracy: 594231/614179 (0.9675208693231127)\n",
      "\n",
      "Epoch:  24%|██▍       | 6/25 [02:37<08:19, 26.30s/it]2023-05-03 15:36:18,403 [INFO] [100000/1433083 (7%)]\tLoss: 208.64\n",
      "2023-05-03 15:36:20,523 [INFO] [200000/1433083 (14%)]\tLoss: 205.36\n",
      "2023-05-03 15:36:22,449 [INFO] [300000/1433083 (21%)]\tLoss: 191.38\n",
      "2023-05-03 15:36:24,326 [INFO] [400000/1433083 (28%)]\tLoss: 208.93\n",
      "2023-05-03 15:36:26,084 [INFO] [500000/1433083 (35%)]\tLoss: 207.43\n",
      "2023-05-03 15:36:27,694 [INFO] [600000/1433083 (42%)]\tLoss: 182.57\n",
      "2023-05-03 15:36:29,345 [INFO] [700000/1433083 (49%)]\tLoss: 164.43\n",
      "2023-05-03 15:36:30,895 [INFO] [800000/1433083 (56%)]\tLoss: 149.68\n",
      "2023-05-03 15:36:32,445 [INFO] [900000/1433083 (63%)]\tLoss: 166.28\n",
      "2023-05-03 15:36:33,960 [INFO] [1000000/1433083 (70%)]\tLoss: 211.56\n",
      "2023-05-03 15:36:35,374 [INFO] [1100000/1433083 (77%)]\tLoss: 218.14\n",
      "2023-05-03 15:36:36,680 [INFO] [1200000/1433083 (84%)]\tLoss: 247.97\n",
      "2023-05-03 15:36:37,852 [INFO] [1300000/1433083 (91%)]\tLoss: 286.16\n",
      "2023-05-03 15:36:38,955 [INFO] [1400000/1433083 (98%)]\tLoss: 420.45\n",
      "2023-05-03 15:36:39,324 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:36:42,117 [INFO] Test set accuracy: 595370/614179 (0.9693753775365163)\n",
      "\n",
      "Epoch:  28%|██▊       | 7/25 [03:03<07:53, 26.28s/it]2023-05-03 15:36:44,645 [INFO] [100000/1433083 (7%)]\tLoss: 182.40\n",
      "2023-05-03 15:36:46,771 [INFO] [200000/1433083 (14%)]\tLoss: 180.40\n",
      "2023-05-03 15:36:48,722 [INFO] [300000/1433083 (21%)]\tLoss: 167.29\n",
      "2023-05-03 15:36:50,586 [INFO] [400000/1433083 (28%)]\tLoss: 162.20\n",
      "2023-05-03 15:36:52,316 [INFO] [500000/1433083 (35%)]\tLoss: 165.34\n",
      "2023-05-03 15:36:53,954 [INFO] [600000/1433083 (42%)]\tLoss: 146.36\n",
      "2023-05-03 15:36:55,592 [INFO] [700000/1433083 (49%)]\tLoss: 132.55\n",
      "2023-05-03 15:36:57,239 [INFO] [800000/1433083 (56%)]\tLoss: 121.08\n",
      "2023-05-03 15:36:58,890 [INFO] [900000/1433083 (63%)]\tLoss: 135.63\n",
      "2023-05-03 15:37:00,391 [INFO] [1000000/1433083 (70%)]\tLoss: 202.84\n",
      "2023-05-03 15:37:01,804 [INFO] [1100000/1433083 (77%)]\tLoss: 209.97\n",
      "2023-05-03 15:37:03,093 [INFO] [1200000/1433083 (84%)]\tLoss: 237.21\n",
      "2023-05-03 15:37:04,158 [INFO] [1300000/1433083 (91%)]\tLoss: 271.30\n",
      "2023-05-03 15:37:05,128 [INFO] [1400000/1433083 (98%)]\tLoss: 387.27\n",
      "2023-05-03 15:37:05,444 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:37:08,351 [INFO] Test set accuracy: 598043/614179 (0.9737275289451447)\n",
      "\n",
      "Epoch:  32%|███▏      | 8/25 [03:29<07:26, 26.26s/it]2023-05-03 15:37:10,830 [INFO] [100000/1433083 (7%)]\tLoss: 220.19\n",
      "2023-05-03 15:37:12,952 [INFO] [200000/1433083 (14%)]\tLoss: 193.86\n",
      "2023-05-03 15:37:14,884 [INFO] [300000/1433083 (21%)]\tLoss: 170.40\n",
      "2023-05-03 15:37:16,716 [INFO] [400000/1433083 (28%)]\tLoss: 160.37\n",
      "2023-05-03 15:37:18,457 [INFO] [500000/1433083 (35%)]\tLoss: 160.24\n",
      "2023-05-03 15:37:20,115 [INFO] [600000/1433083 (42%)]\tLoss: 141.35\n",
      "2023-05-03 15:37:21,768 [INFO] [700000/1433083 (49%)]\tLoss: 127.64\n",
      "2023-05-03 15:37:23,427 [INFO] [800000/1433083 (56%)]\tLoss: 116.26\n",
      "2023-05-03 15:37:25,097 [INFO] [900000/1433083 (63%)]\tLoss: 128.64\n",
      "2023-05-03 15:37:26,585 [INFO] [1000000/1433083 (70%)]\tLoss: 154.54\n",
      "2023-05-03 15:37:27,993 [INFO] [1100000/1433083 (77%)]\tLoss: 159.98\n",
      "2023-05-03 15:37:29,295 [INFO] [1200000/1433083 (84%)]\tLoss: 189.54\n",
      "2023-05-03 15:37:30,468 [INFO] [1300000/1433083 (91%)]\tLoss: 225.59\n",
      "2023-05-03 15:37:31,575 [INFO] [1400000/1433083 (98%)]\tLoss: 353.18\n",
      "2023-05-03 15:37:31,943 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:37:34,736 [INFO] Test set accuracy: 600344/614179 (0.9774739937379819)\n",
      "\n",
      "Epoch:  36%|███▌      | 9/25 [03:56<07:00, 26.30s/it]2023-05-03 15:37:37,221 [INFO] [100000/1433083 (7%)]\tLoss: 174.72\n",
      "2023-05-03 15:37:39,358 [INFO] [200000/1433083 (14%)]\tLoss: 159.78\n",
      "2023-05-03 15:37:41,282 [INFO] [300000/1433083 (21%)]\tLoss: 144.48\n",
      "2023-05-03 15:37:43,133 [INFO] [400000/1433083 (28%)]\tLoss: 137.12\n",
      "2023-05-03 15:37:44,904 [INFO] [500000/1433083 (35%)]\tLoss: 138.41\n",
      "2023-05-03 15:37:46,552 [INFO] [600000/1433083 (42%)]\tLoss: 122.63\n",
      "2023-05-03 15:37:48,194 [INFO] [700000/1433083 (49%)]\tLoss: 111.00\n",
      "2023-05-03 15:37:49,824 [INFO] [800000/1433083 (56%)]\tLoss: 101.34\n",
      "2023-05-03 15:37:51,493 [INFO] [900000/1433083 (63%)]\tLoss: 112.70\n",
      "2023-05-03 15:37:53,003 [INFO] [1000000/1433083 (70%)]\tLoss: 152.53\n",
      "2023-05-03 15:37:54,428 [INFO] [1100000/1433083 (77%)]\tLoss: 157.85\n",
      "2023-05-03 15:37:55,736 [INFO] [1200000/1433083 (84%)]\tLoss: 180.92\n",
      "2023-05-03 15:37:56,901 [INFO] [1300000/1433083 (91%)]\tLoss: 210.30\n",
      "2023-05-03 15:37:58,004 [INFO] [1400000/1433083 (98%)]\tLoss: 329.99\n",
      "2023-05-03 15:37:58,370 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:38:01,191 [INFO] Test set accuracy: 598058/614179 (0.9737519517925556)\n",
      "\n",
      "Epoch:  40%|████      | 10/25 [04:22<06:35, 26.35s/it]2023-05-03 15:38:03,698 [INFO] [100000/1433083 (7%)]\tLoss: 168.59\n",
      "2023-05-03 15:38:05,801 [INFO] [200000/1433083 (14%)]\tLoss: 149.88\n",
      "2023-05-03 15:38:07,744 [INFO] [300000/1433083 (21%)]\tLoss: 132.65\n",
      "2023-05-03 15:38:09,602 [INFO] [400000/1433083 (28%)]\tLoss: 125.09\n",
      "2023-05-03 15:38:11,339 [INFO] [500000/1433083 (35%)]\tLoss: 125.93\n",
      "2023-05-03 15:38:12,938 [INFO] [600000/1433083 (42%)]\tLoss: 111.52\n",
      "2023-05-03 15:38:14,609 [INFO] [700000/1433083 (49%)]\tLoss: 100.94\n",
      "2023-05-03 15:38:16,191 [INFO] [800000/1433083 (56%)]\tLoss: 92.12\n",
      "2023-05-03 15:38:17,783 [INFO] [900000/1433083 (63%)]\tLoss: 102.16\n",
      "2023-05-03 15:38:19,263 [INFO] [1000000/1433083 (70%)]\tLoss: 125.77\n",
      "2023-05-03 15:38:20,671 [INFO] [1100000/1433083 (77%)]\tLoss: 130.56\n",
      "2023-05-03 15:38:21,972 [INFO] [1200000/1433083 (84%)]\tLoss: 153.83\n",
      "2023-05-03 15:38:23,149 [INFO] [1300000/1433083 (91%)]\tLoss: 181.92\n",
      "2023-05-03 15:38:24,148 [INFO] [1400000/1433083 (98%)]\tLoss: 300.90\n",
      "2023-05-03 15:38:24,465 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:38:27,278 [INFO] Test set accuracy: 602320/614179 (0.9806912968369156)\n",
      "\n",
      "Epoch:  44%|████▍     | 11/25 [04:48<06:07, 26.27s/it]2023-05-03 15:38:29,842 [INFO] [100000/1433083 (7%)]\tLoss: 162.78\n",
      "2023-05-03 15:38:31,966 [INFO] [200000/1433083 (14%)]\tLoss: 146.61\n",
      "2023-05-03 15:38:33,913 [INFO] [300000/1433083 (21%)]\tLoss: 128.46\n",
      "2023-05-03 15:38:35,779 [INFO] [400000/1433083 (28%)]\tLoss: 119.35\n",
      "2023-05-03 15:38:37,533 [INFO] [500000/1433083 (35%)]\tLoss: 119.72\n",
      "2023-05-03 15:38:39,174 [INFO] [600000/1433083 (42%)]\tLoss: 105.90\n",
      "2023-05-03 15:38:40,827 [INFO] [700000/1433083 (49%)]\tLoss: 95.63\n",
      "2023-05-03 15:38:42,485 [INFO] [800000/1433083 (56%)]\tLoss: 87.17\n",
      "2023-05-03 15:38:44,151 [INFO] [900000/1433083 (63%)]\tLoss: 95.68\n",
      "2023-05-03 15:38:45,640 [INFO] [1000000/1433083 (70%)]\tLoss: 115.29\n",
      "2023-05-03 15:38:47,046 [INFO] [1100000/1433083 (77%)]\tLoss: 119.29\n",
      "2023-05-03 15:38:48,342 [INFO] [1200000/1433083 (84%)]\tLoss: 140.49\n",
      "2023-05-03 15:38:49,514 [INFO] [1300000/1433083 (91%)]\tLoss: 166.30\n",
      "2023-05-03 15:38:50,619 [INFO] [1400000/1433083 (98%)]\tLoss: 278.59\n",
      "2023-05-03 15:38:50,987 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:38:53,772 [INFO] Test set accuracy: 604535/614179 (0.9842977373045969)\n",
      "\n",
      "Epoch:  48%|████▊     | 12/25 [05:15<05:42, 26.34s/it]2023-05-03 15:38:56,290 [INFO] [100000/1433083 (7%)]\tLoss: 134.54\n",
      "2023-05-03 15:38:58,405 [INFO] [200000/1433083 (14%)]\tLoss: 119.71\n",
      "2023-05-03 15:39:00,318 [INFO] [300000/1433083 (21%)]\tLoss: 104.99\n",
      "2023-05-03 15:39:02,164 [INFO] [400000/1433083 (28%)]\tLoss: 97.60\n",
      "2023-05-03 15:39:03,814 [INFO] [500000/1433083 (35%)]\tLoss: 104.91\n",
      "2023-05-03 15:39:05,421 [INFO] [600000/1433083 (42%)]\tLoss: 94.34\n",
      "2023-05-03 15:39:07,066 [INFO] [700000/1433083 (49%)]\tLoss: 85.87\n",
      "2023-05-03 15:39:08,725 [INFO] [800000/1433083 (56%)]\tLoss: 78.43\n",
      "2023-05-03 15:39:10,396 [INFO] [900000/1433083 (63%)]\tLoss: 86.48\n",
      "2023-05-03 15:39:11,876 [INFO] [1000000/1433083 (70%)]\tLoss: 105.71\n",
      "2023-05-03 15:39:13,306 [INFO] [1100000/1433083 (77%)]\tLoss: 108.91\n",
      "2023-05-03 15:39:14,628 [INFO] [1200000/1433083 (84%)]\tLoss: 127.52\n",
      "2023-05-03 15:39:15,803 [INFO] [1300000/1433083 (91%)]\tLoss: 151.57\n",
      "2023-05-03 15:39:16,802 [INFO] [1400000/1433083 (98%)]\tLoss: 259.22\n",
      "2023-05-03 15:39:17,128 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:39:20,024 [INFO] Test set accuracy: 604867/614179 (0.9848382963272923)\n",
      "\n",
      "Epoch:  52%|█████▏    | 13/25 [05:41<05:15, 26.31s/it]2023-05-03 15:39:22,511 [INFO] [100000/1433083 (7%)]\tLoss: 127.36\n",
      "2023-05-03 15:39:24,524 [INFO] [200000/1433083 (14%)]\tLoss: 110.30\n",
      "2023-05-03 15:39:26,355 [INFO] [300000/1433083 (21%)]\tLoss: 96.49\n",
      "2023-05-03 15:39:28,095 [INFO] [400000/1433083 (28%)]\tLoss: 89.09\n",
      "2023-05-03 15:39:29,739 [INFO] [500000/1433083 (35%)]\tLoss: 96.32\n",
      "2023-05-03 15:39:31,360 [INFO] [600000/1433083 (42%)]\tLoss: 86.76\n",
      "2023-05-03 15:39:33,005 [INFO] [700000/1433083 (49%)]\tLoss: 79.00\n",
      "2023-05-03 15:39:34,641 [INFO] [800000/1433083 (56%)]\tLoss: 72.14\n",
      "2023-05-03 15:39:36,281 [INFO] [900000/1433083 (63%)]\tLoss: 79.15\n",
      "2023-05-03 15:39:37,792 [INFO] [1000000/1433083 (70%)]\tLoss: 95.31\n",
      "2023-05-03 15:39:39,211 [INFO] [1100000/1433083 (77%)]\tLoss: 97.86\n",
      "2023-05-03 15:39:40,528 [INFO] [1200000/1433083 (84%)]\tLoss: 115.13\n",
      "2023-05-03 15:39:41,688 [INFO] [1300000/1433083 (91%)]\tLoss: 138.11\n",
      "2023-05-03 15:39:42,774 [INFO] [1400000/1433083 (98%)]\tLoss: 238.66\n",
      "2023-05-03 15:39:43,151 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:39:45,940 [INFO] Test set accuracy: 605856/614179 (0.9864485760665864)\n",
      "\n",
      "Epoch:  56%|█████▌    | 14/25 [06:07<04:48, 26.19s/it]2023-05-03 15:39:48,441 [INFO] [100000/1433083 (7%)]\tLoss: 121.18\n",
      "2023-05-03 15:39:50,555 [INFO] [200000/1433083 (14%)]\tLoss: 106.71\n",
      "2023-05-03 15:39:52,483 [INFO] [300000/1433083 (21%)]\tLoss: 90.96\n",
      "2023-05-03 15:39:54,348 [INFO] [400000/1433083 (28%)]\tLoss: 82.78\n",
      "2023-05-03 15:39:56,096 [INFO] [500000/1433083 (35%)]\tLoss: 84.86\n",
      "2023-05-03 15:39:57,739 [INFO] [600000/1433083 (42%)]\tLoss: 75.46\n",
      "2023-05-03 15:39:59,388 [INFO] [700000/1433083 (49%)]\tLoss: 68.52\n",
      "2023-05-03 15:40:01,043 [INFO] [800000/1433083 (56%)]\tLoss: 62.57\n",
      "2023-05-03 15:40:02,717 [INFO] [900000/1433083 (63%)]\tLoss: 68.98\n",
      "2023-05-03 15:40:04,206 [INFO] [1000000/1433083 (70%)]\tLoss: 96.24\n",
      "2023-05-03 15:40:05,594 [INFO] [1100000/1433083 (77%)]\tLoss: 99.34\n",
      "2023-05-03 15:40:06,883 [INFO] [1200000/1433083 (84%)]\tLoss: 113.77\n",
      "2023-05-03 15:40:08,048 [INFO] [1300000/1433083 (91%)]\tLoss: 133.61\n",
      "2023-05-03 15:40:09,060 [INFO] [1400000/1433083 (98%)]\tLoss: 228.47\n",
      "2023-05-03 15:40:09,386 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:40:12,176 [INFO] Test set accuracy: 606488/614179 (0.9874775920375005)\n",
      "\n",
      "Epoch:  60%|██████    | 15/25 [06:33<04:22, 26.21s/it]2023-05-03 15:40:14,670 [INFO] [100000/1433083 (7%)]\tLoss: 119.77\n",
      "2023-05-03 15:40:16,754 [INFO] [200000/1433083 (14%)]\tLoss: 95.88\n",
      "2023-05-03 15:40:18,681 [INFO] [300000/1433083 (21%)]\tLoss: 81.33\n",
      "2023-05-03 15:40:20,567 [INFO] [400000/1433083 (28%)]\tLoss: 73.73\n",
      "2023-05-03 15:40:22,316 [INFO] [500000/1433083 (35%)]\tLoss: 75.98\n",
      "2023-05-03 15:40:23,957 [INFO] [600000/1433083 (42%)]\tLoss: 67.61\n",
      "2023-05-03 15:40:25,586 [INFO] [700000/1433083 (49%)]\tLoss: 61.46\n",
      "2023-05-03 15:40:27,246 [INFO] [800000/1433083 (56%)]\tLoss: 56.15\n",
      "2023-05-03 15:40:28,911 [INFO] [900000/1433083 (63%)]\tLoss: 61.93\n",
      "2023-05-03 15:40:30,402 [INFO] [1000000/1433083 (70%)]\tLoss: 77.77\n",
      "2023-05-03 15:40:31,796 [INFO] [1100000/1433083 (77%)]\tLoss: 79.66\n",
      "2023-05-03 15:40:33,119 [INFO] [1200000/1433083 (84%)]\tLoss: 94.55\n",
      "2023-05-03 15:40:34,289 [INFO] [1300000/1433083 (91%)]\tLoss: 113.35\n",
      "2023-05-03 15:40:35,407 [INFO] [1400000/1433083 (98%)]\tLoss: 207.76\n",
      "2023-05-03 15:40:35,767 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:40:38,565 [INFO] Test set accuracy: 606430/614179 (0.9873831570275116)\n",
      "\n",
      "Epoch:  64%|██████▍   | 16/25 [07:00<03:56, 26.26s/it]2023-05-03 15:40:41,091 [INFO] [100000/1433083 (7%)]\tLoss: 97.59\n",
      "2023-05-03 15:40:43,177 [INFO] [200000/1433083 (14%)]\tLoss: 122.53\n",
      "2023-05-03 15:40:45,034 [INFO] [300000/1433083 (21%)]\tLoss: 103.01\n",
      "2023-05-03 15:40:46,795 [INFO] [400000/1433083 (28%)]\tLoss: 124.74\n",
      "2023-05-03 15:40:48,460 [INFO] [500000/1433083 (35%)]\tLoss: 120.44\n",
      "2023-05-03 15:40:50,115 [INFO] [600000/1433083 (42%)]\tLoss: 105.25\n",
      "2023-05-03 15:40:51,750 [INFO] [700000/1433083 (49%)]\tLoss: 94.69\n",
      "2023-05-03 15:40:53,407 [INFO] [800000/1433083 (56%)]\tLoss: 85.72\n",
      "2023-05-03 15:40:55,089 [INFO] [900000/1433083 (63%)]\tLoss: 89.96\n",
      "2023-05-03 15:40:56,599 [INFO] [1000000/1433083 (70%)]\tLoss: 104.43\n",
      "2023-05-03 15:40:58,013 [INFO] [1100000/1433083 (77%)]\tLoss: 104.44\n",
      "2023-05-03 15:40:59,340 [INFO] [1200000/1433083 (84%)]\tLoss: 115.55\n",
      "2023-05-03 15:41:00,472 [INFO] [1300000/1433083 (91%)]\tLoss: 132.02\n",
      "2023-05-03 15:41:01,446 [INFO] [1400000/1433083 (98%)]\tLoss: 206.63\n",
      "2023-05-03 15:41:01,770 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:41:04,547 [INFO] Test set accuracy: 607365/614179 (0.9889055145161264)\n",
      "\n",
      "Epoch:  68%|██████▊   | 17/25 [07:26<03:29, 26.18s/it]2023-05-03 15:41:07,028 [INFO] [100000/1433083 (7%)]\tLoss: 154.92\n",
      "2023-05-03 15:41:09,163 [INFO] [200000/1433083 (14%)]\tLoss: 112.89\n",
      "2023-05-03 15:41:11,091 [INFO] [300000/1433083 (21%)]\tLoss: 94.47\n",
      "2023-05-03 15:41:12,954 [INFO] [400000/1433083 (28%)]\tLoss: 83.95\n",
      "2023-05-03 15:41:14,709 [INFO] [500000/1433083 (35%)]\tLoss: 81.95\n",
      "2023-05-03 15:41:16,368 [INFO] [600000/1433083 (42%)]\tLoss: 72.30\n",
      "2023-05-03 15:41:18,030 [INFO] [700000/1433083 (49%)]\tLoss: 65.70\n",
      "2023-05-03 15:41:19,700 [INFO] [800000/1433083 (56%)]\tLoss: 59.73\n",
      "2023-05-03 15:41:21,377 [INFO] [900000/1433083 (63%)]\tLoss: 64.52\n",
      "2023-05-03 15:41:22,864 [INFO] [1000000/1433083 (70%)]\tLoss: 78.80\n",
      "2023-05-03 15:41:24,278 [INFO] [1100000/1433083 (77%)]\tLoss: 79.72\n",
      "2023-05-03 15:41:25,595 [INFO] [1200000/1433083 (84%)]\tLoss: 91.10\n",
      "2023-05-03 15:41:26,740 [INFO] [1300000/1433083 (91%)]\tLoss: 109.06\n",
      "2023-05-03 15:41:27,826 [INFO] [1400000/1433083 (98%)]\tLoss: 186.09\n",
      "2023-05-03 15:41:28,192 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:41:31,097 [INFO] Test set accuracy: 607769/614179 (0.9895633032063942)\n",
      "\n",
      "Epoch:  72%|███████▏  | 18/25 [07:52<03:04, 26.29s/it]2023-05-03 15:41:33,543 [INFO] [100000/1433083 (7%)]\tLoss: 83.22\n",
      "2023-05-03 15:41:35,654 [INFO] [200000/1433083 (14%)]\tLoss: 72.98\n",
      "2023-05-03 15:41:37,575 [INFO] [300000/1433083 (21%)]\tLoss: 62.36\n",
      "2023-05-03 15:41:39,424 [INFO] [400000/1433083 (28%)]\tLoss: 56.60\n",
      "2023-05-03 15:41:41,182 [INFO] [500000/1433083 (35%)]\tLoss: 60.66\n",
      "2023-05-03 15:41:42,799 [INFO] [600000/1433083 (42%)]\tLoss: 55.61\n",
      "2023-05-03 15:41:44,345 [INFO] [700000/1433083 (49%)]\tLoss: 50.84\n",
      "2023-05-03 15:41:45,881 [INFO] [800000/1433083 (56%)]\tLoss: 46.50\n",
      "2023-05-03 15:41:47,428 [INFO] [900000/1433083 (63%)]\tLoss: 51.28\n",
      "2023-05-03 15:41:48,852 [INFO] [1000000/1433083 (70%)]\tLoss: 63.91\n",
      "2023-05-03 15:41:50,249 [INFO] [1100000/1433083 (77%)]\tLoss: 65.19\n",
      "2023-05-03 15:41:51,568 [INFO] [1200000/1433083 (84%)]\tLoss: 75.27\n",
      "2023-05-03 15:41:52,740 [INFO] [1300000/1433083 (91%)]\tLoss: 91.32\n",
      "2023-05-03 15:41:53,812 [INFO] [1400000/1433083 (98%)]\tLoss: 176.87\n",
      "2023-05-03 15:41:54,177 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:41:56,951 [INFO] Test set accuracy: 607429/614179 (0.9890097186650797)\n",
      "\n",
      "Epoch:  76%|███████▌  | 19/25 [08:18<02:36, 26.16s/it]2023-05-03 15:41:59,456 [INFO] [100000/1433083 (7%)]\tLoss: 71.78\n",
      "2023-05-03 15:42:01,574 [INFO] [200000/1433083 (14%)]\tLoss: 59.66\n",
      "2023-05-03 15:42:03,521 [INFO] [300000/1433083 (21%)]\tLoss: 51.44\n",
      "2023-05-03 15:42:05,412 [INFO] [400000/1433083 (28%)]\tLoss: 47.05\n",
      "2023-05-03 15:42:07,167 [INFO] [500000/1433083 (35%)]\tLoss: 51.92\n",
      "2023-05-03 15:42:08,832 [INFO] [600000/1433083 (42%)]\tLoss: 46.58\n",
      "2023-05-03 15:42:10,484 [INFO] [700000/1433083 (49%)]\tLoss: 42.72\n",
      "2023-05-03 15:42:12,136 [INFO] [800000/1433083 (56%)]\tLoss: 39.23\n",
      "2023-05-03 15:42:13,781 [INFO] [900000/1433083 (63%)]\tLoss: 43.92\n",
      "2023-05-03 15:42:15,256 [INFO] [1000000/1433083 (70%)]\tLoss: 56.94\n",
      "2023-05-03 15:42:16,661 [INFO] [1100000/1433083 (77%)]\tLoss: 57.96\n",
      "2023-05-03 15:42:17,971 [INFO] [1200000/1433083 (84%)]\tLoss: 66.49\n",
      "2023-05-03 15:42:19,151 [INFO] [1300000/1433083 (91%)]\tLoss: 81.47\n",
      "2023-05-03 15:42:20,247 [INFO] [1400000/1433083 (98%)]\tLoss: 168.43\n",
      "2023-05-03 15:42:20,617 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:42:23,419 [INFO] Test set accuracy: 607092/614179 (0.9884610186932474)\n",
      "\n",
      "Epoch:  80%|████████  | 20/25 [08:45<02:11, 26.25s/it]2023-05-03 15:42:25,935 [INFO] [100000/1433083 (7%)]\tLoss: 82.36\n",
      "2023-05-03 15:42:28,042 [INFO] [200000/1433083 (14%)]\tLoss: 64.34\n",
      "2023-05-03 15:42:29,969 [INFO] [300000/1433083 (21%)]\tLoss: 54.05\n",
      "2023-05-03 15:42:31,830 [INFO] [400000/1433083 (28%)]\tLoss: 48.65\n",
      "2023-05-03 15:42:33,583 [INFO] [500000/1433083 (35%)]\tLoss: 54.35\n",
      "2023-05-03 15:42:35,228 [INFO] [600000/1433083 (42%)]\tLoss: 49.12\n",
      "2023-05-03 15:42:36,879 [INFO] [700000/1433083 (49%)]\tLoss: 44.65\n",
      "2023-05-03 15:42:38,540 [INFO] [800000/1433083 (56%)]\tLoss: 40.83\n",
      "2023-05-03 15:42:40,198 [INFO] [900000/1433083 (63%)]\tLoss: 44.62\n",
      "2023-05-03 15:42:41,683 [INFO] [1000000/1433083 (70%)]\tLoss: 55.30\n",
      "2023-05-03 15:42:43,077 [INFO] [1100000/1433083 (77%)]\tLoss: 55.98\n",
      "2023-05-03 15:42:44,390 [INFO] [1200000/1433083 (84%)]\tLoss: 63.35\n",
      "2023-05-03 15:42:45,548 [INFO] [1300000/1433083 (91%)]\tLoss: 77.04\n",
      "2023-05-03 15:42:46,633 [INFO] [1400000/1433083 (98%)]\tLoss: 148.25\n",
      "2023-05-03 15:42:46,995 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:42:49,792 [INFO] Test set accuracy: 607576/614179 (0.9892490625697069)\n",
      "\n",
      "Epoch:  84%|████████▍ | 21/25 [09:11<01:45, 26.29s/it]2023-05-03 15:42:52,301 [INFO] [100000/1433083 (7%)]\tLoss: 80.48\n",
      "2023-05-03 15:42:54,447 [INFO] [200000/1433083 (14%)]\tLoss: 65.37\n",
      "2023-05-03 15:42:56,394 [INFO] [300000/1433083 (21%)]\tLoss: 54.62\n",
      "2023-05-03 15:42:58,251 [INFO] [400000/1433083 (28%)]\tLoss: 48.10\n",
      "2023-05-03 15:42:59,997 [INFO] [500000/1433083 (35%)]\tLoss: 52.46\n",
      "2023-05-03 15:43:01,706 [INFO] [600000/1433083 (42%)]\tLoss: 48.05\n",
      "2023-05-03 15:43:03,353 [INFO] [700000/1433083 (49%)]\tLoss: 43.78\n",
      "2023-05-03 15:43:05,005 [INFO] [800000/1433083 (56%)]\tLoss: 40.00\n",
      "2023-05-03 15:43:06,650 [INFO] [900000/1433083 (63%)]\tLoss: 43.52\n",
      "2023-05-03 15:43:08,142 [INFO] [1000000/1433083 (70%)]\tLoss: 53.04\n",
      "2023-05-03 15:43:09,549 [INFO] [1100000/1433083 (77%)]\tLoss: 53.88\n",
      "2023-05-03 15:43:10,860 [INFO] [1200000/1433083 (84%)]\tLoss: 60.31\n",
      "2023-05-03 15:43:12,022 [INFO] [1300000/1433083 (91%)]\tLoss: 83.08\n",
      "2023-05-03 15:43:13,120 [INFO] [1400000/1433083 (98%)]\tLoss: 148.80\n",
      "2023-05-03 15:43:13,487 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:43:16,270 [INFO] Test set accuracy: 608145/614179 (0.990175502581495)\n",
      "\n",
      "Epoch:  88%|████████▊ | 22/25 [09:37<01:19, 26.34s/it]2023-05-03 15:43:18,764 [INFO] [100000/1433083 (7%)]\tLoss: 64.76\n",
      "2023-05-03 15:43:20,875 [INFO] [200000/1433083 (14%)]\tLoss: 50.94\n",
      "2023-05-03 15:43:22,822 [INFO] [300000/1433083 (21%)]\tLoss: 42.99\n",
      "2023-05-03 15:43:24,645 [INFO] [400000/1433083 (28%)]\tLoss: 38.16\n",
      "2023-05-03 15:43:26,285 [INFO] [500000/1433083 (35%)]\tLoss: 38.68\n",
      "2023-05-03 15:43:27,832 [INFO] [600000/1433083 (42%)]\tLoss: 34.51\n",
      "2023-05-03 15:43:29,421 [INFO] [700000/1433083 (49%)]\tLoss: 31.69\n",
      "2023-05-03 15:43:31,086 [INFO] [800000/1433083 (56%)]\tLoss: 29.25\n",
      "2023-05-03 15:43:32,750 [INFO] [900000/1433083 (63%)]\tLoss: 32.92\n",
      "2023-05-03 15:43:34,269 [INFO] [1000000/1433083 (70%)]\tLoss: 47.36\n",
      "2023-05-03 15:43:35,681 [INFO] [1100000/1433083 (77%)]\tLoss: 47.99\n",
      "2023-05-03 15:43:36,986 [INFO] [1200000/1433083 (84%)]\tLoss: 54.05\n",
      "2023-05-03 15:43:38,166 [INFO] [1300000/1433083 (91%)]\tLoss: 65.30\n",
      "2023-05-03 15:43:39,244 [INFO] [1400000/1433083 (98%)]\tLoss: 144.26\n",
      "2023-05-03 15:43:39,607 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:43:42,423 [INFO] Test set accuracy: 607851/614179 (0.9896968147722407)\n",
      "\n",
      "Epoch:  92%|█████████▏| 23/25 [10:04<00:52, 26.29s/it]2023-05-03 15:43:45,014 [INFO] [100000/1433083 (7%)]\tLoss: 57.09\n",
      "2023-05-03 15:43:47,099 [INFO] [200000/1433083 (14%)]\tLoss: 44.93\n",
      "2023-05-03 15:43:49,024 [INFO] [300000/1433083 (21%)]\tLoss: 37.14\n",
      "2023-05-03 15:43:50,886 [INFO] [400000/1433083 (28%)]\tLoss: 32.65\n",
      "2023-05-03 15:43:52,634 [INFO] [500000/1433083 (35%)]\tLoss: 37.33\n",
      "2023-05-03 15:43:54,289 [INFO] [600000/1433083 (42%)]\tLoss: 33.39\n",
      "2023-05-03 15:43:55,950 [INFO] [700000/1433083 (49%)]\tLoss: 30.58\n",
      "2023-05-03 15:43:57,600 [INFO] [800000/1433083 (56%)]\tLoss: 28.17\n",
      "2023-05-03 15:43:59,262 [INFO] [900000/1433083 (63%)]\tLoss: 31.57\n",
      "2023-05-03 15:44:00,766 [INFO] [1000000/1433083 (70%)]\tLoss: 48.01\n",
      "2023-05-03 15:44:02,170 [INFO] [1100000/1433083 (77%)]\tLoss: 47.90\n",
      "2023-05-03 15:44:03,487 [INFO] [1200000/1433083 (84%)]\tLoss: 52.66\n",
      "2023-05-03 15:44:04,659 [INFO] [1300000/1433083 (91%)]\tLoss: 61.43\n",
      "2023-05-03 15:44:05,748 [INFO] [1400000/1433083 (98%)]\tLoss: 123.54\n",
      "2023-05-03 15:44:06,115 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:44:08,881 [INFO] Test set accuracy: 608352/614179 (0.9905125378757659)\n",
      "\n",
      "Epoch:  96%|█████████▌| 24/25 [10:30<00:26, 26.34s/it]2023-05-03 15:44:11,399 [INFO] [100000/1433083 (7%)]\tLoss: 53.47\n",
      "2023-05-03 15:44:13,523 [INFO] [200000/1433083 (14%)]\tLoss: 40.78\n",
      "2023-05-03 15:44:15,475 [INFO] [300000/1433083 (21%)]\tLoss: 33.65\n",
      "2023-05-03 15:44:17,322 [INFO] [400000/1433083 (28%)]\tLoss: 29.46\n",
      "2023-05-03 15:44:19,077 [INFO] [500000/1433083 (35%)]\tLoss: 33.62\n",
      "2023-05-03 15:44:20,680 [INFO] [600000/1433083 (42%)]\tLoss: 30.06\n",
      "2023-05-03 15:44:22,259 [INFO] [700000/1433083 (49%)]\tLoss: 27.46\n",
      "2023-05-03 15:44:23,905 [INFO] [800000/1433083 (56%)]\tLoss: 25.31\n",
      "2023-05-03 15:44:25,605 [INFO] [900000/1433083 (63%)]\tLoss: 28.03\n",
      "2023-05-03 15:44:27,107 [INFO] [1000000/1433083 (70%)]\tLoss: 36.12\n",
      "2023-05-03 15:44:28,498 [INFO] [1100000/1433083 (77%)]\tLoss: 66.35\n",
      "2023-05-03 15:44:29,792 [INFO] [1200000/1433083 (84%)]\tLoss: 73.26\n",
      "2023-05-03 15:44:30,955 [INFO] [1300000/1433083 (91%)]\tLoss: 82.08\n",
      "2023-05-03 15:44:32,033 [INFO] [1400000/1433083 (98%)]\tLoss: 136.23\n",
      "2023-05-03 15:44:32,407 [INFO] Evaluating trained model ...\n",
      "2023-05-03 15:44:35,197 [INFO] Test set accuracy: 608462/614179 (0.9906916387567793)\n",
      "\n",
      "Epoch: 100%|██████████| 25/25 [10:56<00:00, 26.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 17s, sys: 19.7 s, total: 21min 37s\n",
      "Wall time: 10min 57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dd.train_model(train_data, labels, batch_size=BATCH_SIZE, epochs=EPOCHS, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "Save pretrained model to a given output location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 15:44:35,223 [INFO] Pretrained model checkpoint saved to location: 'models/rnn_classifier_2023-05-03_15_44_35.bin'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory 'models'\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODELS_DIR):\n",
    "    print(\"Creating directory '{}'\".format(MODELS_DIR))\n",
    "    os.makedirs(MODELS_DIR)\n",
    "\n",
    "now = datetime.now()\n",
    "model_filename = \"rnn_classifier_{}.bin\".format(now.strftime(\"%Y-%m-%d_%H_%M_%S\"))\n",
    "model_filepath = os.path.join(MODELS_DIR, model_filename)\n",
    "dd.save_checkpoint(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model generated above, we now score the test dataset against the model to determine if the domain is likely generated by a DGA or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 15:44:35,240 [INFO] Found GPU's now setting up cuda for the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9923882125569256\n"
     ]
    }
   ],
   "source": [
    "dga_detector = DGADetector()\n",
    "dga_detector.load_checkpoint(model_filepath)\n",
    "\n",
    "domain_train, domain_test, type_train, type_test = train_test_split(gdf, \"type\", train_size=0.7)\n",
    "test_df = cudf.DataFrame()\n",
    "test_df[\"type\"] = type_test.reset_index(drop=True)\n",
    "test_df[\"domain\"] = domain_test.reset_index(drop=True)\n",
    "\n",
    "test_dataset = DGADataset(test_df, 100)\n",
    "test_dataloader = DataLoader(test_dataset, batchsize=BATCH_SIZE)\n",
    "\n",
    "pred_results = []\n",
    "true_results = []\n",
    "for chunk in test_dataloader.get_chunks():\n",
    "    pred_results.append(list(dga_detector.predict(chunk['domain']).values_host))\n",
    "    true_results.append(list(chunk['type'].values_host))\n",
    "pred_results = np.concatenate(pred_results)\n",
    "true_results = np.concatenate(true_results)\n",
    "accuracy_score_result = accuracy_score(pred_results, true_results)\n",
    "\n",
    "print('Model accuracy: %s'%(accuracy_score_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.980\n"
     ]
    }
   ],
   "source": [
    "average_precision = average_precision_score(true_results, pred_results)\n",
    "\n",
    "print('Average precision score: {0:0.3f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DGA detection implementation enables users to train their models for detection and also use existing models. This capability could also be used in conjunction with log parsing efforts if the logs contain domain names. Data is kept in GPU memory, removing unnecessary copy/converts and providing a 4X speed advantage over CPU only implementations. This is esepcially true with large batch sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
