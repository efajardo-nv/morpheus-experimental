{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Generation Algorithm (DGA) Detection\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Data Importing\n",
    "* Data Preprocessing\n",
    "* Training and Evaluation\n",
    "* Inference\n",
    "* Conclusion\n",
    "\n",
    "## Introduction\n",
    "[Domain Generation Algorithms](https://en.wikipedia.org/wiki/Domain_generation_algorithm) (DGAs) are used to generate domain names that can be used by the malware to communicate with the command and control servers. IP addresses and static domain names can be easily blocked, and a DGA provides an easy method to generate a large number of domain names and rotate through them to circumvent traditional block lists. We will use a type of recurrent neural network called the [Gated Recurrent Unit](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) (GRU) for this example. This implementation enables users to train their models with up-to-date domain names representative of both benign and DGA generated strings. This capability could also be used in production. This notebook provides a view into the data science workflow to create a DGA detection implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import torch\n",
    "import requests\n",
    "import logging\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from dga_detector import DGADetector\n",
    "from dataloader import DataLoader\n",
    "from dga_dataset import DGADataset\n",
    "from cuml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable console logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Input Dataset from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_CSV = \"../datasets/benign_and_dga_domains.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Benign and DGA dataset\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    r = requests.get(DATA_BASE_URL + INPUT_CSV)\n",
    "    open(INPUT_CSV, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Input Dataset to GPU Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = cudf.read_csv(INPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = gdf['domain']\n",
    "labels = gdf['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have only benign and DGA (malicious) categoriesm, the number of domain types need to be set to 2 (`N_DOMAIN_TYPE=2`). Vocabulary size(`CHAR_VOCAB`) is set to 128 ASCII characters. The values below set for `HIDDEN_SIZE`, `N_LAYERS` of the network, and the `LR` (Learning Rate) give an optimum balance for the network size and performance. They might need be set via experiments when working with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "N_LAYERS = 3\n",
    "CHAR_VOCAB = 128\n",
    "HIDDEN_SIZE = 100\n",
    "N_DOMAIN_TYPE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate DGA detector\n",
    "Now that the data is ready, the datasets are created, and we've set the parameters for the model, we can use the DGA detector method to create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 23:11:26,383 [INFO] Found GPU's now setting up cuda for the model\n"
     ]
    }
   ],
   "source": [
    "dd = DGADetector(lr=LR)\n",
    "dd.init_model(n_layers=N_LAYERS, char_vocab=CHAR_VOCAB, hidden_size=HIDDEN_SIZE, n_domain_type=N_DOMAIN_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "TRAIN_SIZE = 0.7\n",
    "BATCH_SIZE = 10000\n",
    "MODELS_DIR = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Now we train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 23:11:28,490 [INFO] Initiating model training ...\n",
      "2023-05-02 23:11:28,491 [INFO] Truncate domains to width: 100\n",
      "/opt/conda/envs/mor_exp/lib/python3.8/site-packages/cudf/core/column/string.py:3563: FutureWarning: The expand parameter is deprecated and will be removed in a future version. Set expand=False to match future behavior.\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/25 [00:00<?, ?it/s]2023-05-02 23:11:32,119 [INFO] [100000/1433083 (7%)]\tLoss: 7320.60\n",
      "2023-05-02 23:11:34,152 [INFO] [200000/1433083 (14%)]\tLoss: 5294.86\n",
      "2023-05-02 23:11:36,027 [INFO] [300000/1433083 (21%)]\tLoss: 4087.44\n",
      "2023-05-02 23:11:37,856 [INFO] [400000/1433083 (28%)]\tLoss: 3551.52\n",
      "2023-05-02 23:11:39,581 [INFO] [500000/1433083 (35%)]\tLoss: 3195.77\n",
      "2023-05-02 23:11:41,180 [INFO] [600000/1433083 (42%)]\tLoss: 2762.37\n",
      "2023-05-02 23:11:42,691 [INFO] [700000/1433083 (49%)]\tLoss: 2427.19\n",
      "2023-05-02 23:11:44,292 [INFO] [800000/1433083 (56%)]\tLoss: 2164.40\n",
      "2023-05-02 23:11:45,915 [INFO] [900000/1433083 (63%)]\tLoss: 2263.38\n",
      "2023-05-02 23:11:47,397 [INFO] [1000000/1433083 (70%)]\tLoss: 2150.27\n",
      "2023-05-02 23:11:48,777 [INFO] [1100000/1433083 (77%)]\tLoss: 2202.72\n",
      "2023-05-02 23:11:50,064 [INFO] [1200000/1433083 (84%)]\tLoss: 2139.12\n",
      "2023-05-02 23:11:51,211 [INFO] [1300000/1433083 (91%)]\tLoss: 2137.42\n",
      "2023-05-02 23:11:52,295 [INFO] [1400000/1433083 (98%)]\tLoss: 2174.56\n",
      "2023-05-02 23:11:52,648 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:11:55,426 [INFO] Test set accuracy: 407853/614179 (0.6640621056727761)\n",
      "\n",
      "Epoch:   4%|▍         | 1/25 [00:25<10:23, 25.97s/it]2023-05-02 23:11:57,860 [INFO] [100000/1433083 (7%)]\tLoss: 4072.56\n",
      "2023-05-02 23:11:59,945 [INFO] [200000/1433083 (14%)]\tLoss: 2969.67\n",
      "2023-05-02 23:12:01,844 [INFO] [300000/1433083 (21%)]\tLoss: 2238.40\n",
      "2023-05-02 23:12:03,665 [INFO] [400000/1433083 (28%)]\tLoss: 1910.12\n",
      "2023-05-02 23:12:05,408 [INFO] [500000/1433083 (35%)]\tLoss: 1677.65\n",
      "2023-05-02 23:12:07,046 [INFO] [600000/1433083 (42%)]\tLoss: 1433.51\n",
      "2023-05-02 23:12:08,691 [INFO] [700000/1433083 (49%)]\tLoss: 1253.36\n",
      "2023-05-02 23:12:10,344 [INFO] [800000/1433083 (56%)]\tLoss: 1113.49\n",
      "2023-05-02 23:12:12,009 [INFO] [900000/1433083 (63%)]\tLoss: 1086.52\n",
      "2023-05-02 23:12:13,492 [INFO] [1000000/1433083 (70%)]\tLoss: 1063.66\n",
      "2023-05-02 23:12:14,885 [INFO] [1100000/1433083 (77%)]\tLoss: 1067.12\n",
      "2023-05-02 23:12:16,190 [INFO] [1200000/1433083 (84%)]\tLoss: 1052.82\n",
      "2023-05-02 23:12:17,368 [INFO] [1300000/1433083 (91%)]\tLoss: 1094.43\n",
      "2023-05-02 23:12:18,465 [INFO] [1400000/1433083 (98%)]\tLoss: 1184.17\n",
      "2023-05-02 23:12:18,834 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:12:21,626 [INFO] Test set accuracy: 527666/614179 (0.8591404134625248)\n",
      "\n",
      "Epoch:   8%|▊         | 2/25 [00:52<10:00, 26.11s/it]2023-05-02 23:12:24,109 [INFO] [100000/1433083 (7%)]\tLoss: 2214.06\n",
      "2023-05-02 23:12:26,203 [INFO] [200000/1433083 (14%)]\tLoss: 1627.60\n",
      "2023-05-02 23:12:28,099 [INFO] [300000/1433083 (21%)]\tLoss: 1235.42\n",
      "2023-05-02 23:12:29,937 [INFO] [400000/1433083 (28%)]\tLoss: 1136.05\n",
      "2023-05-02 23:12:31,694 [INFO] [500000/1433083 (35%)]\tLoss: 1010.08\n",
      "2023-05-02 23:12:33,343 [INFO] [600000/1433083 (42%)]\tLoss: 866.97\n",
      "2023-05-02 23:12:34,979 [INFO] [700000/1433083 (49%)]\tLoss: 762.29\n",
      "2023-05-02 23:12:36,638 [INFO] [800000/1433083 (56%)]\tLoss: 680.59\n",
      "2023-05-02 23:12:38,295 [INFO] [900000/1433083 (63%)]\tLoss: 661.30\n",
      "2023-05-02 23:12:39,782 [INFO] [1000000/1433083 (70%)]\tLoss: 668.79\n",
      "2023-05-02 23:12:41,278 [INFO] [1100000/1433083 (77%)]\tLoss: 655.21\n",
      "2023-05-02 23:12:42,585 [INFO] [1200000/1433083 (84%)]\tLoss: 674.54\n",
      "2023-05-02 23:12:43,758 [INFO] [1300000/1433083 (91%)]\tLoss: 726.65\n",
      "2023-05-02 23:12:44,862 [INFO] [1400000/1433083 (98%)]\tLoss: 834.93\n",
      "2023-05-02 23:12:45,233 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:12:48,062 [INFO] Test set accuracy: 581576/614179 (0.9469161270574213)\n",
      "\n",
      "Epoch:  12%|█▏        | 3/25 [01:18<09:37, 26.26s/it]2023-05-02 23:12:50,468 [INFO] [100000/1433083 (7%)]\tLoss: 1716.52\n",
      "2023-05-02 23:12:52,569 [INFO] [200000/1433083 (14%)]\tLoss: 1277.55\n",
      "2023-05-02 23:12:54,426 [INFO] [300000/1433083 (21%)]\tLoss: 961.61\n",
      "2023-05-02 23:12:56,306 [INFO] [400000/1433083 (28%)]\tLoss: 816.97\n",
      "2023-05-02 23:12:58,044 [INFO] [500000/1433083 (35%)]\tLoss: 722.43\n",
      "2023-05-02 23:12:59,694 [INFO] [600000/1433083 (42%)]\tLoss: 619.24\n",
      "2023-05-02 23:13:01,351 [INFO] [700000/1433083 (49%)]\tLoss: 544.45\n",
      "2023-05-02 23:13:02,932 [INFO] [800000/1433083 (56%)]\tLoss: 486.28\n",
      "2023-05-02 23:13:04,501 [INFO] [900000/1433083 (63%)]\tLoss: 483.85\n",
      "2023-05-02 23:13:05,894 [INFO] [1000000/1433083 (70%)]\tLoss: 510.80\n",
      "2023-05-02 23:13:07,207 [INFO] [1100000/1433083 (77%)]\tLoss: 507.38\n",
      "2023-05-02 23:13:08,543 [INFO] [1200000/1433083 (84%)]\tLoss: 534.80\n",
      "2023-05-02 23:13:09,711 [INFO] [1300000/1433083 (91%)]\tLoss: 581.21\n",
      "2023-05-02 23:13:10,809 [INFO] [1400000/1433083 (98%)]\tLoss: 701.53\n",
      "2023-05-02 23:13:11,179 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:13:13,977 [INFO] Test set accuracy: 593721/614179 (0.9666904925111409)\n",
      "\n",
      "Epoch:  16%|█▌        | 4/25 [01:44<09:08, 26.12s/it]2023-05-02 23:13:16,505 [INFO] [100000/1433083 (7%)]\tLoss: 414.02\n",
      "2023-05-02 23:13:18,615 [INFO] [200000/1433083 (14%)]\tLoss: 423.00\n",
      "2023-05-02 23:13:20,537 [INFO] [300000/1433083 (21%)]\tLoss: 354.74\n",
      "2023-05-02 23:13:22,427 [INFO] [400000/1433083 (28%)]\tLoss: 348.94\n",
      "2023-05-02 23:13:24,183 [INFO] [500000/1433083 (35%)]\tLoss: 334.40\n",
      "2023-05-02 23:13:25,853 [INFO] [600000/1433083 (42%)]\tLoss: 291.46\n",
      "2023-05-02 23:13:27,499 [INFO] [700000/1433083 (49%)]\tLoss: 260.76\n",
      "2023-05-02 23:13:29,142 [INFO] [800000/1433083 (56%)]\tLoss: 236.17\n",
      "2023-05-02 23:13:30,801 [INFO] [900000/1433083 (63%)]\tLoss: 253.50\n",
      "2023-05-02 23:13:32,306 [INFO] [1000000/1433083 (70%)]\tLoss: 284.69\n",
      "2023-05-02 23:13:33,727 [INFO] [1100000/1433083 (77%)]\tLoss: 292.98\n",
      "2023-05-02 23:13:35,028 [INFO] [1200000/1433083 (84%)]\tLoss: 326.45\n",
      "2023-05-02 23:13:36,201 [INFO] [1300000/1433083 (91%)]\tLoss: 379.20\n",
      "2023-05-02 23:13:37,301 [INFO] [1400000/1433083 (98%)]\tLoss: 518.49\n",
      "2023-05-02 23:13:37,665 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:13:40,462 [INFO] Test set accuracy: 594565/614179 (0.9680646847254628)\n",
      "\n",
      "Epoch:  20%|██        | 5/25 [02:11<08:45, 26.25s/it]2023-05-02 23:13:42,975 [INFO] [100000/1433083 (7%)]\tLoss: 356.70\n",
      "2023-05-02 23:13:45,098 [INFO] [200000/1433083 (14%)]\tLoss: 322.48\n",
      "2023-05-02 23:13:47,020 [INFO] [300000/1433083 (21%)]\tLoss: 279.89\n",
      "2023-05-02 23:13:48,895 [INFO] [400000/1433083 (28%)]\tLoss: 258.35\n",
      "2023-05-02 23:13:50,666 [INFO] [500000/1433083 (35%)]\tLoss: 253.62\n",
      "2023-05-02 23:13:52,306 [INFO] [600000/1433083 (42%)]\tLoss: 222.02\n",
      "2023-05-02 23:13:53,967 [INFO] [700000/1433083 (49%)]\tLoss: 199.54\n",
      "2023-05-02 23:13:55,613 [INFO] [800000/1433083 (56%)]\tLoss: 181.50\n",
      "2023-05-02 23:13:57,285 [INFO] [900000/1433083 (63%)]\tLoss: 200.23\n",
      "2023-05-02 23:13:58,775 [INFO] [1000000/1433083 (70%)]\tLoss: 232.94\n",
      "2023-05-02 23:14:00,183 [INFO] [1100000/1433083 (77%)]\tLoss: 241.66\n",
      "2023-05-02 23:14:01,468 [INFO] [1200000/1433083 (84%)]\tLoss: 273.83\n",
      "2023-05-02 23:14:02,653 [INFO] [1300000/1433083 (91%)]\tLoss: 314.49\n",
      "2023-05-02 23:14:03,715 [INFO] [1400000/1433083 (98%)]\tLoss: 475.11\n",
      "2023-05-02 23:14:04,082 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:14:06,874 [INFO] Test set accuracy: 586133/614179 (0.9543357881008631)\n",
      "\n",
      "Epoch:  24%|██▍       | 6/25 [02:37<08:19, 26.31s/it]2023-05-02 23:14:09,366 [INFO] [100000/1433083 (7%)]\tLoss: 212.76\n",
      "2023-05-02 23:14:11,480 [INFO] [200000/1433083 (14%)]\tLoss: 217.62\n",
      "2023-05-02 23:14:13,411 [INFO] [300000/1433083 (21%)]\tLoss: 201.63\n",
      "2023-05-02 23:14:15,302 [INFO] [400000/1433083 (28%)]\tLoss: 197.93\n",
      "2023-05-02 23:14:17,063 [INFO] [500000/1433083 (35%)]\tLoss: 200.23\n",
      "2023-05-02 23:14:18,726 [INFO] [600000/1433083 (42%)]\tLoss: 176.28\n",
      "2023-05-02 23:14:20,389 [INFO] [700000/1433083 (49%)]\tLoss: 159.38\n",
      "2023-05-02 23:14:22,062 [INFO] [800000/1433083 (56%)]\tLoss: 145.73\n",
      "2023-05-02 23:14:23,735 [INFO] [900000/1433083 (63%)]\tLoss: 165.22\n",
      "2023-05-02 23:14:25,230 [INFO] [1000000/1433083 (70%)]\tLoss: 196.82\n",
      "2023-05-02 23:14:26,637 [INFO] [1100000/1433083 (77%)]\tLoss: 205.32\n",
      "2023-05-02 23:14:27,937 [INFO] [1200000/1433083 (84%)]\tLoss: 233.94\n",
      "2023-05-02 23:14:29,125 [INFO] [1300000/1433083 (91%)]\tLoss: 272.52\n",
      "2023-05-02 23:14:30,213 [INFO] [1400000/1433083 (98%)]\tLoss: 411.87\n",
      "2023-05-02 23:14:30,574 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:14:33,370 [INFO] Test set accuracy: 594664/614179 (0.9682258755183749)\n",
      "\n",
      "Epoch:  28%|██▊       | 7/25 [03:03<07:54, 26.37s/it]2023-05-02 23:14:35,865 [INFO] [100000/1433083 (7%)]\tLoss: 208.46\n",
      "2023-05-02 23:14:37,968 [INFO] [200000/1433083 (14%)]\tLoss: 202.23\n",
      "2023-05-02 23:14:39,885 [INFO] [300000/1433083 (21%)]\tLoss: 185.04\n",
      "2023-05-02 23:14:41,774 [INFO] [400000/1433083 (28%)]\tLoss: 181.70\n",
      "2023-05-02 23:14:43,509 [INFO] [500000/1433083 (35%)]\tLoss: 183.56\n",
      "2023-05-02 23:14:45,172 [INFO] [600000/1433083 (42%)]\tLoss: 161.46\n",
      "2023-05-02 23:14:46,823 [INFO] [700000/1433083 (49%)]\tLoss: 145.85\n",
      "2023-05-02 23:14:48,468 [INFO] [800000/1433083 (56%)]\tLoss: 133.38\n",
      "2023-05-02 23:14:50,024 [INFO] [900000/1433083 (63%)]\tLoss: 149.86\n",
      "2023-05-02 23:14:51,401 [INFO] [1000000/1433083 (70%)]\tLoss: 184.15\n",
      "2023-05-02 23:14:52,713 [INFO] [1100000/1433083 (77%)]\tLoss: 190.56\n",
      "2023-05-02 23:14:54,010 [INFO] [1200000/1433083 (84%)]\tLoss: 216.35\n",
      "2023-05-02 23:14:55,070 [INFO] [1300000/1433083 (91%)]\tLoss: 250.75\n",
      "2023-05-02 23:14:56,105 [INFO] [1400000/1433083 (98%)]\tLoss: 379.88\n",
      "2023-05-02 23:14:56,472 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:14:59,266 [INFO] Test set accuracy: 599454/614179 (0.9760249047915999)\n",
      "\n",
      "Epoch:  32%|███▏      | 8/25 [03:29<07:25, 26.22s/it]2023-05-02 23:15:01,751 [INFO] [100000/1433083 (7%)]\tLoss: 192.95\n",
      "2023-05-02 23:15:03,829 [INFO] [200000/1433083 (14%)]\tLoss: 179.45\n",
      "2023-05-02 23:15:05,756 [INFO] [300000/1433083 (21%)]\tLoss: 162.79\n",
      "2023-05-02 23:15:07,616 [INFO] [400000/1433083 (28%)]\tLoss: 155.21\n",
      "2023-05-02 23:15:09,378 [INFO] [500000/1433083 (35%)]\tLoss: 158.25\n",
      "2023-05-02 23:15:11,038 [INFO] [600000/1433083 (42%)]\tLoss: 139.40\n",
      "2023-05-02 23:15:12,685 [INFO] [700000/1433083 (49%)]\tLoss: 126.06\n",
      "2023-05-02 23:15:14,349 [INFO] [800000/1433083 (56%)]\tLoss: 115.49\n",
      "2023-05-02 23:15:16,030 [INFO] [900000/1433083 (63%)]\tLoss: 129.98\n",
      "2023-05-02 23:15:17,523 [INFO] [1000000/1433083 (70%)]\tLoss: 169.40\n",
      "2023-05-02 23:15:18,931 [INFO] [1100000/1433083 (77%)]\tLoss: 175.44\n",
      "2023-05-02 23:15:20,237 [INFO] [1200000/1433083 (84%)]\tLoss: 198.86\n",
      "2023-05-02 23:15:21,388 [INFO] [1300000/1433083 (91%)]\tLoss: 229.20\n",
      "2023-05-02 23:15:22,487 [INFO] [1400000/1433083 (98%)]\tLoss: 356.87\n",
      "2023-05-02 23:15:22,853 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:15:25,634 [INFO] Test set accuracy: 598920/614179 (0.9751554514237706)\n",
      "\n",
      "Epoch:  36%|███▌      | 9/25 [03:56<07:00, 26.27s/it]2023-05-02 23:15:28,124 [INFO] [100000/1433083 (7%)]\tLoss: 169.65\n",
      "2023-05-02 23:15:30,232 [INFO] [200000/1433083 (14%)]\tLoss: 156.59\n",
      "2023-05-02 23:15:32,162 [INFO] [300000/1433083 (21%)]\tLoss: 141.58\n",
      "2023-05-02 23:15:34,026 [INFO] [400000/1433083 (28%)]\tLoss: 135.60\n",
      "2023-05-02 23:15:35,777 [INFO] [500000/1433083 (35%)]\tLoss: 139.39\n",
      "2023-05-02 23:15:37,443 [INFO] [600000/1433083 (42%)]\tLoss: 122.84\n",
      "2023-05-02 23:15:39,083 [INFO] [700000/1433083 (49%)]\tLoss: 110.99\n",
      "2023-05-02 23:15:40,736 [INFO] [800000/1433083 (56%)]\tLoss: 101.64\n",
      "2023-05-02 23:15:42,400 [INFO] [900000/1433083 (63%)]\tLoss: 113.89\n",
      "2023-05-02 23:15:43,866 [INFO] [1000000/1433083 (70%)]\tLoss: 140.85\n",
      "2023-05-02 23:15:45,246 [INFO] [1100000/1433083 (77%)]\tLoss: 146.13\n",
      "2023-05-02 23:15:46,553 [INFO] [1200000/1433083 (84%)]\tLoss: 168.14\n",
      "2023-05-02 23:15:47,724 [INFO] [1300000/1433083 (91%)]\tLoss: 197.65\n",
      "2023-05-02 23:15:48,796 [INFO] [1400000/1433083 (98%)]\tLoss: 319.29\n",
      "2023-05-02 23:15:49,180 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:15:51,943 [INFO] Test set accuracy: 599653/614179 (0.9763489145672516)\n",
      "\n",
      "Epoch:  40%|████      | 10/25 [04:22<06:34, 26.28s/it]2023-05-02 23:15:54,432 [INFO] [100000/1433083 (7%)]\tLoss: 157.59\n",
      "2023-05-02 23:15:56,524 [INFO] [200000/1433083 (14%)]\tLoss: 140.11\n",
      "2023-05-02 23:15:58,461 [INFO] [300000/1433083 (21%)]\tLoss: 124.24\n",
      "2023-05-02 23:16:00,324 [INFO] [400000/1433083 (28%)]\tLoss: 118.32\n",
      "2023-05-02 23:16:02,089 [INFO] [500000/1433083 (35%)]\tLoss: 122.40\n",
      "2023-05-02 23:16:03,737 [INFO] [600000/1433083 (42%)]\tLoss: 108.63\n",
      "2023-05-02 23:16:05,375 [INFO] [700000/1433083 (49%)]\tLoss: 98.42\n",
      "2023-05-02 23:16:06,969 [INFO] [800000/1433083 (56%)]\tLoss: 90.14\n",
      "2023-05-02 23:16:08,508 [INFO] [900000/1433083 (63%)]\tLoss: 100.93\n",
      "2023-05-02 23:16:09,946 [INFO] [1000000/1433083 (70%)]\tLoss: 122.48\n",
      "2023-05-02 23:16:11,361 [INFO] [1100000/1433083 (77%)]\tLoss: 127.25\n",
      "2023-05-02 23:16:12,646 [INFO] [1200000/1433083 (84%)]\tLoss: 148.20\n",
      "2023-05-02 23:16:13,819 [INFO] [1300000/1433083 (91%)]\tLoss: 174.85\n",
      "2023-05-02 23:16:14,928 [INFO] [1400000/1433083 (98%)]\tLoss: 291.40\n",
      "2023-05-02 23:16:15,291 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:16:18,070 [INFO] Test set accuracy: 602771/614179 (0.9814256104490711)\n",
      "\n",
      "Epoch:  44%|████▍     | 11/25 [04:48<06:07, 26.23s/it]2023-05-02 23:16:20,442 [INFO] [100000/1433083 (7%)]\tLoss: 150.85\n",
      "2023-05-02 23:16:22,413 [INFO] [200000/1433083 (14%)]\tLoss: 128.22\n",
      "2023-05-02 23:16:24,230 [INFO] [300000/1433083 (21%)]\tLoss: 112.07\n",
      "2023-05-02 23:16:25,968 [INFO] [400000/1433083 (28%)]\tLoss: 104.98\n",
      "2023-05-02 23:16:27,690 [INFO] [500000/1433083 (35%)]\tLoss: 109.58\n",
      "2023-05-02 23:16:29,343 [INFO] [600000/1433083 (42%)]\tLoss: 97.54\n",
      "2023-05-02 23:16:30,975 [INFO] [700000/1433083 (49%)]\tLoss: 88.43\n",
      "2023-05-02 23:16:32,555 [INFO] [800000/1433083 (56%)]\tLoss: 81.00\n",
      "2023-05-02 23:16:34,194 [INFO] [900000/1433083 (63%)]\tLoss: 90.49\n",
      "2023-05-02 23:16:35,570 [INFO] [1000000/1433083 (70%)]\tLoss: 110.46\n",
      "2023-05-02 23:16:36,873 [INFO] [1100000/1433083 (77%)]\tLoss: 114.47\n",
      "2023-05-02 23:16:38,062 [INFO] [1200000/1433083 (84%)]\tLoss: 133.23\n",
      "2023-05-02 23:16:39,134 [INFO] [1300000/1433083 (91%)]\tLoss: 157.21\n",
      "2023-05-02 23:16:40,102 [INFO] [1400000/1433083 (98%)]\tLoss: 271.52\n",
      "2023-05-02 23:16:40,461 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:16:43,261 [INFO] Test set accuracy: 602216/614179 (0.9805219650948664)\n",
      "\n",
      "Epoch:  48%|████▊     | 12/25 [05:13<05:36, 25.92s/it]2023-05-02 23:16:45,648 [INFO] [100000/1433083 (7%)]\tLoss: 190.61\n",
      "2023-05-02 23:16:47,611 [INFO] [200000/1433083 (14%)]\tLoss: 147.50\n",
      "2023-05-02 23:16:49,432 [INFO] [300000/1433083 (21%)]\tLoss: 121.77\n",
      "2023-05-02 23:16:51,172 [INFO] [400000/1433083 (28%)]\tLoss: 110.78\n",
      "2023-05-02 23:16:52,804 [INFO] [500000/1433083 (35%)]\tLoss: 111.44\n",
      "2023-05-02 23:16:54,359 [INFO] [600000/1433083 (42%)]\tLoss: 98.23\n",
      "2023-05-02 23:16:55,893 [INFO] [700000/1433083 (49%)]\tLoss: 88.43\n",
      "2023-05-02 23:16:57,418 [INFO] [800000/1433083 (56%)]\tLoss: 80.76\n",
      "2023-05-02 23:16:59,058 [INFO] [900000/1433083 (63%)]\tLoss: 88.63\n",
      "2023-05-02 23:17:00,539 [INFO] [1000000/1433083 (70%)]\tLoss: 108.77\n",
      "2023-05-02 23:17:01,957 [INFO] [1100000/1433083 (77%)]\tLoss: 111.56\n",
      "2023-05-02 23:17:03,254 [INFO] [1200000/1433083 (84%)]\tLoss: 126.94\n",
      "2023-05-02 23:17:04,438 [INFO] [1300000/1433083 (91%)]\tLoss: 149.83\n",
      "2023-05-02 23:17:05,498 [INFO] [1400000/1433083 (98%)]\tLoss: 247.77\n",
      "2023-05-02 23:17:05,856 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:17:08,655 [INFO] Test set accuracy: 606014/614179 (0.986705830059315)\n",
      "\n",
      "Epoch:  52%|█████▏    | 13/25 [05:39<05:09, 25.76s/it]2023-05-02 23:17:11,134 [INFO] [100000/1433083 (7%)]\tLoss: 254.53\n",
      "2023-05-02 23:17:13,201 [INFO] [200000/1433083 (14%)]\tLoss: 174.79\n",
      "2023-05-02 23:17:15,008 [INFO] [300000/1433083 (21%)]\tLoss: 143.09\n",
      "2023-05-02 23:17:16,869 [INFO] [400000/1433083 (28%)]\tLoss: 126.45\n",
      "2023-05-02 23:17:18,616 [INFO] [500000/1433083 (35%)]\tLoss: 122.19\n",
      "2023-05-02 23:17:20,206 [INFO] [600000/1433083 (42%)]\tLoss: 106.55\n",
      "2023-05-02 23:17:21,862 [INFO] [700000/1433083 (49%)]\tLoss: 95.65\n",
      "2023-05-02 23:17:23,492 [INFO] [800000/1433083 (56%)]\tLoss: 87.05\n",
      "2023-05-02 23:17:25,042 [INFO] [900000/1433083 (63%)]\tLoss: 93.91\n",
      "2023-05-02 23:17:26,419 [INFO] [1000000/1433083 (70%)]\tLoss: 108.56\n",
      "2023-05-02 23:17:27,713 [INFO] [1100000/1433083 (77%)]\tLoss: 110.35\n",
      "2023-05-02 23:17:28,913 [INFO] [1200000/1433083 (84%)]\tLoss: 123.81\n",
      "2023-05-02 23:17:30,048 [INFO] [1300000/1433083 (91%)]\tLoss: 144.84\n",
      "2023-05-02 23:17:31,133 [INFO] [1400000/1433083 (98%)]\tLoss: 237.22\n",
      "2023-05-02 23:17:31,483 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:17:34,252 [INFO] Test set accuracy: 606687/614179 (0.9878016018131522)\n",
      "\n",
      "Epoch:  56%|█████▌    | 14/25 [06:04<04:42, 25.71s/it]2023-05-02 23:17:36,755 [INFO] [100000/1433083 (7%)]\tLoss: 140.21\n",
      "2023-05-02 23:17:38,874 [INFO] [200000/1433083 (14%)]\tLoss: 104.92\n",
      "2023-05-02 23:17:40,796 [INFO] [300000/1433083 (21%)]\tLoss: 90.41\n",
      "2023-05-02 23:17:42,644 [INFO] [400000/1433083 (28%)]\tLoss: 84.05\n",
      "2023-05-02 23:17:44,396 [INFO] [500000/1433083 (35%)]\tLoss: 92.43\n",
      "2023-05-02 23:17:46,033 [INFO] [600000/1433083 (42%)]\tLoss: 83.79\n",
      "2023-05-02 23:17:47,657 [INFO] [700000/1433083 (49%)]\tLoss: 77.02\n",
      "2023-05-02 23:17:49,249 [INFO] [800000/1433083 (56%)]\tLoss: 71.14\n",
      "2023-05-02 23:17:50,817 [INFO] [900000/1433083 (63%)]\tLoss: 78.59\n",
      "2023-05-02 23:17:52,317 [INFO] [1000000/1433083 (70%)]\tLoss: 91.93\n",
      "2023-05-02 23:17:53,727 [INFO] [1100000/1433083 (77%)]\tLoss: 93.87\n",
      "2023-05-02 23:17:55,039 [INFO] [1200000/1433083 (84%)]\tLoss: 109.40\n",
      "2023-05-02 23:17:56,217 [INFO] [1300000/1433083 (91%)]\tLoss: 128.80\n",
      "2023-05-02 23:17:57,284 [INFO] [1400000/1433083 (98%)]\tLoss: 228.73\n",
      "2023-05-02 23:17:57,642 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:18:00,434 [INFO] Test set accuracy: 606159/614179 (0.9869419175842873)\n",
      "\n",
      "Epoch:  60%|██████    | 15/25 [06:30<04:18, 25.85s/it]2023-05-02 23:18:02,913 [INFO] [100000/1433083 (7%)]\tLoss: 113.21\n",
      "2023-05-02 23:18:05,009 [INFO] [200000/1433083 (14%)]\tLoss: 88.68\n",
      "2023-05-02 23:18:06,928 [INFO] [300000/1433083 (21%)]\tLoss: 75.74\n",
      "2023-05-02 23:18:08,736 [INFO] [400000/1433083 (28%)]\tLoss: 70.11\n",
      "2023-05-02 23:18:10,364 [INFO] [500000/1433083 (35%)]\tLoss: 73.28\n",
      "2023-05-02 23:18:11,899 [INFO] [600000/1433083 (42%)]\tLoss: 65.02\n",
      "2023-05-02 23:18:13,471 [INFO] [700000/1433083 (49%)]\tLoss: 58.97\n",
      "2023-05-02 23:18:15,103 [INFO] [800000/1433083 (56%)]\tLoss: 54.24\n",
      "2023-05-02 23:18:16,758 [INFO] [900000/1433083 (63%)]\tLoss: 61.32\n",
      "2023-05-02 23:18:18,238 [INFO] [1000000/1433083 (70%)]\tLoss: 81.08\n",
      "2023-05-02 23:18:19,643 [INFO] [1100000/1433083 (77%)]\tLoss: 83.55\n",
      "2023-05-02 23:18:20,942 [INFO] [1200000/1433083 (84%)]\tLoss: 94.23\n",
      "2023-05-02 23:18:22,104 [INFO] [1300000/1433083 (91%)]\tLoss: 112.29\n",
      "2023-05-02 23:18:23,190 [INFO] [1400000/1433083 (98%)]\tLoss: 201.95\n",
      "2023-05-02 23:18:23,553 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:18:26,329 [INFO] Test set accuracy: 607260/614179 (0.9887345545842499)\n",
      "\n",
      "Epoch:  64%|██████▍   | 16/25 [06:56<03:52, 25.86s/it]2023-05-02 23:18:28,804 [INFO] [100000/1433083 (7%)]\tLoss: 110.37\n",
      "2023-05-02 23:18:30,890 [INFO] [200000/1433083 (14%)]\tLoss: 83.21\n",
      "2023-05-02 23:18:32,828 [INFO] [300000/1433083 (21%)]\tLoss: 70.79\n",
      "2023-05-02 23:18:34,694 [INFO] [400000/1433083 (28%)]\tLoss: 64.73\n",
      "2023-05-02 23:18:36,452 [INFO] [500000/1433083 (35%)]\tLoss: 72.01\n",
      "2023-05-02 23:18:38,115 [INFO] [600000/1433083 (42%)]\tLoss: 65.91\n",
      "2023-05-02 23:18:39,768 [INFO] [700000/1433083 (49%)]\tLoss: 60.56\n",
      "2023-05-02 23:18:41,430 [INFO] [800000/1433083 (56%)]\tLoss: 55.73\n",
      "2023-05-02 23:18:43,110 [INFO] [900000/1433083 (63%)]\tLoss: 61.77\n",
      "2023-05-02 23:18:44,599 [INFO] [1000000/1433083 (70%)]\tLoss: 74.17\n",
      "2023-05-02 23:18:46,011 [INFO] [1100000/1433083 (77%)]\tLoss: 85.51\n",
      "2023-05-02 23:18:47,302 [INFO] [1200000/1433083 (84%)]\tLoss: 95.61\n",
      "2023-05-02 23:18:48,459 [INFO] [1300000/1433083 (91%)]\tLoss: 114.53\n",
      "2023-05-02 23:18:49,541 [INFO] [1400000/1433083 (98%)]\tLoss: 193.89\n",
      "2023-05-02 23:18:49,904 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:18:52,654 [INFO] Test set accuracy: 607713/614179 (0.98947212457606)\n",
      "\n",
      "Epoch:  68%|██████▊   | 17/25 [07:23<03:28, 26.00s/it]2023-05-02 23:18:55,149 [INFO] [100000/1433083 (7%)]\tLoss: 97.89\n",
      "2023-05-02 23:18:57,131 [INFO] [200000/1433083 (14%)]\tLoss: 73.59\n",
      "2023-05-02 23:18:58,964 [INFO] [300000/1433083 (21%)]\tLoss: 62.09\n",
      "2023-05-02 23:19:00,819 [INFO] [400000/1433083 (28%)]\tLoss: 56.73\n",
      "2023-05-02 23:19:02,571 [INFO] [500000/1433083 (35%)]\tLoss: 65.70\n",
      "2023-05-02 23:19:04,224 [INFO] [600000/1433083 (42%)]\tLoss: 60.85\n",
      "2023-05-02 23:19:05,868 [INFO] [700000/1433083 (49%)]\tLoss: 56.29\n",
      "2023-05-02 23:19:07,416 [INFO] [800000/1433083 (56%)]\tLoss: 52.04\n",
      "2023-05-02 23:19:09,075 [INFO] [900000/1433083 (63%)]\tLoss: 57.69\n",
      "2023-05-02 23:19:10,574 [INFO] [1000000/1433083 (70%)]\tLoss: 68.44\n",
      "2023-05-02 23:19:11,969 [INFO] [1100000/1433083 (77%)]\tLoss: 70.59\n",
      "2023-05-02 23:19:13,363 [INFO] [1200000/1433083 (84%)]\tLoss: 78.87\n",
      "2023-05-02 23:19:14,547 [INFO] [1300000/1433083 (91%)]\tLoss: 101.07\n",
      "2023-05-02 23:19:15,651 [INFO] [1400000/1433083 (98%)]\tLoss: 187.11\n",
      "2023-05-02 23:19:16,021 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:19:18,784 [INFO] Test set accuracy: 607728/614179 (0.989496547423471)\n",
      "\n",
      "Epoch:  72%|███████▏  | 18/25 [07:49<03:02, 26.04s/it]2023-05-02 23:19:21,272 [INFO] [100000/1433083 (7%)]\tLoss: 83.65\n",
      "2023-05-02 23:19:23,373 [INFO] [200000/1433083 (14%)]\tLoss: 65.92\n",
      "2023-05-02 23:19:25,283 [INFO] [300000/1433083 (21%)]\tLoss: 56.10\n",
      "2023-05-02 23:19:27,154 [INFO] [400000/1433083 (28%)]\tLoss: 51.24\n",
      "2023-05-02 23:19:28,889 [INFO] [500000/1433083 (35%)]\tLoss: 53.87\n",
      "2023-05-02 23:19:30,539 [INFO] [600000/1433083 (42%)]\tLoss: 48.02\n",
      "2023-05-02 23:19:32,182 [INFO] [700000/1433083 (49%)]\tLoss: 43.70\n",
      "2023-05-02 23:19:33,713 [INFO] [800000/1433083 (56%)]\tLoss: 40.26\n",
      "2023-05-02 23:19:35,368 [INFO] [900000/1433083 (63%)]\tLoss: 46.01\n",
      "2023-05-02 23:19:36,857 [INFO] [1000000/1433083 (70%)]\tLoss: 59.53\n",
      "2023-05-02 23:19:38,261 [INFO] [1100000/1433083 (77%)]\tLoss: 61.54\n",
      "2023-05-02 23:19:39,573 [INFO] [1200000/1433083 (84%)]\tLoss: 69.04\n",
      "2023-05-02 23:19:40,745 [INFO] [1300000/1433083 (91%)]\tLoss: 85.83\n",
      "2023-05-02 23:19:41,832 [INFO] [1400000/1433083 (98%)]\tLoss: 163.56\n",
      "2023-05-02 23:19:42,148 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:19:44,923 [INFO] Test set accuracy: 608049/614179 (0.990019196358065)\n",
      "\n",
      "Epoch:  76%|███████▌  | 19/25 [08:15<02:36, 26.07s/it]2023-05-02 23:19:47,405 [INFO] [100000/1433083 (7%)]\tLoss: 75.37\n",
      "2023-05-02 23:19:49,508 [INFO] [200000/1433083 (14%)]\tLoss: 57.85\n",
      "2023-05-02 23:19:51,426 [INFO] [300000/1433083 (21%)]\tLoss: 49.35\n",
      "2023-05-02 23:19:53,274 [INFO] [400000/1433083 (28%)]\tLoss: 45.01\n",
      "2023-05-02 23:19:55,018 [INFO] [500000/1433083 (35%)]\tLoss: 46.88\n",
      "2023-05-02 23:19:56,671 [INFO] [600000/1433083 (42%)]\tLoss: 41.79\n",
      "2023-05-02 23:19:58,327 [INFO] [700000/1433083 (49%)]\tLoss: 38.12\n",
      "2023-05-02 23:19:59,994 [INFO] [800000/1433083 (56%)]\tLoss: 35.09\n",
      "2023-05-02 23:20:01,658 [INFO] [900000/1433083 (63%)]\tLoss: 40.54\n",
      "2023-05-02 23:20:03,151 [INFO] [1000000/1433083 (70%)]\tLoss: 56.84\n",
      "2023-05-02 23:20:04,555 [INFO] [1100000/1433083 (77%)]\tLoss: 57.98\n",
      "2023-05-02 23:20:05,877 [INFO] [1200000/1433083 (84%)]\tLoss: 64.71\n",
      "2023-05-02 23:20:07,060 [INFO] [1300000/1433083 (91%)]\tLoss: 77.10\n",
      "2023-05-02 23:20:08,140 [INFO] [1400000/1433083 (98%)]\tLoss: 158.14\n",
      "2023-05-02 23:20:08,503 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:20:11,291 [INFO] Test set accuracy: 608158/614179 (0.9901966690492511)\n",
      "\n",
      "Epoch:  80%|████████  | 20/25 [08:41<02:10, 26.16s/it]2023-05-02 23:20:13,802 [INFO] [100000/1433083 (7%)]\tLoss: 73.00\n",
      "2023-05-02 23:20:15,907 [INFO] [200000/1433083 (14%)]\tLoss: 56.25\n",
      "2023-05-02 23:20:17,815 [INFO] [300000/1433083 (21%)]\tLoss: 47.88\n",
      "2023-05-02 23:20:19,670 [INFO] [400000/1433083 (28%)]\tLoss: 43.43\n",
      "2023-05-02 23:20:21,438 [INFO] [500000/1433083 (35%)]\tLoss: 44.31\n",
      "2023-05-02 23:20:23,099 [INFO] [600000/1433083 (42%)]\tLoss: 39.34\n",
      "2023-05-02 23:20:24,742 [INFO] [700000/1433083 (49%)]\tLoss: 35.71\n",
      "2023-05-02 23:20:26,386 [INFO] [800000/1433083 (56%)]\tLoss: 32.81\n",
      "2023-05-02 23:20:28,058 [INFO] [900000/1433083 (63%)]\tLoss: 38.50\n",
      "2023-05-02 23:20:29,567 [INFO] [1000000/1433083 (70%)]\tLoss: 50.55\n",
      "2023-05-02 23:20:30,967 [INFO] [1100000/1433083 (77%)]\tLoss: 51.03\n",
      "2023-05-02 23:20:32,258 [INFO] [1200000/1433083 (84%)]\tLoss: 57.00\n",
      "2023-05-02 23:20:33,423 [INFO] [1300000/1433083 (91%)]\tLoss: 68.92\n",
      "2023-05-02 23:20:34,508 [INFO] [1400000/1433083 (98%)]\tLoss: 156.49\n",
      "2023-05-02 23:20:34,872 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:20:37,648 [INFO] Test set accuracy: 606842/614179 (0.9880539712363985)\n",
      "\n",
      "Epoch:  84%|████████▍ | 21/25 [09:08<01:44, 26.22s/it]2023-05-02 23:20:40,139 [INFO] [100000/1433083 (7%)]\tLoss: 69.89\n",
      "2023-05-02 23:20:42,228 [INFO] [200000/1433083 (14%)]\tLoss: 52.32\n",
      "2023-05-02 23:20:44,168 [INFO] [300000/1433083 (21%)]\tLoss: 44.19\n",
      "2023-05-02 23:20:46,046 [INFO] [400000/1433083 (28%)]\tLoss: 40.09\n",
      "2023-05-02 23:20:47,790 [INFO] [500000/1433083 (35%)]\tLoss: 41.02\n",
      "2023-05-02 23:20:49,437 [INFO] [600000/1433083 (42%)]\tLoss: 36.33\n",
      "2023-05-02 23:20:51,033 [INFO] [700000/1433083 (49%)]\tLoss: 32.96\n",
      "2023-05-02 23:20:52,694 [INFO] [800000/1433083 (56%)]\tLoss: 30.14\n",
      "2023-05-02 23:20:54,356 [INFO] [900000/1433083 (63%)]\tLoss: 34.62\n",
      "2023-05-02 23:20:55,845 [INFO] [1000000/1433083 (70%)]\tLoss: 42.31\n",
      "2023-05-02 23:20:57,245 [INFO] [1100000/1433083 (77%)]\tLoss: 42.89\n",
      "2023-05-02 23:20:58,553 [INFO] [1200000/1433083 (84%)]\tLoss: 48.44\n",
      "2023-05-02 23:20:59,721 [INFO] [1300000/1433083 (91%)]\tLoss: 65.03\n",
      "2023-05-02 23:21:00,811 [INFO] [1400000/1433083 (98%)]\tLoss: 127.43\n",
      "2023-05-02 23:21:01,173 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:21:03,952 [INFO] Test set accuracy: 608407/614179 (0.9906020883162726)\n",
      "\n",
      "Epoch:  88%|████████▊ | 22/25 [09:34<01:18, 26.24s/it]2023-05-02 23:21:06,453 [INFO] [100000/1433083 (7%)]\tLoss: 1755.77\n",
      "2023-05-02 23:21:08,549 [INFO] [200000/1433083 (14%)]\tLoss: 925.94\n",
      "2023-05-02 23:21:10,476 [INFO] [300000/1433083 (21%)]\tLoss: 642.44\n",
      "2023-05-02 23:21:12,344 [INFO] [400000/1433083 (28%)]\tLoss: 507.39\n",
      "2023-05-02 23:21:14,114 [INFO] [500000/1433083 (35%)]\tLoss: 425.70\n",
      "2023-05-02 23:21:15,746 [INFO] [600000/1433083 (42%)]\tLoss: 359.91\n",
      "2023-05-02 23:21:17,401 [INFO] [700000/1433083 (49%)]\tLoss: 313.02\n",
      "2023-05-02 23:21:19,061 [INFO] [800000/1433083 (56%)]\tLoss: 277.03\n",
      "2023-05-02 23:21:20,727 [INFO] [900000/1433083 (63%)]\tLoss: 258.65\n",
      "2023-05-02 23:21:22,221 [INFO] [1000000/1433083 (70%)]\tLoss: 249.41\n",
      "2023-05-02 23:21:23,629 [INFO] [1100000/1433083 (77%)]\tLoss: 235.19\n",
      "2023-05-02 23:21:24,951 [INFO] [1200000/1433083 (84%)]\tLoss: 228.55\n",
      "2023-05-02 23:21:26,217 [INFO] [1300000/1433083 (91%)]\tLoss: 229.18\n",
      "2023-05-02 23:21:27,294 [INFO] [1400000/1433083 (98%)]\tLoss: 278.00\n",
      "2023-05-02 23:21:27,653 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:21:30,433 [INFO] Test set accuracy: 608563/614179 (0.9908560859293464)\n",
      "\n",
      "Epoch:  92%|█████████▏| 23/25 [10:00<00:52, 26.32s/it]2023-05-02 23:21:32,935 [INFO] [100000/1433083 (7%)]\tLoss: 80.99\n",
      "2023-05-02 23:21:35,020 [INFO] [200000/1433083 (14%)]\tLoss: 59.36\n",
      "2023-05-02 23:21:36,944 [INFO] [300000/1433083 (21%)]\tLoss: 49.32\n",
      "2023-05-02 23:21:38,790 [INFO] [400000/1433083 (28%)]\tLoss: 44.59\n",
      "2023-05-02 23:21:40,524 [INFO] [500000/1433083 (35%)]\tLoss: 45.59\n",
      "2023-05-02 23:21:42,174 [INFO] [600000/1433083 (42%)]\tLoss: 40.53\n",
      "2023-05-02 23:21:43,816 [INFO] [700000/1433083 (49%)]\tLoss: 37.00\n",
      "2023-05-02 23:21:45,443 [INFO] [800000/1433083 (56%)]\tLoss: 34.00\n",
      "2023-05-02 23:21:47,105 [INFO] [900000/1433083 (63%)]\tLoss: 38.10\n",
      "2023-05-02 23:21:48,477 [INFO] [1000000/1433083 (70%)]\tLoss: 47.13\n",
      "2023-05-02 23:21:49,771 [INFO] [1100000/1433083 (77%)]\tLoss: 48.01\n",
      "2023-05-02 23:21:50,974 [INFO] [1200000/1433083 (84%)]\tLoss: 52.97\n",
      "2023-05-02 23:21:52,034 [INFO] [1300000/1433083 (91%)]\tLoss: 63.42\n",
      "2023-05-02 23:21:53,012 [INFO] [1400000/1433083 (98%)]\tLoss: 120.38\n",
      "2023-05-02 23:21:53,350 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:21:56,120 [INFO] Test set accuracy: 608936/614179 (0.9914634007349649)\n",
      "\n",
      "Epoch:  96%|█████████▌| 24/25 [10:26<00:26, 26.13s/it]2023-05-02 23:21:58,526 [INFO] [100000/1433083 (7%)]\tLoss: 56.42\n",
      "2023-05-02 23:22:00,506 [INFO] [200000/1433083 (14%)]\tLoss: 43.14\n",
      "2023-05-02 23:22:02,410 [INFO] [300000/1433083 (21%)]\tLoss: 35.67\n",
      "2023-05-02 23:22:04,257 [INFO] [400000/1433083 (28%)]\tLoss: 32.36\n",
      "2023-05-02 23:22:05,989 [INFO] [500000/1433083 (35%)]\tLoss: 33.78\n",
      "2023-05-02 23:22:07,646 [INFO] [600000/1433083 (42%)]\tLoss: 30.03\n",
      "2023-05-02 23:22:09,309 [INFO] [700000/1433083 (49%)]\tLoss: 27.45\n",
      "2023-05-02 23:22:10,960 [INFO] [800000/1433083 (56%)]\tLoss: 25.17\n",
      "2023-05-02 23:22:12,566 [INFO] [900000/1433083 (63%)]\tLoss: 29.04\n",
      "2023-05-02 23:22:14,044 [INFO] [1000000/1433083 (70%)]\tLoss: 39.98\n",
      "2023-05-02 23:22:15,462 [INFO] [1100000/1433083 (77%)]\tLoss: 40.48\n",
      "2023-05-02 23:22:16,776 [INFO] [1200000/1433083 (84%)]\tLoss: 44.60\n",
      "2023-05-02 23:22:17,947 [INFO] [1300000/1433083 (91%)]\tLoss: 55.42\n",
      "2023-05-02 23:22:19,044 [INFO] [1400000/1433083 (98%)]\tLoss: 113.09\n",
      "2023-05-02 23:22:19,413 [INFO] Evaluating trained model ...\n",
      "2023-05-02 23:22:22,187 [INFO] Test set accuracy: 609149/614179 (0.9918102051682002)\n",
      "\n",
      "Epoch: 100%|██████████| 25/25 [10:52<00:00, 26.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 16s, sys: 19.6 s, total: 21min 35s\n",
      "Wall time: 10min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dd.train_model(train_data, labels, batch_size=BATCH_SIZE, epochs=EPOCHS, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "Save pretrained model to a given output location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 23:22:22,210 [INFO] Pretrained model checkpoint saved to location: 'models/rnn_classifier_2023-05-02_23_22_22.bin'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory 'models'\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODELS_DIR):\n",
    "    print(\"Creating directory '{}'\".format(MODELS_DIR))\n",
    "    os.makedirs(MODELS_DIR)\n",
    "\n",
    "now = datetime.now()\n",
    "model_filename = \"rnn_classifier_{}.bin\".format(now.strftime(\"%Y-%m-%d_%H_%M_%S\"))\n",
    "model_filepath = os.path.join(MODELS_DIR, model_filename)\n",
    "dd.save_checkpoint(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model generated above, we now score the test dataset against the model to determine if the domain is likely generated by a DGA or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 23:22:22,232 [INFO] Found GPU's now setting up cuda for the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9931111288402892\n"
     ]
    }
   ],
   "source": [
    "dga_detector = DGADetector()\n",
    "dga_detector.load_checkpoint(model_filepath)\n",
    "\n",
    "domain_train, domain_test, type_train, type_test = train_test_split(gdf, \"type\", train_size=0.7)\n",
    "test_df = cudf.DataFrame()\n",
    "test_df[\"type\"] = type_test.reset_index(drop=True)\n",
    "test_df[\"domain\"] = domain_test.reset_index(drop=True)\n",
    "\n",
    "test_dataset = DGADataset(test_df, 100)\n",
    "test_dataloader = DataLoader(test_dataset, batchsize=BATCH_SIZE)\n",
    "\n",
    "pred_results = []\n",
    "true_results = []\n",
    "for chunk in test_dataloader.get_chunks():\n",
    "    pred_results.append(list(dga_detector.predict(chunk['domain']).values_host))\n",
    "    true_results.append(list(chunk['type'].values_host))\n",
    "pred_results = np.concatenate(pred_results)\n",
    "true_results = np.concatenate(true_results)\n",
    "accuracy_score_result = accuracy_score(pred_results, true_results)\n",
    "\n",
    "print('Model accuracy: %s'%(accuracy_score_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.982\n"
     ]
    }
   ],
   "source": [
    "average_precision = average_precision_score(true_results, pred_results)\n",
    "\n",
    "print('Average precision score: {0:0.3f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DGA detection implementation enables users to train their models for detection and also use existing models. This capability could also be used in conjunction with log parsing efforts if the logs contain domain names. Data is kept in GPU memory, removing unnecessary copy/converts and providing a 4X speed advantage over CPU only implementations. This is esepcially true with large batch sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
