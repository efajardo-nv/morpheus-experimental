{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Generation Algorithm (DGA) Detection\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Data Importing\n",
    "* Data Preprocessing\n",
    "* Training and Evaluation\n",
    "* Inference\n",
    "* Conclusion\n",
    "\n",
    "## Introduction\n",
    "[Domain Generation Algorithms](https://en.wikipedia.org/wiki/Domain_generation_algorithm) (DGAs) are used to generate domain names that can be used by the malware to communicate with the command and control servers. IP addresses and static domain names can be easily blocked, and a DGA provides an easy method to generate a large number of domain names and rotate through them to circumvent traditional block lists. We will use a type of recurrent neural network called the [Gated Recurrent Unit](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) (GRU) for this example. This implementation enables users to train their models with up-to-date domain names representative of both benign and DGA generated strings. This capability could also be used in production. This notebook provides a view into the data science workflow to create a DGA detection implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import torch\n",
    "import requests\n",
    "import logging\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from dga_detector import DGADetector\n",
    "from dataloader import DataLoader\n",
    "from dga_dataset import DGADataset\n",
    "from cuml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable console logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Input Dataset from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_CSV = \"../datasets/benign_and_dga_domains.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Benign and DGA dataset\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    r = requests.get(DATA_BASE_URL + INPUT_CSV)\n",
    "    open(INPUT_CSV, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Input Dataset to GPU Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = cudf.read_csv(INPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = gdf['domain']\n",
    "labels = gdf['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have only benign and DGA (malicious) categoriesm, the number of domain types need to be set to 2 (`N_DOMAIN_TYPE=2`). Vocabulary size(`CHAR_VOCAB`) is set to 128 ASCII characters. The values below set for `HIDDEN_SIZE`, `N_LAYERS` of the network, and the `LR` (Learning Rate) give an optimum balance for the network size and performance. They might need be set via experiments when working with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "N_LAYERS = 3\n",
    "CHAR_VOCAB = 128\n",
    "HIDDEN_SIZE = 100\n",
    "N_DOMAIN_TYPE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate DGA detector\n",
    "Now that the data is ready, the datasets are created, and we've set the parameters for the model, we can use the DGA detector method to create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 21:06:44,204 [INFO] Found GPU's now setting up cuda for the model\n"
     ]
    }
   ],
   "source": [
    "dd = DGADetector(lr=LR)\n",
    "dd.init_model(n_layers=N_LAYERS, char_vocab=CHAR_VOCAB, hidden_size=HIDDEN_SIZE, n_domain_type=N_DOMAIN_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "TRAIN_SIZE = 0.7\n",
    "BATCH_SIZE = 10000\n",
    "MODELS_DIR = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Now we train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 21:06:46,283 [INFO] Initiating model training ...\n",
      "2023-05-02 21:06:46,284 [INFO] Truncate domains to width: 100\n",
      "/opt/conda/envs/mor_exp/lib/python3.8/site-packages/cudf/core/column/string.py:3563: FutureWarning: The expand parameter is deprecated and will be removed in a future version. Set expand=False to match future behavior.\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/25 [00:00<?, ?it/s]2023-05-02 21:06:49,969 [INFO] [100000/1433083 (7%)]\tLoss: 6938.24\n",
      "2023-05-02 21:06:52,000 [INFO] [200000/1433083 (14%)]\tLoss: 5141.84\n",
      "2023-05-02 21:06:53,884 [INFO] [300000/1433083 (21%)]\tLoss: 3975.97\n",
      "2023-05-02 21:06:55,626 [INFO] [400000/1433083 (28%)]\tLoss: 3446.93\n",
      "2023-05-02 21:06:57,307 [INFO] [500000/1433083 (35%)]\tLoss: 3107.64\n",
      "2023-05-02 21:06:58,829 [INFO] [600000/1433083 (42%)]\tLoss: 2693.71\n",
      "2023-05-02 21:07:00,312 [INFO] [700000/1433083 (49%)]\tLoss: 2367.30\n",
      "2023-05-02 21:07:01,940 [INFO] [800000/1433083 (56%)]\tLoss: 2107.43\n",
      "2023-05-02 21:07:03,584 [INFO] [900000/1433083 (63%)]\tLoss: 2229.94\n",
      "2023-05-02 21:07:05,054 [INFO] [1000000/1433083 (70%)]\tLoss: 2121.57\n",
      "2023-05-02 21:07:06,448 [INFO] [1100000/1433083 (77%)]\tLoss: 2148.78\n",
      "2023-05-02 21:07:07,757 [INFO] [1200000/1433083 (84%)]\tLoss: 2090.81\n",
      "2023-05-02 21:07:08,907 [INFO] [1300000/1433083 (91%)]\tLoss: 2088.02\n",
      "2023-05-02 21:07:09,926 [INFO] [1400000/1433083 (98%)]\tLoss: 2129.22\n",
      "2023-05-02 21:07:10,277 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:07:13,017 [INFO] Test set accuracy: 429074/614179 (0.6986139219999381)\n",
      "\n",
      "Epoch:   4%|▍         | 1/25 [00:25<10:18, 25.76s/it]2023-05-02 21:07:15,487 [INFO] [100000/1433083 (7%)]\tLoss: 4071.65\n",
      "2023-05-02 21:07:17,531 [INFO] [200000/1433083 (14%)]\tLoss: 2970.02\n",
      "2023-05-02 21:07:19,445 [INFO] [300000/1433083 (21%)]\tLoss: 2221.69\n",
      "2023-05-02 21:07:21,276 [INFO] [400000/1433083 (28%)]\tLoss: 1901.18\n",
      "2023-05-02 21:07:22,978 [INFO] [500000/1433083 (35%)]\tLoss: 1661.90\n",
      "2023-05-02 21:07:24,586 [INFO] [600000/1433083 (42%)]\tLoss: 1421.06\n",
      "2023-05-02 21:07:26,195 [INFO] [700000/1433083 (49%)]\tLoss: 1240.94\n",
      "2023-05-02 21:07:27,731 [INFO] [800000/1433083 (56%)]\tLoss: 1102.33\n",
      "2023-05-02 21:07:29,378 [INFO] [900000/1433083 (63%)]\tLoss: 1046.16\n",
      "2023-05-02 21:07:30,861 [INFO] [1000000/1433083 (70%)]\tLoss: 1053.48\n",
      "2023-05-02 21:07:32,248 [INFO] [1100000/1433083 (77%)]\tLoss: 1021.55\n",
      "2023-05-02 21:07:33,541 [INFO] [1200000/1433083 (84%)]\tLoss: 1014.34\n",
      "2023-05-02 21:07:34,701 [INFO] [1300000/1433083 (91%)]\tLoss: 1057.18\n",
      "2023-05-02 21:07:35,730 [INFO] [1400000/1433083 (98%)]\tLoss: 1149.44\n",
      "2023-05-02 21:07:36,088 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:07:38,833 [INFO] Test set accuracy: 545656/614179 (0.8884315484573716)\n",
      "\n",
      "Epoch:   8%|▊         | 2/25 [00:51<09:53, 25.79s/it]2023-05-02 21:07:41,368 [INFO] [100000/1433083 (7%)]\tLoss: 2715.71\n",
      "2023-05-02 21:07:43,468 [INFO] [200000/1433083 (14%)]\tLoss: 1940.46\n",
      "2023-05-02 21:07:45,393 [INFO] [300000/1433083 (21%)]\tLoss: 1460.89\n",
      "2023-05-02 21:07:47,158 [INFO] [400000/1433083 (28%)]\tLoss: 1247.39\n",
      "2023-05-02 21:07:48,872 [INFO] [500000/1433083 (35%)]\tLoss: 1090.03\n",
      "2023-05-02 21:07:50,511 [INFO] [600000/1433083 (42%)]\tLoss: 931.40\n",
      "2023-05-02 21:07:52,166 [INFO] [700000/1433083 (49%)]\tLoss: 814.62\n",
      "2023-05-02 21:07:53,808 [INFO] [800000/1433083 (56%)]\tLoss: 725.56\n",
      "2023-05-02 21:07:55,457 [INFO] [900000/1433083 (63%)]\tLoss: 701.58\n",
      "2023-05-02 21:07:56,957 [INFO] [1000000/1433083 (70%)]\tLoss: 706.02\n",
      "2023-05-02 21:07:58,357 [INFO] [1100000/1433083 (77%)]\tLoss: 693.12\n",
      "2023-05-02 21:07:59,763 [INFO] [1200000/1433083 (84%)]\tLoss: 707.10\n",
      "2023-05-02 21:08:00,928 [INFO] [1300000/1433083 (91%)]\tLoss: 754.04\n",
      "2023-05-02 21:08:01,951 [INFO] [1400000/1433083 (98%)]\tLoss: 857.59\n",
      "2023-05-02 21:08:02,325 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:08:05,150 [INFO] Test set accuracy: 586995/614179 (0.9557392877320781)\n",
      "\n",
      "Epoch:  12%|█▏        | 3/25 [01:17<09:32, 26.03s/it]2023-05-02 21:08:07,665 [INFO] [100000/1433083 (7%)]\tLoss: 1647.01\n",
      "2023-05-02 21:08:09,782 [INFO] [200000/1433083 (14%)]\tLoss: 1217.58\n",
      "2023-05-02 21:08:11,716 [INFO] [300000/1433083 (21%)]\tLoss: 941.68\n",
      "2023-05-02 21:08:13,571 [INFO] [400000/1433083 (28%)]\tLoss: 806.35\n",
      "2023-05-02 21:08:15,310 [INFO] [500000/1433083 (35%)]\tLoss: 722.85\n",
      "2023-05-02 21:08:16,897 [INFO] [600000/1433083 (42%)]\tLoss: 620.60\n",
      "2023-05-02 21:08:18,438 [INFO] [700000/1433083 (49%)]\tLoss: 545.45\n",
      "2023-05-02 21:08:19,990 [INFO] [800000/1433083 (56%)]\tLoss: 488.51\n",
      "2023-05-02 21:08:21,571 [INFO] [900000/1433083 (63%)]\tLoss: 484.88\n",
      "2023-05-02 21:08:22,951 [INFO] [1000000/1433083 (70%)]\tLoss: 509.56\n",
      "2023-05-02 21:08:24,351 [INFO] [1100000/1433083 (77%)]\tLoss: 504.91\n",
      "2023-05-02 21:08:25,643 [INFO] [1200000/1433083 (84%)]\tLoss: 539.45\n",
      "2023-05-02 21:08:26,803 [INFO] [1300000/1433083 (91%)]\tLoss: 583.36\n",
      "2023-05-02 21:08:27,832 [INFO] [1400000/1433083 (98%)]\tLoss: 694.61\n",
      "2023-05-02 21:08:28,173 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:08:30,963 [INFO] Test set accuracy: 593487/614179 (0.9663094960915303)\n",
      "\n",
      "Epoch:  16%|█▌        | 4/25 [01:43<09:04, 25.95s/it]2023-05-02 21:08:33,459 [INFO] [100000/1433083 (7%)]\tLoss: 336.31\n",
      "2023-05-02 21:08:35,587 [INFO] [200000/1433083 (14%)]\tLoss: 421.51\n",
      "2023-05-02 21:08:37,522 [INFO] [300000/1433083 (21%)]\tLoss: 364.64\n",
      "2023-05-02 21:08:39,384 [INFO] [400000/1433083 (28%)]\tLoss: 351.95\n",
      "2023-05-02 21:08:41,128 [INFO] [500000/1433083 (35%)]\tLoss: 340.86\n",
      "2023-05-02 21:08:42,785 [INFO] [600000/1433083 (42%)]\tLoss: 296.94\n",
      "2023-05-02 21:08:44,429 [INFO] [700000/1433083 (49%)]\tLoss: 264.87\n",
      "2023-05-02 21:08:46,070 [INFO] [800000/1433083 (56%)]\tLoss: 240.79\n",
      "2023-05-02 21:08:47,732 [INFO] [900000/1433083 (63%)]\tLoss: 257.65\n",
      "2023-05-02 21:08:49,231 [INFO] [1000000/1433083 (70%)]\tLoss: 289.05\n",
      "2023-05-02 21:08:50,645 [INFO] [1100000/1433083 (77%)]\tLoss: 296.48\n",
      "2023-05-02 21:08:51,960 [INFO] [1200000/1433083 (84%)]\tLoss: 339.00\n",
      "2023-05-02 21:08:53,124 [INFO] [1300000/1433083 (91%)]\tLoss: 387.47\n",
      "2023-05-02 21:08:54,150 [INFO] [1400000/1433083 (98%)]\tLoss: 528.46\n",
      "2023-05-02 21:08:54,518 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:08:57,316 [INFO] Test set accuracy: 594241/614179 (0.9675371512213866)\n",
      "\n",
      "Epoch:  20%|██        | 5/25 [02:10<08:41, 26.09s/it]2023-05-02 21:08:59,799 [INFO] [100000/1433083 (7%)]\tLoss: 267.67\n",
      "2023-05-02 21:09:01,875 [INFO] [200000/1433083 (14%)]\tLoss: 253.31\n",
      "2023-05-02 21:09:03,857 [INFO] [300000/1433083 (21%)]\tLoss: 236.54\n",
      "2023-05-02 21:09:05,695 [INFO] [400000/1433083 (28%)]\tLoss: 227.10\n",
      "2023-05-02 21:09:07,458 [INFO] [500000/1433083 (35%)]\tLoss: 229.97\n",
      "2023-05-02 21:09:09,116 [INFO] [600000/1433083 (42%)]\tLoss: 201.33\n",
      "2023-05-02 21:09:10,763 [INFO] [700000/1433083 (49%)]\tLoss: 181.00\n",
      "2023-05-02 21:09:12,433 [INFO] [800000/1433083 (56%)]\tLoss: 165.98\n",
      "2023-05-02 21:09:14,113 [INFO] [900000/1433083 (63%)]\tLoss: 186.20\n",
      "2023-05-02 21:09:15,590 [INFO] [1000000/1433083 (70%)]\tLoss: 220.54\n",
      "2023-05-02 21:09:16,994 [INFO] [1100000/1433083 (77%)]\tLoss: 230.76\n",
      "2023-05-02 21:09:18,307 [INFO] [1200000/1433083 (84%)]\tLoss: 268.28\n",
      "2023-05-02 21:09:19,477 [INFO] [1300000/1433083 (91%)]\tLoss: 310.62\n",
      "2023-05-02 21:09:20,495 [INFO] [1400000/1433083 (98%)]\tLoss: 472.90\n",
      "2023-05-02 21:09:20,857 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:09:23,656 [INFO] Test set accuracy: 587022/614179 (0.9557832488574178)\n",
      "\n",
      "Epoch:  24%|██▍       | 6/25 [02:36<08:17, 26.18s/it]2023-05-02 21:09:26,171 [INFO] [100000/1433083 (7%)]\tLoss: 220.94\n",
      "2023-05-02 21:09:28,279 [INFO] [200000/1433083 (14%)]\tLoss: 214.27\n",
      "2023-05-02 21:09:30,227 [INFO] [300000/1433083 (21%)]\tLoss: 204.82\n",
      "2023-05-02 21:09:32,094 [INFO] [400000/1433083 (28%)]\tLoss: 204.42\n",
      "2023-05-02 21:09:33,866 [INFO] [500000/1433083 (35%)]\tLoss: 208.04\n",
      "2023-05-02 21:09:35,494 [INFO] [600000/1433083 (42%)]\tLoss: 182.19\n",
      "2023-05-02 21:09:37,147 [INFO] [700000/1433083 (49%)]\tLoss: 163.98\n",
      "2023-05-02 21:09:38,795 [INFO] [800000/1433083 (56%)]\tLoss: 150.59\n",
      "2023-05-02 21:09:40,462 [INFO] [900000/1433083 (63%)]\tLoss: 168.31\n",
      "2023-05-02 21:09:41,874 [INFO] [1000000/1433083 (70%)]\tLoss: 197.55\n",
      "2023-05-02 21:09:43,178 [INFO] [1100000/1433083 (77%)]\tLoss: 205.97\n",
      "2023-05-02 21:09:44,381 [INFO] [1200000/1433083 (84%)]\tLoss: 239.58\n",
      "2023-05-02 21:09:45,449 [INFO] [1300000/1433083 (91%)]\tLoss: 278.76\n",
      "2023-05-02 21:09:46,417 [INFO] [1400000/1433083 (98%)]\tLoss: 410.61\n",
      "2023-05-02 21:09:46,737 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:09:49,569 [INFO] Test set accuracy: 597172/614179 (0.9723093756054831)\n",
      "\n",
      "Epoch:  28%|██▊       | 7/25 [03:02<07:49, 26.09s/it]2023-05-02 21:09:51,983 [INFO] [100000/1433083 (7%)]\tLoss: 254.59\n",
      "2023-05-02 21:09:54,067 [INFO] [200000/1433083 (14%)]\tLoss: 221.33\n",
      "2023-05-02 21:09:56,018 [INFO] [300000/1433083 (21%)]\tLoss: 204.62\n",
      "2023-05-02 21:09:57,877 [INFO] [400000/1433083 (28%)]\tLoss: 193.26\n",
      "2023-05-02 21:09:59,633 [INFO] [500000/1433083 (35%)]\tLoss: 194.41\n",
      "2023-05-02 21:10:01,281 [INFO] [600000/1433083 (42%)]\tLoss: 169.73\n",
      "2023-05-02 21:10:02,932 [INFO] [700000/1433083 (49%)]\tLoss: 152.55\n",
      "2023-05-02 21:10:04,566 [INFO] [800000/1433083 (56%)]\tLoss: 139.90\n",
      "2023-05-02 21:10:06,226 [INFO] [900000/1433083 (63%)]\tLoss: 154.94\n",
      "2023-05-02 21:10:07,737 [INFO] [1000000/1433083 (70%)]\tLoss: 186.91\n",
      "2023-05-02 21:10:09,135 [INFO] [1100000/1433083 (77%)]\tLoss: 194.28\n",
      "2023-05-02 21:10:10,556 [INFO] [1200000/1433083 (84%)]\tLoss: 223.05\n",
      "2023-05-02 21:10:11,727 [INFO] [1300000/1433083 (91%)]\tLoss: 258.56\n",
      "2023-05-02 21:10:12,774 [INFO] [1400000/1433083 (98%)]\tLoss: 387.09\n",
      "2023-05-02 21:10:13,131 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:10:15,908 [INFO] Test set accuracy: 599615/614179 (0.9762870433538106)\n",
      "\n",
      "Epoch:  32%|███▏      | 8/25 [03:28<07:24, 26.17s/it]2023-05-02 21:10:18,406 [INFO] [100000/1433083 (7%)]\tLoss: 178.59\n",
      "2023-05-02 21:10:20,505 [INFO] [200000/1433083 (14%)]\tLoss: 167.93\n",
      "2023-05-02 21:10:22,443 [INFO] [300000/1433083 (21%)]\tLoss: 159.94\n",
      "2023-05-02 21:10:24,312 [INFO] [400000/1433083 (28%)]\tLoss: 154.19\n",
      "2023-05-02 21:10:26,044 [INFO] [500000/1433083 (35%)]\tLoss: 158.38\n",
      "2023-05-02 21:10:27,703 [INFO] [600000/1433083 (42%)]\tLoss: 138.82\n",
      "2023-05-02 21:10:29,350 [INFO] [700000/1433083 (49%)]\tLoss: 125.31\n",
      "2023-05-02 21:10:30,997 [INFO] [800000/1433083 (56%)]\tLoss: 115.31\n",
      "2023-05-02 21:10:32,642 [INFO] [900000/1433083 (63%)]\tLoss: 129.49\n",
      "2023-05-02 21:10:34,131 [INFO] [1000000/1433083 (70%)]\tLoss: 177.84\n",
      "2023-05-02 21:10:35,552 [INFO] [1100000/1433083 (77%)]\tLoss: 184.50\n",
      "2023-05-02 21:10:36,854 [INFO] [1200000/1433083 (84%)]\tLoss: 208.02\n",
      "2023-05-02 21:10:38,022 [INFO] [1300000/1433083 (91%)]\tLoss: 240.10\n",
      "2023-05-02 21:10:39,040 [INFO] [1400000/1433083 (98%)]\tLoss: 377.86\n",
      "2023-05-02 21:10:39,401 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:10:42,219 [INFO] Test set accuracy: 590252/614179 (0.9610423019999056)\n",
      "\n",
      "Epoch:  36%|███▌      | 9/25 [03:54<06:59, 26.21s/it]2023-05-02 21:10:44,658 [INFO] [100000/1433083 (7%)]\tLoss: 166.89\n",
      "2023-05-02 21:10:46,676 [INFO] [200000/1433083 (14%)]\tLoss: 158.33\n",
      "2023-05-02 21:10:48,597 [INFO] [300000/1433083 (21%)]\tLoss: 149.62\n",
      "2023-05-02 21:10:50,461 [INFO] [400000/1433083 (28%)]\tLoss: 143.75\n",
      "2023-05-02 21:10:52,206 [INFO] [500000/1433083 (35%)]\tLoss: 146.58\n",
      "2023-05-02 21:10:53,861 [INFO] [600000/1433083 (42%)]\tLoss: 128.54\n",
      "2023-05-02 21:10:55,511 [INFO] [700000/1433083 (49%)]\tLoss: 115.94\n",
      "2023-05-02 21:10:57,161 [INFO] [800000/1433083 (56%)]\tLoss: 106.61\n",
      "2023-05-02 21:10:58,837 [INFO] [900000/1433083 (63%)]\tLoss: 119.12\n",
      "2023-05-02 21:11:00,331 [INFO] [1000000/1433083 (70%)]\tLoss: 141.55\n",
      "2023-05-02 21:11:01,744 [INFO] [1100000/1433083 (77%)]\tLoss: 147.59\n",
      "2023-05-02 21:11:03,045 [INFO] [1200000/1433083 (84%)]\tLoss: 173.48\n",
      "2023-05-02 21:11:04,241 [INFO] [1300000/1433083 (91%)]\tLoss: 203.47\n",
      "2023-05-02 21:11:05,267 [INFO] [1400000/1433083 (98%)]\tLoss: 333.74\n",
      "2023-05-02 21:11:05,639 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:11:08,459 [INFO] Test set accuracy: 598655/614179 (0.9747239811195108)\n",
      "\n",
      "Epoch:  40%|████      | 10/25 [04:21<06:33, 26.22s/it]2023-05-02 21:11:10,972 [INFO] [100000/1433083 (7%)]\tLoss: 149.92\n",
      "2023-05-02 21:11:13,078 [INFO] [200000/1433083 (14%)]\tLoss: 139.48\n",
      "2023-05-02 21:11:15,004 [INFO] [300000/1433083 (21%)]\tLoss: 131.34\n",
      "2023-05-02 21:11:16,867 [INFO] [400000/1433083 (28%)]\tLoss: 124.71\n",
      "2023-05-02 21:11:18,605 [INFO] [500000/1433083 (35%)]\tLoss: 126.98\n",
      "2023-05-02 21:11:20,253 [INFO] [600000/1433083 (42%)]\tLoss: 111.39\n",
      "2023-05-02 21:11:21,807 [INFO] [700000/1433083 (49%)]\tLoss: 100.47\n",
      "2023-05-02 21:11:23,457 [INFO] [800000/1433083 (56%)]\tLoss: 92.44\n",
      "2023-05-02 21:11:25,123 [INFO] [900000/1433083 (63%)]\tLoss: 103.90\n",
      "2023-05-02 21:11:26,596 [INFO] [1000000/1433083 (70%)]\tLoss: 124.81\n",
      "2023-05-02 21:11:28,009 [INFO] [1100000/1433083 (77%)]\tLoss: 130.31\n",
      "2023-05-02 21:11:29,330 [INFO] [1200000/1433083 (84%)]\tLoss: 151.64\n",
      "2023-05-02 21:11:30,508 [INFO] [1300000/1433083 (91%)]\tLoss: 180.29\n",
      "2023-05-02 21:11:31,543 [INFO] [1400000/1433083 (98%)]\tLoss: 297.21\n",
      "2023-05-02 21:11:31,905 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:11:34,690 [INFO] Test set accuracy: 603161/614179 (0.9820606044817554)\n",
      "\n",
      "Epoch:  44%|████▍     | 11/25 [04:47<06:07, 26.22s/it]2023-05-02 21:11:37,174 [INFO] [100000/1433083 (7%)]\tLoss: 164.02\n",
      "2023-05-02 21:11:39,297 [INFO] [200000/1433083 (14%)]\tLoss: 140.60\n",
      "2023-05-02 21:11:41,225 [INFO] [300000/1433083 (21%)]\tLoss: 127.24\n",
      "2023-05-02 21:11:43,086 [INFO] [400000/1433083 (28%)]\tLoss: 118.49\n",
      "2023-05-02 21:11:44,722 [INFO] [500000/1433083 (35%)]\tLoss: 119.04\n",
      "2023-05-02 21:11:46,255 [INFO] [600000/1433083 (42%)]\tLoss: 104.26\n",
      "2023-05-02 21:11:47,863 [INFO] [700000/1433083 (49%)]\tLoss: 93.93\n",
      "2023-05-02 21:11:49,510 [INFO] [800000/1433083 (56%)]\tLoss: 86.24\n",
      "2023-05-02 21:11:51,178 [INFO] [900000/1433083 (63%)]\tLoss: 96.12\n",
      "2023-05-02 21:11:52,673 [INFO] [1000000/1433083 (70%)]\tLoss: 117.01\n",
      "2023-05-02 21:11:54,082 [INFO] [1100000/1433083 (77%)]\tLoss: 176.77\n",
      "2023-05-02 21:11:55,382 [INFO] [1200000/1433083 (84%)]\tLoss: 198.30\n",
      "2023-05-02 21:11:56,558 [INFO] [1300000/1433083 (91%)]\tLoss: 225.68\n",
      "2023-05-02 21:11:57,593 [INFO] [1400000/1433083 (98%)]\tLoss: 318.92\n",
      "2023-05-02 21:11:57,956 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:12:00,715 [INFO] Test set accuracy: 605680/614179 (0.9861620146569648)\n",
      "\n",
      "Epoch:  48%|████▊     | 12/25 [05:13<05:40, 26.16s/it]2023-05-02 21:12:03,187 [INFO] [100000/1433083 (7%)]\tLoss: 171.53\n",
      "2023-05-02 21:12:05,295 [INFO] [200000/1433083 (14%)]\tLoss: 141.07\n",
      "2023-05-02 21:12:07,245 [INFO] [300000/1433083 (21%)]\tLoss: 126.62\n",
      "2023-05-02 21:12:09,082 [INFO] [400000/1433083 (28%)]\tLoss: 116.64\n",
      "2023-05-02 21:12:10,828 [INFO] [500000/1433083 (35%)]\tLoss: 116.56\n",
      "2023-05-02 21:12:12,489 [INFO] [600000/1433083 (42%)]\tLoss: 102.05\n",
      "2023-05-02 21:12:14,160 [INFO] [700000/1433083 (49%)]\tLoss: 91.80\n",
      "2023-05-02 21:12:15,824 [INFO] [800000/1433083 (56%)]\tLoss: 84.15\n",
      "2023-05-02 21:12:17,481 [INFO] [900000/1433083 (63%)]\tLoss: 92.98\n",
      "2023-05-02 21:12:18,979 [INFO] [1000000/1433083 (70%)]\tLoss: 109.37\n",
      "2023-05-02 21:12:20,374 [INFO] [1100000/1433083 (77%)]\tLoss: 113.72\n",
      "2023-05-02 21:12:21,668 [INFO] [1200000/1433083 (84%)]\tLoss: 130.90\n",
      "2023-05-02 21:12:22,801 [INFO] [1300000/1433083 (91%)]\tLoss: 157.59\n",
      "2023-05-02 21:12:23,833 [INFO] [1400000/1433083 (98%)]\tLoss: 271.72\n",
      "2023-05-02 21:12:24,194 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:12:26,998 [INFO] Test set accuracy: 604946/614179 (0.9849669233236564)\n",
      "\n",
      "Epoch:  52%|█████▏    | 13/25 [05:39<05:14, 26.20s/it]2023-05-02 21:12:29,499 [INFO] [100000/1433083 (7%)]\tLoss: 133.05\n",
      "2023-05-02 21:12:31,534 [INFO] [200000/1433083 (14%)]\tLoss: 110.77\n",
      "2023-05-02 21:12:33,439 [INFO] [300000/1433083 (21%)]\tLoss: 100.64\n",
      "2023-05-02 21:12:35,298 [INFO] [400000/1433083 (28%)]\tLoss: 93.69\n",
      "2023-05-02 21:12:37,055 [INFO] [500000/1433083 (35%)]\tLoss: 99.21\n",
      "2023-05-02 21:12:38,698 [INFO] [600000/1433083 (42%)]\tLoss: 87.11\n",
      "2023-05-02 21:12:40,347 [INFO] [700000/1433083 (49%)]\tLoss: 78.69\n",
      "2023-05-02 21:12:42,006 [INFO] [800000/1433083 (56%)]\tLoss: 72.27\n",
      "2023-05-02 21:12:43,617 [INFO] [900000/1433083 (63%)]\tLoss: 80.76\n",
      "2023-05-02 21:12:45,057 [INFO] [1000000/1433083 (70%)]\tLoss: 95.67\n",
      "2023-05-02 21:12:46,473 [INFO] [1100000/1433083 (77%)]\tLoss: 99.69\n",
      "2023-05-02 21:12:47,787 [INFO] [1200000/1433083 (84%)]\tLoss: 116.13\n",
      "2023-05-02 21:12:48,969 [INFO] [1300000/1433083 (91%)]\tLoss: 141.76\n",
      "2023-05-02 21:12:49,999 [INFO] [1400000/1433083 (98%)]\tLoss: 250.43\n",
      "2023-05-02 21:12:50,375 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:12:53,188 [INFO] Test set accuracy: 604599/614179 (0.9844019414535502)\n",
      "\n",
      "Epoch:  56%|█████▌    | 14/25 [06:05<04:48, 26.20s/it]2023-05-02 21:12:55,690 [INFO] [100000/1433083 (7%)]\tLoss: 122.58\n",
      "2023-05-02 21:12:57,805 [INFO] [200000/1433083 (14%)]\tLoss: 99.38\n",
      "2023-05-02 21:12:59,749 [INFO] [300000/1433083 (21%)]\tLoss: 92.41\n",
      "2023-05-02 21:13:01,635 [INFO] [400000/1433083 (28%)]\tLoss: 85.10\n",
      "2023-05-02 21:13:03,304 [INFO] [500000/1433083 (35%)]\tLoss: 88.93\n",
      "2023-05-02 21:13:04,858 [INFO] [600000/1433083 (42%)]\tLoss: 78.10\n",
      "2023-05-02 21:13:06,513 [INFO] [700000/1433083 (49%)]\tLoss: 70.58\n",
      "2023-05-02 21:13:08,163 [INFO] [800000/1433083 (56%)]\tLoss: 64.87\n",
      "2023-05-02 21:13:09,832 [INFO] [900000/1433083 (63%)]\tLoss: 72.80\n",
      "2023-05-02 21:13:11,322 [INFO] [1000000/1433083 (70%)]\tLoss: 88.90\n",
      "2023-05-02 21:13:12,714 [INFO] [1100000/1433083 (77%)]\tLoss: 92.22\n",
      "2023-05-02 21:13:14,028 [INFO] [1200000/1433083 (84%)]\tLoss: 107.15\n",
      "2023-05-02 21:13:15,201 [INFO] [1300000/1433083 (91%)]\tLoss: 130.44\n",
      "2023-05-02 21:13:16,236 [INFO] [1400000/1433083 (98%)]\tLoss: 231.22\n",
      "2023-05-02 21:13:16,597 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:13:19,367 [INFO] Test set accuracy: 605951/614179 (0.986603254100189)\n",
      "\n",
      "Epoch:  60%|██████    | 15/25 [06:32<04:21, 26.19s/it]2023-05-02 21:13:21,854 [INFO] [100000/1433083 (7%)]\tLoss: 107.96\n",
      "2023-05-02 21:13:23,970 [INFO] [200000/1433083 (14%)]\tLoss: 85.93\n",
      "2023-05-02 21:13:25,908 [INFO] [300000/1433083 (21%)]\tLoss: 79.36\n",
      "2023-05-02 21:13:27,791 [INFO] [400000/1433083 (28%)]\tLoss: 73.02\n",
      "2023-05-02 21:13:29,419 [INFO] [500000/1433083 (35%)]\tLoss: 74.59\n",
      "2023-05-02 21:13:31,011 [INFO] [600000/1433083 (42%)]\tLoss: 65.68\n",
      "2023-05-02 21:13:32,653 [INFO] [700000/1433083 (49%)]\tLoss: 59.54\n",
      "2023-05-02 21:13:34,313 [INFO] [800000/1433083 (56%)]\tLoss: 54.83\n",
      "2023-05-02 21:13:35,998 [INFO] [900000/1433083 (63%)]\tLoss: 64.99\n",
      "2023-05-02 21:13:37,488 [INFO] [1000000/1433083 (70%)]\tLoss: 83.32\n",
      "2023-05-02 21:13:38,900 [INFO] [1100000/1433083 (77%)]\tLoss: 89.38\n",
      "2023-05-02 21:13:40,208 [INFO] [1200000/1433083 (84%)]\tLoss: 103.63\n",
      "2023-05-02 21:13:41,379 [INFO] [1300000/1433083 (91%)]\tLoss: 122.80\n",
      "2023-05-02 21:13:42,416 [INFO] [1400000/1433083 (98%)]\tLoss: 217.50\n",
      "2023-05-02 21:13:42,786 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:13:45,579 [INFO] Test set accuracy: 606187/614179 (0.9869875068994544)\n",
      "\n",
      "Epoch:  64%|██████▍   | 16/25 [06:58<03:55, 26.20s/it]2023-05-02 21:13:48,089 [INFO] [100000/1433083 (7%)]\tLoss: 101.27\n",
      "2023-05-02 21:13:50,194 [INFO] [200000/1433083 (14%)]\tLoss: 84.51\n",
      "2023-05-02 21:13:52,157 [INFO] [300000/1433083 (21%)]\tLoss: 73.09\n",
      "2023-05-02 21:13:54,004 [INFO] [400000/1433083 (28%)]\tLoss: 67.31\n",
      "2023-05-02 21:13:55,757 [INFO] [500000/1433083 (35%)]\tLoss: 77.05\n",
      "2023-05-02 21:13:57,421 [INFO] [600000/1433083 (42%)]\tLoss: 71.00\n",
      "2023-05-02 21:13:59,065 [INFO] [700000/1433083 (49%)]\tLoss: 67.40\n",
      "2023-05-02 21:14:00,741 [INFO] [800000/1433083 (56%)]\tLoss: 64.17\n",
      "2023-05-02 21:14:02,405 [INFO] [900000/1433083 (63%)]\tLoss: 74.53\n",
      "2023-05-02 21:14:03,918 [INFO] [1000000/1433083 (70%)]\tLoss: 85.93\n",
      "2023-05-02 21:14:05,332 [INFO] [1100000/1433083 (77%)]\tLoss: 87.65\n",
      "2023-05-02 21:14:06,630 [INFO] [1200000/1433083 (84%)]\tLoss: 98.32\n",
      "2023-05-02 21:14:07,818 [INFO] [1300000/1433083 (91%)]\tLoss: 115.96\n",
      "2023-05-02 21:14:08,856 [INFO] [1400000/1433083 (98%)]\tLoss: 208.65\n",
      "2023-05-02 21:14:09,233 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:14:12,036 [INFO] Test set accuracy: 606636/614179 (0.987718564131955)\n",
      "\n",
      "Epoch:  68%|██████▊   | 17/25 [07:24<03:30, 26.28s/it]2023-05-02 21:14:14,550 [INFO] [100000/1433083 (7%)]\tLoss: 102.36\n",
      "2023-05-02 21:14:16,587 [INFO] [200000/1433083 (14%)]\tLoss: 77.04\n",
      "2023-05-02 21:14:18,420 [INFO] [300000/1433083 (21%)]\tLoss: 67.73\n",
      "2023-05-02 21:14:20,256 [INFO] [400000/1433083 (28%)]\tLoss: 60.90\n",
      "2023-05-02 21:14:21,984 [INFO] [500000/1433083 (35%)]\tLoss: 62.13\n",
      "2023-05-02 21:14:23,644 [INFO] [600000/1433083 (42%)]\tLoss: 54.71\n",
      "2023-05-02 21:14:25,310 [INFO] [700000/1433083 (49%)]\tLoss: 49.74\n",
      "2023-05-02 21:14:26,959 [INFO] [800000/1433083 (56%)]\tLoss: 45.81\n",
      "2023-05-02 21:14:28,606 [INFO] [900000/1433083 (63%)]\tLoss: 52.37\n",
      "2023-05-02 21:14:30,097 [INFO] [1000000/1433083 (70%)]\tLoss: 82.36\n",
      "2023-05-02 21:14:31,505 [INFO] [1100000/1433083 (77%)]\tLoss: 86.60\n",
      "2023-05-02 21:14:32,923 [INFO] [1200000/1433083 (84%)]\tLoss: 98.55\n",
      "2023-05-02 21:14:34,073 [INFO] [1300000/1433083 (91%)]\tLoss: 114.85\n",
      "2023-05-02 21:14:35,116 [INFO] [1400000/1433083 (98%)]\tLoss: 200.52\n",
      "2023-05-02 21:14:35,487 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:14:38,276 [INFO] Test set accuracy: 606618/614179 (0.9876892567150619)\n",
      "\n",
      "Epoch:  72%|███████▏  | 18/25 [07:51<03:03, 26.27s/it]2023-05-02 21:14:40,769 [INFO] [100000/1433083 (7%)]\tLoss: 99.31\n",
      "2023-05-02 21:14:42,885 [INFO] [200000/1433083 (14%)]\tLoss: 76.57\n",
      "2023-05-02 21:14:44,825 [INFO] [300000/1433083 (21%)]\tLoss: 66.07\n",
      "2023-05-02 21:14:46,676 [INFO] [400000/1433083 (28%)]\tLoss: 58.91\n",
      "2023-05-02 21:14:48,440 [INFO] [500000/1433083 (35%)]\tLoss: 59.27\n",
      "2023-05-02 21:14:50,112 [INFO] [600000/1433083 (42%)]\tLoss: 51.99\n",
      "2023-05-02 21:14:51,750 [INFO] [700000/1433083 (49%)]\tLoss: 47.17\n",
      "2023-05-02 21:14:53,424 [INFO] [800000/1433083 (56%)]\tLoss: 43.31\n",
      "2023-05-02 21:14:55,088 [INFO] [900000/1433083 (63%)]\tLoss: 49.02\n",
      "2023-05-02 21:14:56,576 [INFO] [1000000/1433083 (70%)]\tLoss: 75.94\n",
      "2023-05-02 21:14:57,985 [INFO] [1100000/1433083 (77%)]\tLoss: 78.37\n",
      "2023-05-02 21:14:59,291 [INFO] [1200000/1433083 (84%)]\tLoss: 87.38\n",
      "2023-05-02 21:15:00,467 [INFO] [1300000/1433083 (91%)]\tLoss: 102.02\n",
      "2023-05-02 21:15:01,505 [INFO] [1400000/1433083 (98%)]\tLoss: 180.12\n",
      "2023-05-02 21:15:01,873 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:15:04,658 [INFO] Test set accuracy: 607259/614179 (0.9887329263944225)\n",
      "\n",
      "Epoch:  76%|███████▌  | 19/25 [08:17<02:37, 26.30s/it]2023-05-02 21:15:07,168 [INFO] [100000/1433083 (7%)]\tLoss: 74.89\n",
      "2023-05-02 21:15:09,268 [INFO] [200000/1433083 (14%)]\tLoss: 57.80\n",
      "2023-05-02 21:15:11,200 [INFO] [300000/1433083 (21%)]\tLoss: 50.99\n",
      "2023-05-02 21:15:13,059 [INFO] [400000/1433083 (28%)]\tLoss: 45.93\n",
      "2023-05-02 21:15:14,806 [INFO] [500000/1433083 (35%)]\tLoss: 54.44\n",
      "2023-05-02 21:15:16,450 [INFO] [600000/1433083 (42%)]\tLoss: 48.83\n",
      "2023-05-02 21:15:18,082 [INFO] [700000/1433083 (49%)]\tLoss: 44.51\n",
      "2023-05-02 21:15:19,749 [INFO] [800000/1433083 (56%)]\tLoss: 40.93\n",
      "2023-05-02 21:15:21,426 [INFO] [900000/1433083 (63%)]\tLoss: 46.09\n",
      "2023-05-02 21:15:22,921 [INFO] [1000000/1433083 (70%)]\tLoss: 55.73\n",
      "2023-05-02 21:15:24,326 [INFO] [1100000/1433083 (77%)]\tLoss: 57.62\n",
      "2023-05-02 21:15:25,622 [INFO] [1200000/1433083 (84%)]\tLoss: 67.17\n",
      "2023-05-02 21:15:26,795 [INFO] [1300000/1433083 (91%)]\tLoss: 92.60\n",
      "2023-05-02 21:15:27,823 [INFO] [1400000/1433083 (98%)]\tLoss: 171.07\n",
      "2023-05-02 21:15:28,192 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:15:30,980 [INFO] Test set accuracy: 607493/614179 (0.9891139228140331)\n",
      "\n",
      "Epoch:  80%|████████  | 20/25 [08:43<02:11, 26.31s/it]2023-05-02 21:15:33,478 [INFO] [100000/1433083 (7%)]\tLoss: 73.48\n",
      "2023-05-02 21:15:35,570 [INFO] [200000/1433083 (14%)]\tLoss: 55.84\n",
      "2023-05-02 21:15:37,480 [INFO] [300000/1433083 (21%)]\tLoss: 48.63\n",
      "2023-05-02 21:15:39,323 [INFO] [400000/1433083 (28%)]\tLoss: 43.39\n",
      "2023-05-02 21:15:41,079 [INFO] [500000/1433083 (35%)]\tLoss: 44.25\n",
      "2023-05-02 21:15:42,754 [INFO] [600000/1433083 (42%)]\tLoss: 39.11\n",
      "2023-05-02 21:15:44,387 [INFO] [700000/1433083 (49%)]\tLoss: 35.65\n",
      "2023-05-02 21:15:46,040 [INFO] [800000/1433083 (56%)]\tLoss: 32.74\n",
      "2023-05-02 21:15:47,722 [INFO] [900000/1433083 (63%)]\tLoss: 39.40\n",
      "2023-05-02 21:15:49,204 [INFO] [1000000/1433083 (70%)]\tLoss: 51.52\n",
      "2023-05-02 21:15:50,596 [INFO] [1100000/1433083 (77%)]\tLoss: 52.99\n",
      "2023-05-02 21:15:51,903 [INFO] [1200000/1433083 (84%)]\tLoss: 60.22\n",
      "2023-05-02 21:15:53,016 [INFO] [1300000/1433083 (91%)]\tLoss: 79.36\n",
      "2023-05-02 21:15:54,040 [INFO] [1400000/1433083 (98%)]\tLoss: 157.36\n",
      "2023-05-02 21:15:54,400 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:15:57,178 [INFO] Test set accuracy: 607671/614179 (0.9894037406033095)\n",
      "\n",
      "Epoch:  84%|████████▍ | 21/25 [09:09<01:45, 26.27s/it]2023-05-02 21:15:59,669 [INFO] [100000/1433083 (7%)]\tLoss: 62.04\n",
      "2023-05-02 21:16:01,780 [INFO] [200000/1433083 (14%)]\tLoss: 47.65\n",
      "2023-05-02 21:16:03,613 [INFO] [300000/1433083 (21%)]\tLoss: 41.48\n",
      "2023-05-02 21:16:05,364 [INFO] [400000/1433083 (28%)]\tLoss: 37.02\n",
      "2023-05-02 21:16:07,075 [INFO] [500000/1433083 (35%)]\tLoss: 38.73\n",
      "2023-05-02 21:16:08,733 [INFO] [600000/1433083 (42%)]\tLoss: 34.30\n",
      "2023-05-02 21:16:10,377 [INFO] [700000/1433083 (49%)]\tLoss: 31.30\n",
      "2023-05-02 21:16:12,032 [INFO] [800000/1433083 (56%)]\tLoss: 28.68\n",
      "2023-05-02 21:16:13,694 [INFO] [900000/1433083 (63%)]\tLoss: 33.99\n",
      "2023-05-02 21:16:15,176 [INFO] [1000000/1433083 (70%)]\tLoss: 45.48\n",
      "2023-05-02 21:16:16,474 [INFO] [1100000/1433083 (77%)]\tLoss: 46.88\n",
      "2023-05-02 21:16:17,753 [INFO] [1200000/1433083 (84%)]\tLoss: 53.14\n",
      "2023-05-02 21:16:18,928 [INFO] [1300000/1433083 (91%)]\tLoss: 65.49\n",
      "2023-05-02 21:16:19,956 [INFO] [1400000/1433083 (98%)]\tLoss: 148.87\n",
      "2023-05-02 21:16:20,319 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:16:23,098 [INFO] Test set accuracy: 606723/614179 (0.9878602166469385)\n",
      "\n",
      "Epoch:  88%|████████▊ | 22/25 [09:35<01:18, 26.17s/it]2023-05-02 21:16:25,612 [INFO] [100000/1433083 (7%)]\tLoss: 55.22\n",
      "2023-05-02 21:16:27,708 [INFO] [200000/1433083 (14%)]\tLoss: 42.97\n",
      "2023-05-02 21:16:29,629 [INFO] [300000/1433083 (21%)]\tLoss: 37.27\n",
      "2023-05-02 21:16:31,482 [INFO] [400000/1433083 (28%)]\tLoss: 33.16\n",
      "2023-05-02 21:16:33,232 [INFO] [500000/1433083 (35%)]\tLoss: 36.24\n",
      "2023-05-02 21:16:34,901 [INFO] [600000/1433083 (42%)]\tLoss: 31.99\n",
      "2023-05-02 21:16:36,552 [INFO] [700000/1433083 (49%)]\tLoss: 29.16\n",
      "2023-05-02 21:16:38,184 [INFO] [800000/1433083 (56%)]\tLoss: 26.65\n",
      "2023-05-02 21:16:39,866 [INFO] [900000/1433083 (63%)]\tLoss: 31.66\n",
      "2023-05-02 21:16:41,370 [INFO] [1000000/1433083 (70%)]\tLoss: 41.20\n",
      "2023-05-02 21:16:42,783 [INFO] [1100000/1433083 (77%)]\tLoss: 42.39\n",
      "2023-05-02 21:16:44,107 [INFO] [1200000/1433083 (84%)]\tLoss: 48.09\n",
      "2023-05-02 21:16:45,379 [INFO] [1300000/1433083 (91%)]\tLoss: 58.89\n",
      "2023-05-02 21:16:46,405 [INFO] [1400000/1433083 (98%)]\tLoss: 131.03\n",
      "2023-05-02 21:16:46,769 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:16:49,539 [INFO] Test set accuracy: 607254/614179 (0.9887247854452855)\n",
      "\n",
      "Epoch:  92%|█████████▏| 23/25 [10:02<00:52, 26.25s/it]2023-05-02 21:16:52,033 [INFO] [100000/1433083 (7%)]\tLoss: 46.35\n",
      "2023-05-02 21:16:54,141 [INFO] [200000/1433083 (14%)]\tLoss: 36.21\n",
      "2023-05-02 21:16:56,036 [INFO] [300000/1433083 (21%)]\tLoss: 37.86\n",
      "2023-05-02 21:16:57,890 [INFO] [400000/1433083 (28%)]\tLoss: 33.15\n",
      "2023-05-02 21:16:59,653 [INFO] [500000/1433083 (35%)]\tLoss: 36.84\n",
      "2023-05-02 21:17:01,298 [INFO] [600000/1433083 (42%)]\tLoss: 32.44\n",
      "2023-05-02 21:17:02,855 [INFO] [700000/1433083 (49%)]\tLoss: 29.38\n",
      "2023-05-02 21:17:04,388 [INFO] [800000/1433083 (56%)]\tLoss: 26.75\n",
      "2023-05-02 21:17:06,022 [INFO] [900000/1433083 (63%)]\tLoss: 30.73\n",
      "2023-05-02 21:17:07,504 [INFO] [1000000/1433083 (70%)]\tLoss: 45.26\n",
      "2023-05-02 21:17:08,904 [INFO] [1100000/1433083 (77%)]\tLoss: 45.87\n",
      "2023-05-02 21:17:10,214 [INFO] [1200000/1433083 (84%)]\tLoss: 50.78\n",
      "2023-05-02 21:17:11,396 [INFO] [1300000/1433083 (91%)]\tLoss: 62.31\n",
      "2023-05-02 21:17:12,442 [INFO] [1400000/1433083 (98%)]\tLoss: 124.59\n",
      "2023-05-02 21:17:12,806 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:17:15,603 [INFO] Test set accuracy: 608367/614179 (0.9905369607231768)\n",
      "\n",
      "Epoch:  96%|█████████▌| 24/25 [10:28<00:26, 26.19s/it]2023-05-02 21:17:18,115 [INFO] [100000/1433083 (7%)]\tLoss: 87.07\n",
      "2023-05-02 21:17:20,250 [INFO] [200000/1433083 (14%)]\tLoss: 62.01\n",
      "2023-05-02 21:17:22,188 [INFO] [300000/1433083 (21%)]\tLoss: 53.08\n",
      "2023-05-02 21:17:24,036 [INFO] [400000/1433083 (28%)]\tLoss: 84.24\n",
      "2023-05-02 21:17:25,788 [INFO] [500000/1433083 (35%)]\tLoss: 80.75\n",
      "2023-05-02 21:17:27,423 [INFO] [600000/1433083 (42%)]\tLoss: 70.26\n",
      "2023-05-02 21:17:29,085 [INFO] [700000/1433083 (49%)]\tLoss: 62.50\n",
      "2023-05-02 21:17:30,749 [INFO] [800000/1433083 (56%)]\tLoss: 56.27\n",
      "2023-05-02 21:17:32,404 [INFO] [900000/1433083 (63%)]\tLoss: 58.70\n",
      "2023-05-02 21:17:33,914 [INFO] [1000000/1433083 (70%)]\tLoss: 65.38\n",
      "2023-05-02 21:17:35,312 [INFO] [1100000/1433083 (77%)]\tLoss: 64.92\n",
      "2023-05-02 21:17:36,598 [INFO] [1200000/1433083 (84%)]\tLoss: 68.62\n",
      "2023-05-02 21:17:37,774 [INFO] [1300000/1433083 (91%)]\tLoss: 77.44\n",
      "2023-05-02 21:17:38,795 [INFO] [1400000/1433083 (98%)]\tLoss: 132.90\n",
      "2023-05-02 21:17:39,166 [INFO] Evaluating trained model ...\n",
      "2023-05-02 21:17:41,952 [INFO] Test set accuracy: 608710/614179 (0.9910954298339735)\n",
      "\n",
      "Epoch: 100%|██████████| 25/25 [10:54<00:00, 26.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 21s, sys: 19.2 s, total: 21min 40s\n",
      "Wall time: 10min 55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dd.train_model(train_data, labels, batch_size=BATCH_SIZE, epochs=EPOCHS, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "Save pretrained model to a given output location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 21:17:41,976 [INFO] Pretrained model checkpoint saved to location: 'models/rnn_classifier_2023-05-02_21_17_41.bin'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory 'models'\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODELS_DIR):\n",
    "    print(\"Creating directory '{}'\".format(MODELS_DIR))\n",
    "    os.makedirs(MODELS_DIR)\n",
    "\n",
    "now = datetime.now()\n",
    "model_filename = \"rnn_classifier_{}.bin\".format(now.strftime(\"%Y-%m-%d_%H_%M_%S\"))\n",
    "model_filepath = os.path.join(MODELS_DIR, model_filename)\n",
    "dd.save_checkpoint(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model generated above, we now score the test dataset against the model to determine if the domain is likely generated by a DGA or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 21:17:42,013 [INFO] Found GPU's now setting up cuda for the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9924810193770871\n"
     ]
    }
   ],
   "source": [
    "dga_detector = DGADetector()\n",
    "dga_detector.load_checkpoint(model_filepath)\n",
    "\n",
    "domain_train, domain_test, type_train, type_test = train_test_split(gdf, \"type\", train_size=0.7)\n",
    "test_df = cudf.DataFrame()\n",
    "test_df[\"type\"] = type_test.reset_index(drop=True)\n",
    "test_df[\"domain\"] = domain_test.reset_index(drop=True)\n",
    "\n",
    "test_dataset = DGADataset(test_df, 100)\n",
    "test_dataloader = DataLoader(test_dataset, batchsize=BATCH_SIZE)\n",
    "\n",
    "pred_results = []\n",
    "true_results = []\n",
    "for chunk in test_dataloader.get_chunks():\n",
    "    pred_results.append(list(dga_detector.predict(chunk['domain']).values_host))\n",
    "    true_results.append(list(chunk['type'].values_host))\n",
    "pred_results = np.concatenate(pred_results)\n",
    "true_results = np.concatenate(true_results)\n",
    "accuracy_score_result = accuracy_score(pred_results, true_results)\n",
    "\n",
    "print('Model accuracy: %s'%(accuracy_score_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.980\n"
     ]
    }
   ],
   "source": [
    "average_precision = average_precision_score(true_results, pred_results)\n",
    "\n",
    "print('Average precision score: {0:0.3f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DGA detection implementation enables users to train their models for detection and also use existing models. This capability could also be used in conjunction with log parsing efforts if the logs contain domain names. Data is kept in GPU memory, removing unnecessary copy/converts and providing a 4X speed advantage over CPU only implementations. This is esepcially true with large batch sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
